これまでのステップで、Retriever が集めてきた情報 (Context) を使って、AI (LLM) に質問に答えてもらう **RAG チェーン** を作りましたね！ AI が外部の知識を使って回答してくれるなんて、すごい進歩です！

でも、AI の回答を見たとき、「へぇ、そうなんだ。でも、それって本当に合ってるのかな？」「どの資料を参考にしたんだろう？」って思うこと、ありませんか？ 特に仕事で使う場合、情報の出所が分からないと、ちょっと不安になりますよね。

そこで今回のステップでは、RAG チェーンをさらにパワーアップさせて、AI が回答を生成する際に**参考にした情報源 (Source Documents)** も一緒に表示する方法を探求します！ これができるようになると、AI の回答の**信頼性**や**透明性**がグッと高まり、ユーザーも安心して結果を利用できるようになります。さあ、AI の回答に「裏付け」を与える方法を学びましょう！

## 1. はじめに：このステップで目指すこと

### 🎯 今回のゴール

このステップを最後まで体験すると、あなたはこんなことができるようになります！

- AI の答えに「これは確かな情報だよ！」という**出典**や**引用元**を付けることが、なぜ大切なのかが分かります！
- 前に作った RAG チェーンを改造して、LLM が作った答えと、その根拠になった**ソースドキュメント (Context)** の両方を出力する魔法を覚えます！
- LCEL のカッコいい機能、辞書 `{}` を使った並列データフローを使って、答えと根拠の両方をゲットする技を身につけます！
- **具体的な成果物:** ステップ 17 の RAG チェーンを改良し、最終的な回答文字列だけでなく、回答生成に使われた **Document** オブジェクト（メタデータ付き！）も含む辞書を出力し、そこから出典情報を表示する Python コードを完成させます！

### 🔑 このステップのポイント

今回の信頼性アップ大作戦で重要なキーワードはこちら！

- **引用 / 出典 (Citation / Source Documents):** AI の答えが、どの情報から来ているかを示すこと！
- **透明性 / 信頼性 (Transparency / Trustworthiness):** AI を安心して使うためのカギ！
- **LCEL (LangChain Expression Language):** チェーンを繋ぐ魔法の言葉！
- **並列データフロー (`{}` / `RunnableParallel`):** 複数の結果を一つの辞書にまとめる LCEL の技！
- **`RunnablePassthrough`:** データを次の工程にそっと渡す便利なやつ。
- **メタデータ (Metadata):** 文書についている「タグ」のような情報。出典探しに役立ちます！

### ✅ 前提知識

この新しい挑戦を始める前に、以下の準備は OK？

- **ステップ 17 の経験:** Retriever、プロンプト、LLM、パーサーを繋いだ基本的な RAG チェーンを作ったことがあること。
- **LCEL の基本:** パイプ **`|`** と **`RunnablePassthrough`** の使い方を知っていること。
- **Python の基本:** 辞書やリストを扱えること、for ループが書けること。
- **必要な装備:** `langchain` や `faiss-cpu` などのライブラリと API キーの準備。（FAISS を使う前提ですよ！）

準備ができたら、回答に「根拠」という名の信頼性をプラスしに行きましょう！

---

## 2. 準備運動：どうやって出典も一緒に出すの？

### 🎯 目標

あれ？前の RAG チェーンだと、なんで出典情報が消えちゃったんだっけ？ そして、LCEL を使うとどうやって答えと出典の両方をうまく出力できるのか、その秘密を探ってみましょう！

### なぜ出典情報が消えちゃう？

ステップ 17 で作った基本的な RAG チェーン、ちょっとだけ思い出してみましょう。

```python
# ステップ 17 のチェーン (イメージ)
# データを準備する部分 -> プロンプトに入れる -> LLM が考える -> 文字列だけ出す！
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | rag_prompt
    | llm
    | StrOutputParser() # ← ここで文字列だけになっちゃう！
)
```

そう、最後の `StrOutputParser()` が、LLM の考えた答え (`AIMessage`) から**文字の部分だけ**を取り出していたんですね。だから、答えを作るのに使ったはずの Context (Document リスト) は、最終結果には含まれていなかったんです。途中で置いてきぼりになっちゃってたんですね。

### 回答と Context を両方ゲットする作戦！

「じゃあ、どうすればいいの？」って思いますよね。答えも欲しいし、どの Context を使ったかも知りたい！

でも大丈夫！LCEL には、これを解決するクールな方法があるんです！ それが、**複数の情報を並行して処理し、結果を一つの辞書 `{}` にまとめて出力する** 機能です！ (内部的には **`RunnableParallel`** という部品が活躍しています)

作戦はこうです！

1.  まず、Retriever で Context を探しつつ、元の質問も忘れないように、両方を持つ辞書を作ります。
    `{"context": [Documentのリスト], "question": "ユーザーの質問"}`
2.  次に、この辞書を使って、**同時に**二つのことをやります！
    - **回答作り:** 辞書の中の Context と質問を使って、プロンプトを作り、LLM に渡して答え (文字列) を作ってもらう。
    - **Context キープ:** 辞書の中の Context (Document のリスト) を、そのまま大事に持っておく。
3.  最後に、出来上がった「回答文字列」と、大事に持っていた「Context リスト」を、**一つの辞書にパッケージング**して出力！
    `{"answer": "これが答えだよ", "context": [参考にしたのはこれ！]}`

これで、答えとその根拠を両方ゲットできますね！ この「Context キープ」の部分で、また **`RunnablePassthrough`** や簡単な `lambda` 関数が役立ちます。

---

## 3. 実践タイム：出典付き RAG チェーンを組み立てよう！

### 🎯 目標

LCEL の辞書 `{}` 記法と、前回エラー解決で活躍した `itemgetter`、そして `RunnablePassthrough` を駆使して、LLM の回答と使われた Context の両方を返す、新しい RAG チェーンを組み立てて、動かしてみましょう！

### ステップ・バイ・ステップ実装

#### 1. 準備 (インポート、基本オブジェクトの準備):

まずは必要な道具を揃えます。インポートを確認し、LLM、Embedding モデル、FAISS Vector Store、ベース Retriever、Context フォーマット関数を用意しましょう。（ステップ 17/20 のコードをベースにします）

```python
# step21_rag_citation_final.py
import os
import sys
from dotenv import load_dotenv
from operator import itemgetter # ★ これが必要！

# --- 必要な LangChain モジュール ---
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document
# RunnablePassthrough, RunnableLambda, RunnableParallel をインポート
from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableParallel

# Vector Store (FAISS)
try:
    from langchain_community.vectorstores import FAISS
    # FAISS 本体も必要
    try:
        import faiss
        print("FAISS をインポートしました。")
    except ImportError:
        print("エラー: faiss ライブラリが見つかりません。\n   'pip install faiss-cpu' または 'pip install faiss-gpu' を実行してください。")
        sys.exit(1)
except ImportError:
    print("エラー: FAISS または langchain-community が見つかりません。\n   'pip install faiss-cpu langchain-community' を実行してください。")
    sys.exit(1)

# Text Splitter (サンプル文書用)
try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    print("RecursiveCharacterTextSplitter をインポートしました。")
except ImportError:
    print("エラー: RecursiveCharacterTextSplitter が見つかりません。\n   'pip install langchain-text-splitters' を実行してください。")
    sys.exit(1)

# トークン計算用
try:
    import tiktoken
    print("tiktoken をインポートしました。")
except ImportError:
    print("エラー: tiktoken が見つかりません。\n   'pip install tiktoken' を実行してください。")
    sys.exit(1)

print("--- 必要なモジュールのインポート完了 ---")

# --- 基本設定 (LLM, Embedding) ---
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    print("エラー: OpenAI API キーが設定されていません。")
    sys.exit(1)
else:
    print("OpenAI API キーを読み込みました。")

try:
    embedding_dim = 1536
    embeddings_model = OpenAIEmbeddings(model="text-embedding-3-small", api_key=openai_api_key)
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0, api_key=openai_api_key)
    print(f"--- Embedding モデルと LLM 準備完了 ---")
except Exception as e:
    print(f"エラー: モデルの初期化に失敗しました: {e}")
    sys.exit(1)

# --- Vector Store と Retriever の準備 ---
sample_documents = [
    Document(page_content="LangChain は LLM アプリケーション開発を支援するフレームワークです。", metadata={"source": "doc-b", "page": 1}),
    Document(page_content="LCEL は LangChain Expression Language の略で、チェーン構築を容易にします。", metadata={"source": "doc-b", "page": 2}), # ★ page メタデータあり！
    Document(page_content="エージェントはツールを使って外部システムと対話し、タスクを実行します。", metadata={"source": "doc-c", "page": 1}),
    Document(page_content="LangGraph は状態を持つ複雑なグラフを構築できます。", metadata={"source": "doc-a", "page": 1}),
]
try:
    vector_store = FAISS.from_documents(sample_documents, embeddings_model)
    retriever = vector_store.as_retriever(search_kwargs={'k': 2})
    print("--- Vector Store (FAISS) と Retriever 準備完了 (k=2) ---")
except Exception as e:
    print(f"エラー: Vector Store または Retriever の準備中にエラー: {e}")
    sys.exit(1)

# --- Context フォーマット関数 ---
def format_docs(docs: list[Document]) -> str:
    return "\n\n".join(doc.page_content for doc in docs)

format_docs_runnable = RunnableLambda(format_docs)
print("--- Context フォーマット関数準備完了 ---")
```

- 前回のエラー解決に使った `itemgetter` を忘れずにインポートします。
- サンプル文書に `page` メタデータがあることを確認します。

#### 2. RAG プロンプトテンプレートの準備:

プロンプトは前回と同じです。Context と Question を待っています。

```python
# step21_rag_citation_final.py (続き)

# RAG 用のプロンプトテンプレート (ステップ 17 と同じ)
template = """
以下の Context のみを基にして、最後の Question に答えてください。

Context:
{context}

Question: {question}
"""
rag_prompt = ChatPromptTemplate.from_template(template)
print("--- RAG プロンプトテンプレート準備完了 ---")
```

#### 3. 出典付き RAG チェーンの組み立て (LCEL):

ここがクライマックス！ 辞書 `{}` と `itemgetter` を使って、回答 (answer) と文脈 (context) を両方出力するチェーンを組み立てます。

```python
# step21_rag_citation_final.py (続き)

print("\n--- 出典付き RAG チェーンの組み立て ---")

# 1. Context を取得し、質問文字列と一緒に保持する部分 (★ itemgetter を使用 ★)
#    入力: {"question": "..."}
#    出力: {"context": [Document...], "question": "..."} (questionは文字列)
setup_and_retrieval = RunnableParallel(
    context=itemgetter("question") | retriever,
    question=itemgetter("question")
)

# 2. 回答を生成する部分のチェーン (変更なし)
answer_generation_chain = (
    rag_prompt
    | llm
    | StrOutputParser()
)

# 3. 全体を組み立て: 回答生成と Context 保持を並行処理
#    入力は {"question": "..."}
#    出力は {"answer": "...", "context": [Document...]}
rag_chain_with_source = (
    setup_and_retrieval # ① Context と Question を準備
    | RunnableParallel( # ② answer と context を並行して準備
          # "answer" の準備:
          # ①の出力辞書から context ([Docs..]) を取り出し文字列化し、
          # question と一緒に answer_generation_chain へ渡す
          answer=RunnablePassthrough.assign(
              context=itemgetter("context") | format_docs_runnable
          ) | answer_generation_chain,
          # "context" の準備:
          # ①の出力辞書から context ([Docs..]) をそのまま取り出す
          context=itemgetter("context")
      )
)

print("--- 出典付き RAG チェーン組み立て完了 ---")
print("処理の流れ: 入力(question) -> Retriever & 質問保持 -> (並列データ処理: 回答生成 / Context保持) -> 最終辞書出力")
```

- **`setup_and_retrieval`:** `itemgetter("question")` を使うことで、入力辞書から質問文字列だけを正しく取り出し、`retriever` と `"question"` キーの両方に渡せるように修正しました。これで `TypeError` は起きないはず！
- **`rag_chain_with_source`:**
  - `RunnableParallel` を使って、最終的に `answer` と `context` のキーを持つ辞書を作ります。
  - `answer` の計算では、`setup_and_retrieval` から来た `context` (Document リスト) を `itemgetter` で取り出し、`format_docs_runnable` で文字列化してから `answer_generation_chain` に渡します。
  - `context` の値は、`itemgetter("context")` で `setup_and_retrieval` から来た Document リストをそのまま使います。

#### 4. 出典付き RAG チェーンの実行と出典表示:

組み立てた `rag_chain_with_source` を実行！ 返ってきた辞書から回答と出典情報を取り出して表示します。今度こそ動くはず！

```python
# step21_rag_citation_final.py (続き)

# --- 出典付き RAG チェーンの実行 ---
question = "LCEL について教えて"
print(f"\n--- 出典付き RAG チェーン実行 (質問: '{question}') ---")

try:
    # チェーンを実行！ 出力は answer と context を含む辞書
    rag_output = rag_chain_with_source.invoke({"question": question})

    print("\n--- [AIの回答] ---")
    print(rag_output.get("answer", "回答がありません"))

    print("\n--- [出典情報 (Context)] ---")
    context_docs = rag_output.get("context")
    if context_docs:
        print(f"({len(context_docs)} 件の Context が参照されました)")
        for doc in context_docs:
            source = doc.metadata.get("source", "不明なソース")
            page = doc.metadata.get("page", "-") # page メタデータを表示！
            print(f"- 出典: {source}, ページ: {page}")
    else:
        print("出典情報が見つかりませんでした。")

except Exception as e:
    print(f"エラー: RAG チェーンの実行中にエラーが発生しました: {e}")

print("\n--- 処理終了 ---")
```

- `.invoke()` の結果 (`rag_output`) から `answer` と `context` を取り出して表示します。
- 今度は `page` メタデータもちゃんと表示するようにしました！

### 完成コード (`step21_rag_citation_final.py`)

上記の実装 1〜4 を結合したものが、このステップの完成コードです。

### 実行結果の例

```text
# ...(初期化ログ)...
--- Vector Store (FAISS) と Retriever 準備完了 (k=2) ---
--- Context フォーマット関数準備完了 ---
--- RAG プロンプトテンプレート準備完了 ---

--- 出典付き RAG チェーンの組み立て ---
--- 出典付き RAG チェーン組み立て完了 ---
処理の流れ: 入力(question) -> Retriever検索 & 質問保持 -> (並列データ処理: 回答生成 / Context保持) -> 最終辞書出力

--- 出典付き RAG チェーン実行 (質問: 'LCEL について教えて') ---

--- [AIの回答] ---
LCELはLangChain Expression Languageの略で、チェーン構築を容易にします。

--- [出典情報 (Context)] ---
(2 件の Context が参照されました)
- 出典: doc-b, ページ: 2
- 出典: doc-b, ページ: 1

--- 処理終了 ---
```

- 無事に動きましたね！ AI の回答と、その根拠となった文書の出典（今回は `source` と `page`）が表示されました！

---

## 4. 深掘り解説：出典表示の仕組みと応用

### 🎯 目標

LCEL で複数の情報を同時に出力する仕組み (**`RunnableParallel`** と辞書 `{}` の使い方) と、出典情報の活用方法、そして実用上の注意点について理解を深めます。

### 並列データフロー (`RunnableParallel` / `{}`) の仕組み

LCEL チェーンの途中で `{"キー1": runnable1, "キー2": runnable2}` のような辞書を使うと、LangChain は入力データをそれぞれの `runnable` に渡し、処理を実行します。そして、各処理の結果を対応するキー (`キー1`, `キー2`) に格納した**新しい辞書**を生成して、次のステップに渡します。これは複数のデータ経路を作り、結果を一つの構造にまとめるのに非常に便利です。（実行自体が CPU レベルで完全に並列になるかは、実行環境や非同期処理を使うかによります。）

### 出典情報の活用方法

取得した Context (**Document** リスト) に含まれる `metadata` は、工夫次第でもっと役立ちます！

- **基本的な表示:** `source` や `page` を表示するのが簡単で効果的です。
- **より詳細に:** Context の一部を抜粋して表示したり、URL ならリンクにしたり。
- **信頼度の表示:** 検索スコアも一緒に保持して表示する、など。

アプリの目的に合わせて、最適な見せ方を考えてみましょう。

### 実用上の注意点

- **メタデータの質:** `metadata` に有用な情報（正確な出典、ページ番号など）が入っていることが大前提です！
- **Context Window とコスト:** LLM に渡す Context が多すぎると制限を超えたりコストが増えたりします。`k` の値を調整しましょう。
- **エラーハンドリング:** `metadata` に期待するキーがない場合も考慮して `.get()` を使うのが安全です。

---

## 5. 最終チェック：回答と出典は表示された？

### 🎯 目標

作成した出典付き RAG チェーンが期待通りに動作し、回答と Context (Source Document) の両方を含む辞書を出力し、そこから出典情報を表示できているかを確認します。

### 確認してみましょう！

- **実行！:** `step21_rag_citation_final.py` を実行してください。
- **エラーなし？:** 今度こそエラーなく最後まで実行できましたか？
- **出力形式:** 最終出力は「AI の回答」と「出典情報」の両方を含む形式になっていますか？
- **回答内容:** AI の回答は質問に対して適切で、Context の内容に基づいているように見えますか？
- **出典情報:**
  - `出典: ..., ページ: ...` のように表示されていますか？
  - 表示された出典情報は、回答内容と関連がありそうですか？
  - `question` 変数を別の内容に変えて実行すると、回答と出典は適切に変わりますか？

バッチリ動いていたら、素晴らしいです！

---

## 6. まとめ：学びの整理と次へのステップ

### ✅ 達成したこと！

これで、AI の回答に「根拠」を示すことができる、より信頼性の高い RAG システムを構築できるようになりました！

- RAG の回答において **出典情報** を示すことの重要性を理解しました。
- **LCEL** の並列データフロー機能 (辞書 `{}` / **`RunnableParallel`**) と **`RunnablePassthrough`**, **`itemgetter`** を使って、回答文字列とソースドキュメント (**Context**) の両方をチェーンの最終出力として取得する方法を学びました。
- 出力された Context リストから **メタデータ** (`source`, `page` など) を取り出し、出典として表示する方法を実装しました。
- これにより、RAG アプリケーションの **透明性** と **信頼性** を向上させる基本的な手法を習得しました。

### 🔑 学んだキーワード

- **引用 / 出典 (Citation / Source Documents)**
- **透明性 (Transparency)**
- **信頼性 (Trustworthiness)**
- **LCEL**
- **`RunnableParallel` / 辞書ショートハンド (`{}`)**
- **`RunnablePassthrough`**
- **`itemgetter`** (from `operator`)
- **メタデータ (Metadata)** (`source`, `page`)
- **RAG チェーン (複数出力)**

### 🚀 次のステップへ！

基本的な RAG チェーンを構築し、検索を改善し、さらに出典も表示できるようになりました。RAG の基本的な部品と組み立て方は、かなり理解が深まったのではないでしょうか。

ここまでの知識があれば、かなり実用的な Q&A システムなどを構築できるはずです。

次の **ステップ 22「AI への指示改善！RAG プロンプト最適化」** では、RAG システムの性能をさらに引き出すために、**プロンプト**に焦点を当てます。LLM に対して、取得した Context をより効果的に使わせるには、あるいは Context に答えがない場合に適切に応答させるには、どのような指示（プロンプト）を与えればよいのか、具体的なテクニックを探求していきます。RAG の最後の仕上げ、プロンプトエンジニアリングの世界に進みましょう！
