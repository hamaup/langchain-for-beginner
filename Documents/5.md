これまでのステップで、プロンプトの組み立て方 (ステップ 3) や、AI の応答を特定の形式にする Output Parser (ステップ 4) を学びましたね。しかし、これらをコードで繋ぎ合わせるのは、少し手間がかかると感じたかもしれません。

このステップでは、その手間を劇的に減らす LangChain の強力な機能、**LCEL (LangChain Expression Language)** を紹介します。LCEL を使えば、まるでレゴブロックを繋ぐように、プロンプト、モデル、パーサーといった部品（コンポーネント）を **魔法のパイプ `|`** で簡単に連結できます。これにより、AI アプリケーションの処理の流れ（チェーン）を、より直感的かつ効率的に構築できるようになります。

### 1. はじめに：このステップで目指すこと

#### 🎯 今回のゴール

- LangChain Expression Language (LCEL) の基本的な考え方と、なぜそれが開発を効率化するのかを理解する。
- LangChain コンポーネントが共通して持つ `Runnable` という性質と、それらを繋ぐパイプ演算子 `|` の使い方を習得する。
- **具体的な成果物:** 簡単な質問応答プロンプト、LLM、そしてシンプルな出力パーサー (`StrOutputParser`) をパイプ `|` で接続し、一連の処理（チェーン）として実行できる Python プログラムを作成する。

#### 🔑 このステップのポイント

- **LCEL の魅力:** コードがシンプルになり、複雑な処理も組み立てやすくなる理由を知る。
- **`Runnable` とは？:** LangChain の部品が持つ「共通の接続部（インターフェース）」を理解する。これを理解することが LCEL を使いこなす鍵です。
- **パイプ `|` の使い方:** コンポーネントの出力と入力をスムーズに繋げる方法をマスターする。

#### 前提知識

- ステップ 4「AI の応答を整形！Output Parser 入門」の内容、特に `StrOutputParser` の役割についての基本的な理解。
- ステップ 3「AI への指示書！プロンプトを工夫しよう」の内容（`ChatPromptTemplate`）。
- ステップ 2「AI と初対話！LLM に話しかけてみよう」の内容（`ChatOpenAI`, `.invoke()`）。
- ステップ 1 で準備した開発環境（Python, venv, API キー設定など）。

---

### 2. 準備運動：ハンズオンのための基礎知識

#### 🎯 目標

- LCEL が開発をどのように変えるのか、そしてその中心的な概念である `Runnable` とパイプ `|` 演算子の役割を具体的に理解する。

#### なぜ LCEL (LangChain Expression Language) なのか？

料理のレシピを想像してみてください。材料を準備し、切り、炒め、味付けする…といった一連の工程がありますよね。従来の LangChain の使い方では、これらの工程を一つ一つ個別の Python コード（メソッド呼び出し）として書いていました。

```python
# LCELを使わない場合のイメージ
# 1. プロンプトに材料（データ）をセット
messages = prompt_template.format_messages(country="日本")
# 2. LLM（シェフ）に調理を依頼
response_message = llm.invoke(messages)
# 3. 出来上がった料理（応答）を盛り付け（整形）
final_output = output_parser.invoke(response_message) # または .parse()
```

LCEL は、このレシピの工程を、もっと流れが見やすい形で書けるようにします。

```python
# LCELを使ったレシピ
recipe_chain = prompt_template | llm | output_parser
# レシピ全体を実行！
final_output = recipe_chain.invoke({"country": "日本"})
```

このように、パイプ `|` で工程を繋ぐだけで、一連の処理の流れ（チェーン）を表現できます。コードが短くなるだけでなく、「データがどのように流れて処理されていくか」が非常に分かりやすくなります。

#### `Runnable`：繋がるコンポーネントの秘密

なぜ、プロンプトや LLM、パーサーといった異なる部品を `|` で繋げられるのでしょうか？ その秘密は、これらのコンポーネントの多くが **`Runnable`** という共通の設計思想（プロトコル）に基づいて作られているからです。

`Runnable` は、その名の通り「実行可能」な部品であることを示します。そして、重要なのは、すべての `Runnable` が **共通の操作方法（インターフェース）** を持っていることです。例えば、

- `.invoke(入力)`: 一つの入力を受け取って、一つの結果を返す（基本的な実行）。
- `.stream(入力)`: 一つの入力を受け取って、結果を少しずつ順番に返す（応答をリアルタイムで表示したい場合など）。

などがあります。（他にも非同期処理用やバッチ処理用もありますが、まずは `.invoke()` と `.stream()` が基本です）

すべての部品が同じ「接続部の形」と「基本的な動かし方」を持っているから、パイプ `|` で簡単に繋ぎ合わせることができるのです。まるで、メーカーが違っても規格が合っていれば繋がるレゴブロックのようです。

#### 使う道具（クラス・メソッド）の紹介

このステップで主に使う `Runnable` な部品と操作です。

- **`ChatPromptTemplate`** (from `langchain.prompts`):
  - 入力（辞書）を受け取り、LLM 向けのメッセージリストを出力する `Runnable`。
- **`ChatOpenAI`** (from `langchain_openai`):
  - メッセージリストを入力として受け取り、AI の応答メッセージ (`AIMessage`) を出力する `Runnable`。
- **`StrOutputParser`** (from `langchain_core.output_parsers`):
  - **【重要】** `AIMessage` を入力として受け取り、その中のテキストコンテンツ (`.content`) だけを文字列 (`str`) として出力する、最もシンプルな `Runnable` なパーサー。
  - **インポートパスについて:** LangChain のコア機能は `langchain_core` パッケージに集約される傾向にあります。`StrOutputParser` もその一つで、`from langchain_core.output_parsers import StrOutputParser` とインポートするのが最も基本的で推奨される方法です。（互換性のために他のパスからもインポートできる場合がありますが、コアからインポートするのが確実です。）
- **パイプ演算子 `|`**:
  - 左側の `Runnable` の出力を、右側の `Runnable` の入力に渡すための接続演算子。繋いだ結果もまた新しい `Runnable` になります。
- **`.invoke(input)` メソッド**:
  - `Runnable` なチェーン全体を実行するための命令。チェーンの最初の部品が必要とするデータを `input` として渡します。

---

### 3. 実践タイム：コードを書いて動かしてみよう！

#### 🎯 目標

- 簡単な質問応答プロンプト、LLM、`StrOutputParser` を実際にパイプ `|` で接続し、LCEL チェーンを構築・実行する Python コードを作成し、その動作を確認する。

#### ステップ・バイ・ステップ実装

1.  **必要なライブラリのインポートと準備:**
    LLM、プロンプトテンプレート、そして `StrOutputParser` を準備します。プロンプトは「国の首都を尋ねる」シンプルなものに変更します。

    ```python
    # step5_lcel_pipe_intro.py
    import os
    from dotenv import load_dotenv
    from langchain_openai import ChatOpenAI
    from langchain.prompts import ChatPromptTemplate
    # StrOutputParser を langchain_core からインポート
    from langchain_core.output_parsers import StrOutputParser

    # 環境変数の読み込み
    load_dotenv()
    print("--- 環境変数読み込み完了 ---")

    # LLMの準備
    try:
        # temperature=0 に設定し、応答の再現性を高める
        llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
        print(f"--- LLM準備完了: {llm.model_name} (temperature={llm.temperature}) ---")
        print("   (temperature=0 は、毎回ほぼ同じ応答を返す設定です)")
    except Exception as e:
        # APIキーがない、無効な場合などにエラーが発生する可能性があります
        print(f"❌ エラー: ChatOpenAI の初期化に失敗しました: {e}")
        print("   確認: OpenAI APIキーは正しく .env ファイルに設定されていますか？")
        exit() # 処理を中断

    # プロンプトテンプレートの準備
    # {country} という名前の変数（プレースホルダ）を含むテンプレート
    # よりシンプルな質問応答タスクに変更
    prompt = ChatPromptTemplate.from_template(
        "{country} の首都はどこですか？"
    )
    print("--- プロンプトテンプレート準備完了 ---")

    # 出力パーサーの準備
    output_parser = StrOutputParser()
    print("--- Output Parser (StrOutputParser) 準備完了 ---")
    ```

    - **`temperature=0` について:** この設定は、LLM の応答のランダム性を最小限にし、同じ入力に対しては（ほぼ）同じ結果を返すようにします。デバッグや動作確認の際には、結果が安定するため便利です。

2.  **LCEL チェーンの構築と実行 (プロンプト | LLM):**
    まず、プロンプトと LLM だけを繋いだチェーンを作り、実行してみます。入力として国名を渡します。

    ```python
    # step5_lcel_pipe_intro.py (続き)
    print("\n--- チェーン 1: プロンプト | LLM ---")
    chain_prompt_llm = prompt | llm
    print("   チェーン構築完了")

    # プロンプトテンプレート内の {country} に代入する値を定義
    # これがチェーンの最初の入力となる
    # キー名はテンプレート内の変数名 {country} と一致させる
    input_data = {"country": "フランス"}

    try:
        print(f"\n   実行中... チェーンへの入力 (辞書形式): {input_data}")
        # チェーンの .invoke() には、最初の要素 (プロンプト) が必要とする
        # 変数名 ({country}) をキーとした辞書を渡す
        response_message = chain_prompt_llm.invoke(input_data)
        print("✅ OK: 応答を受信しました。")

        print("\n   【応答の型】:", type(response_message))
        print("   【応答 (AIMessage)】:")
        print("   ", response_message)
        print("\n   【応答の内容 (content)】:")
        print("   ", response_message.content)
    except Exception as e:
        print(f"❌ エラー: チェーン 1 の実行中にエラーが発生しました: {e}")

    ```

    - このチェーンの最後は `llm` なので、`.invoke()` の結果は `AIMessage` オブジェクトになります。

3.  **LCEL チェーンの構築と実行 (プロンプト | LLM | パーサー):**
    次に、`StrOutputParser` を繋いだ完全なチェーンを作り、**同じ入力データ**で実行してみます。

    ```python
    # step5_lcel_pipe_intro.py (続き)
    print("\n--- チェーン 2: プロンプト | LLM | StrOutputParser ---")
    chain_full = prompt | llm | output_parser
    print("   チェーン構築完了")

    try:
        # チェーン1と同じ入力データを使用する
        print(f"\n   実行中... チェーンへの入力 (辞書形式): {input_data}")
        # こちらのチェーンも、最初の要素 (プロンプト) が必要とする辞書を入力とする
        response_string = chain_full.invoke(input_data)
        print("✅ OK: 応答を受信しました。")

        print("\n   【応答の型】:", type(response_string))
        print("   【応答 (文字列)】:")
        print("   ", response_string)
    except Exception as e:
        # LLMの応答形式が予期せぬものだった場合などにエラーになる可能性もゼロではない
        print(f"❌ エラー: チェーン 2 の実行中にエラーが発生しました: {e}")

    print("\n--- 処理終了 ---")
    ```

    - このチェーンの最後は `output_parser` なので、`.invoke()` の結果は直接 `str` (文字列) になります。

#### 完成コード

```python
# step5_lcel_pipe_intro.py
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
# StrOutputParser を langchain_core からインポート
from langchain_core.output_parsers import StrOutputParser

# 環境変数の読み込み
load_dotenv()
print("--- 環境変数読み込み完了 ---")

# LLMの準備
try:
    # temperature=0 に設定し、応答の再現性を高める
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
    print(f"--- LLM準備完了: {llm.model_name} (temperature={llm.temperature}) ---")
    print("   (temperature=0 は、毎回ほぼ同じ応答を返す設定です)")
except Exception as e:
    # APIキーがない、無効な場合などにエラーが発生する可能性があります
    print(f"❌ エラー: ChatOpenAI の初期化に失敗しました: {e}")
    print("   確認: OpenAI APIキーは正しく .env ファイルに設定されていますか？")
    exit() # 処理を中断

# プロンプトテンプレートの準備
# {country} という名前の変数（プレースホルダ）を含むテンプレート
# よりシンプルな質問応答タスクに変更
prompt = ChatPromptTemplate.from_template(
    "{country} の首都はどこですか？"
)
print("--- プロンプトテンプレート準備完了 ---")

# 出力パーサーの準備
output_parser = StrOutputParser()
print("--- Output Parser (StrOutputParser) 準備完了 ---")


# --- チェーン 1: プロンプト | LLM ---
print("\n--- チェーン 1: プロンプト | LLM ---")
chain_prompt_llm = prompt | llm
print("   チェーン構築完了")

# プロンプトテンプレート内の {country} に代入する値を定義
# これがチェーンの最初の入力となる
# キー名はテンプレート内の変数名 {country} と一致させる
input_data = {"country": "フランス"}

try:
    print(f"\n   実行中... チェーンへの入力 (辞書形式): {input_data}")
    # チェーンの .invoke() には、最初の要素 (この場合はプロンプト) が
    # 必要とする入力形式（ここでは辞書）でデータを渡す
    response_message = chain_prompt_llm.invoke(input_data)
    print("✅ OK: 応答を受信しました。")

    print("\n   【応答の型】:", type(response_message))
    print("   【応答 (AIMessage)】:")
    print("   ", response_message)
    print("\n   【応答の内容 (content)】:")
    print("   ", response_message.content)
except Exception as e:
    print(f"❌ エラー: チェーン 1 の実行中にエラーが発生しました: {e}")


# --- チェーン 2: プロンプト | LLM | StrOutputParser ---
print("\n--- チェーン 2: プロンプト | LLM | StrOutputParser ---")
chain_full = prompt | llm | output_parser
print("   チェーン構築完了")

try:
    # チェーン1と同じ入力データを使用する
    print(f"\n   実行中... チェーンへの入力 (辞書形式): {input_data}")
    # こちらのチェーンも、最初の要素 (プロンプト) が必要とする辞書を入力とする
    response_string = chain_full.invoke(input_data)
    print("✅ OK: 応答を受信しました。")

    print("\n   【応答の型】:", type(response_string))
    print("   【応答 (文字列)】:")
    print("   ", response_string)
except Exception as e:
    # LLMの応答形式が予期せぬものだった場合などにエラーになる可能性もゼロではない
    print(f"❌ エラー: チェーン 2 の実行中にエラーが発生しました: {e}")


print("\n--- 処理終了 ---")
```

#### 実行結果の例

```text
--- 環境変数読み込み完了 ---
--- LLM準備完了: gpt-3.5-turbo (temperature=0.0) ---
   (temperature=0 は、毎回ほぼ同じ応答を返す設定です)
--- プロンプトテンプレート準備完了 ---
--- Output Parser (StrOutputParser) 準備完了 ---

--- チェーン 1: プロンプト | LLM ---
   チェーン構築完了

   実行中... チェーンへの入力 (辞書形式): {'country': 'フランス'}
✅ OK: 応答を受信しました。

   【応答の型】: <class 'langchain_core.messages.ai.AIMessage'>
   【応答 (AIMessage)】:
     AIMessage(content='フランスの首都はパリです。', response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 20, 'total_tokens': 33}, ...}, id='run-...')

   【応答の内容 (content)】:
     フランスの首都はパリです。

--- チェーン 2: プロンプト | LLM | StrOutputParser ---
   チェーン構築完了

   実行中... チェーンへの入力 (辞書形式): {'country': 'フランス'}
✅ OK: 応答を受信しました。

   【応答の型】: <class 'str'>
   【応答 (文字列)】:
     フランスの首都はパリです。

--- 処理終了 ---
```

- プロンプトがシンプルな質問応答になったことで、コードの意図と実行結果がより直接的に結びつきやすくなったかと思います。

---

### 4. 深掘り解説：仕組みをもっと詳しく知ろう

#### 🎯 目標

- LCEL がなぜこれほど柔軟にコンポーネントを接続できるのか、`Runnable` の持つ標準インターフェースの重要性を理解する。
- `StrOutputParser` と、より複雑な `StructuredOutputParser` の使い分けを理解する。

#### `Runnable` の標準インターフェース

LCEL の中心にある `Runnable` プロトコルは、各コンポーネントに共通の操作方法を提供します。これがパイプ `|` での接続を可能にしています。主要なメソッドを再確認しましょう。

- **`.invoke(input)`**: チェーン全体を**一回**実行し、最終結果を**一つ**返す、最も基本的な方法です。
- **`.stream(input)`**: LLM の応答などを**少しずつ**（トークンごとなど）受け取りたい場合に使います。タイプライターのように応答を表示するアプリケーションなどで役立ちます。（詳細な使い方は後のステップで学びます）

これらの標準インターフェースがあるおかげで、LCEL は個々のコンポーネントの詳細な実装を知らなくても、それらを統一的に扱ってチェーンを構築・実行できるのです。

#### `StrOutputParser` vs `StructuredOutputParser`

このステップでは `StrOutputParser` を使いました。これは非常にシンプルで、LLM の応答 (`AIMessage`) からテキスト部分 (`content`) を取り出して、ただの文字列 (`str`) に変換するだけです。今回の「首都はどこですか？」のような質問に対する単純な回答テキストを得るのに適しています。

一方、ステップ 4 で学んだ `StructuredOutputParser` は、もっと複雑なタスクに使います。LLM に対して「JSON 形式で答えて」のように指示し、その JSON 文字列を Python の辞書 (`dict`) など、プログラムで扱いやすい**構造化されたデータ**に変換したい場合に使用します。例えば、「国の名前、首都、人口を JSON で返して」といった場合です。

- **応答が単純なテキストで良い場合 → `StrOutputParser`**
- **応答を JSON や特定のデータ構造で扱いたい場合 → `StructuredOutputParser` や他の構造化パーサー (例: `PydanticOutputParser`)**

このように、目的に応じて適切な Output Parser をチェーンの最後に繋ぐことが重要です。

#### LCEL を使うメリット再確認

- **可読性向上:** 処理の流れが `|` で繋がり、コードが読みやすくなります。
- **保守性向上:** チェーンの一部を変更・修正するのが容易になります。
- **再利用性:** 作成したチェーン（`Runnable`）は、別のチェーンの一部として再利用できます。

#### チェーンがうまく動かないときは？

LCEL チェーンは強力ですが、コンポーネント間のデータの型が合わないなどの理由で、エラーが発生することもあります。例えば、LLM が予期せず `AIMessage` ではなくエラー情報を返したり、パーサーが期待する形式のデータを受け取れなかったりする場合です。

基本的な考え方としては、**問題を切り分ける**ことです。

1.  `prompt | llm` の部分だけで実行 (`.invoke()`) してみて、LLM から期待通りの応答 (`AIMessage`) が返ってきているか確認します。今回の例なら、「フランスの首都はパリです。」という内容のメッセージが返ってくるかを見ます。
2.  もし LLM の応答は正しいのに、パーサーを繋ぐとエラーになるなら、問題はパーサー部分にある可能性が高いです。（`StrOutputParser` はシンプルなのでエラーになりにくいですが、複雑なパーサーでは起こりえます）

このように、チェーンを短い部分に分けて動作確認することが、デバッグの第一歩となります。詳細なデバッグ方法は、次のステップで学びます。

---

### 5. 最終チェック：動作確認と問題解決

#### 🎯 目標

- 作成した LCEL チェーンが期待通りに動作し、基本的なエラーハンドリングについても理解する。

#### 確認してみよう

- **実行方法:** コマンドラインで `python step5_lcel_pipe_intro.py` を実行します。
- **確認パターン:**
  - エラーメッセージ（特に `❌ エラー:` で始まるもの）が表示されずに、最後まで実行できましたか？
  - コード内の `input_data` の `country` の値を別の国名（例: `"日本"` や `"イタリア"`）に変更して実行し、AI の応答が変わることを確認してみましょう。
  - チェーン 1 の出力が `AIMessage` で、チェーン 2 の出力が `str` であることを再確認しましょう。
- **エラーハンドリングについて:**
  - コード内の `try...except` ブロックは、プログラムが予期せぬエラーで完全に停止してしまうのを防ぐための基本的な仕組みです。
  - 特に `ChatOpenAI` の初期化時には、API キーの設定ミスなどが原因でエラーが起こりやすいです。そのため、初期化の段階でエラーチェックを入れています。
  - チェーン実行中のエラーは、LLM との通信問題や、コンポーネント間のデータ形式の不整合などが考えられますが、詳細な原因究明と対策は次のステップで扱います。このステップでは、まず「エラーが起きたらメッセージが表示される」ことを確認できれば OK です。

---

### 6. まとめ：学びの整理と次へのステップ

#### 🎯 目標

- LCEL の基本、`Runnable` の概念、パイプ `|` 演算子によるチェーン構築を確実に理解し、知識を定着させる。

#### ✅ 達成したこと

- LCEL を使うことで、LangChain コンポーネントの連携コードをシンプルに記述できることを理解した。
- `Runnable` が LangChain コンポーネントの共通インターフェースであり、パイプ `|` で接続できる理由を学んだ。
- パイプ演算子 `|` を使って、プロンプト (`ChatPromptTemplate`)、LLM (`ChatOpenAI`)、出力パーサー (`StrOutputParser`) を繋ぎ、LCEL チェーンを構築・実行できた。
- チェーンの最後のコンポーネントによって、`.invoke()` の戻り値の型が変わることを確認した (`AIMessage` vs `str`)。
- チェーンへの入力は、最初のコンポーネント（通常はプロンプト）が必要とする形式（多くは辞書）で渡すことを理解した。

#### 🔑 学んだキーワード

- `LCEL` (LangChain Expression Language)
- `Runnable` (プロトコル、インターフェース)
- `|` (パイプ演算子)
- `.invoke()` (同期実行メソッド)
- `.stream()` (ストリーミング用メソッド - 簡単な紹介)
- チェーン (Chain)
- `ChatPromptTemplate`
- `ChatOpenAI`
- `StrOutputParser` (from `langchain_core.output_parsers`)
- `temperature` (LLM パラメータ)

#### 🚀 次のステップへ

LCEL とパイプ `|` を使った基本的なチェーン構築ができるようになりましたね！ これで、より複雑な処理もエレガントに記述する準備が整いました。

しかし、実際にアプリケーションを開発していると、「チェーンの途中のデータはどうなっているんだろう？」「どこで処理が間違っているんだろう？」と、チェーンの内部を詳しく見たくなる場面が出てきます。

次の **ステップ 6「LCEL 入門(2): 基本チェーン構築とデバッグ」** では、今回構築したような基本的なチェーンをベースに、**チェーンの途中経過を確認する方法**や、**期待通りに動作しない場合のデバッグ**の基本的なテクニックを学びます。LCEL をさらに深く理解し、自在に操るためのスキルを身につけましょう！
