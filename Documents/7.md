AI が応答を生成するのを待つ時間は、時に長く感じられることがあります。特にインタラクティブなアプリケーションでは、応答がリアルタイムに表示される方が、ユーザー体験は格段に向上します。これを実現するのが**ストリーミング**技術です。

このステップでは、LangChain を使って LLM からの応答をストリーミングで受け取り、コンソールなどに逐次表示する方法を学びます。LCEL (LangChain Expression Language) で構築したチェーンに対し、`.stream()` や `.astream()` といったメソッドを活用します。

### 1. はじめに：このステップで目指すこと

#### 🎯 今回のゴール

- LLM の応答を待つのではなく、生成され次第、少しずつリアルタイムに受け取る「ストリーミング」の仕組みを理解し、実装できるようになる。
- **具体的な成果物:** 簡単な質問応答を行う LCEL チェーンを作成し、その応答を同期および非同期のストリーミングによってコンソールに逐次表示する Python プログラムを作成する。

#### 🔑 このステップのポイント

- **ストリーミング:** 応答データを一度にまとめてではなく、継続的な流れ（ストリーム）として部分的に送受信する技術。ユーザーの体感応答速度を向上させる。
- **LCEL (LangChain Expression Language):** LangChain のコンポーネント（プロンプト、モデル、パーサーなど）をパイプ `|` 演算子で接続し、処理フローを構築するための宣言的な方法。
- **`.stream()` と `.astream()`:** `Runnable` オブジェクト（LCEL チェーンなど）が持つ、ストリーミング処理を開始するためのメソッド（同期/非同期）。
- **チャンク (Chunk):** ストリーミング中に送られてくる、データの断片（この場合は応答テキストの一部など）。
- **`streaming=True`:** LLM (`ChatOpenAI` など) をストリーミングで使用する際に、初期化時に指定することが推奨されるパラメータ。

#### ✅ 前提知識

- ステップ 6 までの内容、特に LCEL を用いた基本的なチェーンの構築方法。
- Python の `for` ループ、`try...except` による基本的なエラーハンドリング。
- (任意) Python の `asyncio`, `async/await` に関する基本的な知識（非同期ストリーミング `.astream()` を利用する場合）。
- ステップ 1 で準備した開発環境。

---

### 2. 準備運動：ストリーミングの基本原理

#### 🎯 目標

- なぜストリーミングによって応答が逐次表示できるのか、その基本的な仕組みと、LangChain で利用する主要なメソッドの役割を理解する。

#### ストリーミングの仕組み：なぜリアルタイムに見えるのか？

通常、LCEL チェーンで `.invoke()` を呼び出すと、チェーン内の全処理（プロンプト生成、LLM 呼び出し、結果のパースなど）が完了し、最終的な結果がまとめて返されるまで待機します。

一方、`.stream()` や `.astream()` を使用すると、LLM が応答の生成を開始した時点から、生成された部分が「チャンク」と呼ばれる小さな断片として、次々にプログラムに送信されます。プログラムは、これらのチャンクを受け取るたびに画面に出力などの処理を行うため、ユーザーには応答がリアルタイムで生成・表示されているように見えます。これにより、応答全体の完了を待つ必要がなくなり、体感的な待ち時間が大幅に短縮されます。

#### ストリーミング用メソッド：`.stream()` と `.astream()`

LangChain のコアコンセプトである `Runnable` プロトコルを実装したオブジェクト（プロンプト、LLM、パーサー、そしてそれらを `|` で繋いだチェーン自体）は、ストリーミング用のメソッドを提供します。

- **`.stream(input, config=None)` (同期):**
  - このメソッドを呼び出すと、応答チャンクを一つずつ生成する**イテレータ**が返されます。
  - Python の標準的な `for` ループを使用して、これらのチャンクを順番に処理できます。
  - 実装が比較的容易で、シンプルなスクリプトや同期的な処理フローに適しています。
- **`.astream(input, config=None)` (非同期):**
  - このメソッドは `async def` で定義された非同期関数内で `await` と共に使用します。
  - 応答チャンクを一つずつ生成する**非同期ジェネレータ**を返します。
  - `async for` ループを使用してチャンクを処理します。
  - Web アプリケーションのバックエンド (FastAPI など) や GUI アプリケーションのように、ブロッキングせずに他の処理と並行してストリーミングを行いたい場合に不可欠です。Python の `asyncio` ライブラリを活用します。

#### 応答チャンク (Chunk) とは？

ストリーミング中に受け取る各チャンクは、応答データの一部です。チェーンの構成（特に最後の出力パーサー）によって、チャンクの内容は異なります。例えば、`StrOutputParser` を使っている場合はテキストの一部（文字列）になりますが、パーサーがない場合は `AIMessageChunk` のような、より詳細な情報を含むオブジェクトになることがあります。

---

### 3. 実践タイム：コードでストリーミングを実装する

#### 🎯 目標

- `.stream()` と `.astream()` を用いて LLM の応答を逐次表示する Python コードを作成し、その動作を確認する。

#### ファイルの準備

- 作業フォルダ内に `step7_streaming_practical.py` という名前で新しい Python ファイルを作成します。

#### ステップ・バイ・ステップ実装

1.  **必要なモジュールのインポートと準備:**
    `langchain_core` から標準的なコンポーネントをインポートし、LLM を `streaming=True` で初期化します。

    ```python
    # step7_streaming_practical.py
    import os
    import asyncio
    from dotenv import load_dotenv

    # --- LangChain Core Modules ---
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import StrOutputParser

    # --- LangChain OpenAI Integration ---
    from langchain_openai import ChatOpenAI

    # 環境変数の読み込み
    load_dotenv()
    print("--- 環境変数読み込み完了 ---")

    # LLMの準備 (streaming=True を指定)
    try:
        # streaming=True を指定して、ストリーミングに適した応答を促す
        llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7, streaming=True)
        print(f"--- LLM準備完了: {llm.model_name} (temperature={llm.temperature}, streaming=True) ---")
    except Exception as e:
        print(f"❌ エラー: ChatOpenAI の初期化に失敗しました: {e}")
        print("   (APIキー、アカウント設定、ライブラリのバージョンを確認してください)")
        exit()

    # プロンプトテンプレートの準備
    prompt = ChatPromptTemplate.from_template(
        "{topic} について、その魅力と将来性を３つのポイントで解説してください。"
    )
    print("--- プロンプトテンプレート準備完了 ---")

    # 出力パーサーの準備 (LLM応答を文字列に変換)
    output_parser = StrOutputParser()
    print("--- Output Parser (StrOutputParser) 準備完了 ---")

    # LCEL チェーンの構築
    chain = prompt | llm | output_parser
    print("--- LCEL チェーン構築完了 ---")
    ```

    - `temperature` を少し上げて (例: 0.7)、毎回少し違う応答が生成されるようにしています。

2.  **同期ストリーミング `.stream()` の実装:**
    `for` ループを用いてチャンクを受け取り、`print()` で逐次出力します。

    ```python
    # step7_streaming_practical.py (続き)
    print("\n--- 同期ストリーミング (.stream) 開始 ---")
    topic_sync = "再生可能エネルギー"
    print(f"> トピック: {topic_sync}")
    print("AI応答:")

    try:
        full_response_sync = ""
        # .stream() は同期イテレータを返す
        for chunk in chain.stream({"topic": topic_sync}):
            # 受け取ったチャンクを改行せずに出力 (flush=Trueで即時表示)
            print(chunk, end="", flush=True)
            full_response_sync += chunk # 完全な応答を後で確認するために結合
        print("\n--- .stream() 完了 ---")
        # print("\n[デバッグ] 結合された応答(同期):", full_response_sync) # 必要に応じて確認
    except Exception as e:
        print(f"\n❌ エラー: 同期ストリーミング中にエラーが発生しました: {e}")
        print("   (ネットワーク接続、APIキー、利用制限などを確認してください)")

    ```

3.  **非同期ストリーミング `.astream()` の実装:**
    `async def` 関数内で `async for` を使用し、`asyncio.run()` で実行します。

    ```python
    # step7_streaming_practical.py (続き)
    # 非同期ストリーミング用の関数を定義
    async def run_async_streaming():
        print("\n--- 非同期ストリーミング (.astream) 開始 ---")
        topic_async = "人工知能と創造性"
        print(f"> トピック: {topic_async}")
        print("AI応答:")

        try:
            full_response_async = ""
            # .astream() は非同期ジェネレータを返す
            async for chunk in chain.astream({"topic": topic_async}):
                # 受け取ったチャンクを改行せずに出力
                print(chunk, end="", flush=True)
                full_response_async += chunk # 完全な応答を後で確認するために結合
            print("\n--- .astream() 完了 ---")
            # print("\n[デバッグ] 結合された応答(非同期):", full_response_async) # 必要に応じて確認
        except Exception as e:
            print(f"\n❌ エラー: 非同期ストリーミング中にエラーが発生しました: {e}")
            print("   (ネットワーク接続、APIキー、利用制限などを確認してください)")

    # スクリプトとして直接実行された場合に非同期関数を呼び出す
    if __name__ == "__main__":
        print("\n--- 非同期処理実行 ---")
        try:
            # asyncio.run() を使って非同期関数を実行
            asyncio.run(run_async_streaming())
        except RuntimeError as e:
            # Jupyter Notebook など、既にイベントループが実行中の環境への対応
            try:
                loop = asyncio.get_running_loop()
                if loop.is_running():
                    print("\n注意: 既存のイベントループが検出されました。")
                    print("      Jupyter Notebookなどの環境では、セルで `await run_async_streaming()` を直接実行してください。")
                else:
                    # ループはあるが実行中でない場合 (通常は考えにくいが念のため)
                     print(f"イベントループは存在しますが、実行中ではありません。: {e}")
            except RuntimeError:
                 # get_running_loop() 自体がエラーを出す場合 (ループが存在しない)
                 print(f"実行中のイベントループが見つかりませんでした。: {e}")
        except Exception as e:
            print(f"非同期処理の実行中に予期せぬエラーが発生しました: {e}")

    print("\n--- 全ての処理が終了しました ---")
    ```

#### 完成コード (`step7_streaming_practical.py`)

```python
# step7_streaming_practical.py
import os
import asyncio
from dotenv import load_dotenv

# --- LangChain Core Modules ---
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# --- LangChain OpenAI Integration ---
from langchain_openai import ChatOpenAI

# 環境変数の読み込み
load_dotenv()
print("--- 環境変数読み込み完了 ---")

# LLMの準備 (streaming=True を指定)
try:
    # streaming=True を指定して、ストリーミングに適した応答を促す
    # temperature を少し上げて多様な応答を生成させる
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7, streaming=True)
    print(f"--- LLM準備完了: {llm.model_name} (temperature={llm.temperature}, streaming=True) ---")
except Exception as e:
    print(f"❌ エラー: ChatOpenAI の初期化に失敗しました: {e}")
    print("   (APIキー、アカウント設定、ライブラリのバージョンを確認してください)")
    exit()

# プロンプトテンプレートの準備
prompt = ChatPromptTemplate.from_template(
    "{topic} について、その魅力と将来性を３つのポイントで解説してください。"
)
print("--- プロンプトテンプレート準備完了 ---")

# 出力パーサーの準備 (LLM応答を文字列に変換)
output_parser = StrOutputParser()
print("--- Output Parser (StrOutputParser) 準備完了 ---")

# LCEL チェーンの構築
chain = prompt | llm | output_parser
print("--- LCEL チェーン構築完了 ---")

# --- 同期ストリーミング (.stream) ---
print("\n--- 同期ストリーミング (.stream) 開始 ---")
topic_sync = "再生可能エネルギー"
print(f"> トピック: {topic_sync}")
print("AI応答:")
try:
    full_response_sync = ""
    # .stream() は同期イテレータを返す
    for chunk in chain.stream({"topic": topic_sync}):
        # 受け取ったチャンクを改行せずに出力 (flush=Trueで即時表示)
        print(chunk, end="", flush=True)
        full_response_sync += chunk # 完全な応答を後で確認するために結合
    print("\n--- .stream() 完了 ---")
    # print("\n[デバッグ] 結合された応答(同期):", full_response_sync) # 必要に応じて確認
except Exception as e:
    print(f"\n❌ エラー: 同期ストリーミング中にエラーが発生しました: {e}")
    print("   (ネットワーク接続、APIキー、利用制限などを確認してください)")


# --- 非同期ストリーミング (.astream) ---
# 非同期ストリーミング用の関数を定義
async def run_async_streaming():
    print("\n--- 非同期ストリーミング (.astream) 開始 ---")
    topic_async = "人工知能と創造性"
    print(f"> トピック: {topic_async}")
    print("AI応答:")
    try:
        full_response_async = ""
        # .astream() は非同期ジェネレータを返す
        async for chunk in chain.astream({"topic": topic_async}):
            # 受け取ったチャンクを改行せずに出力
            print(chunk, end="", flush=True)
            full_response_async += chunk # 完全な応答を後で確認するために結合
        print("\n--- .astream() 完了 ---")
        # print("\n[デバッグ] 結合された応答(非同期):", full_response_async) # 必要に応じて確認
    except Exception as e:
        print(f"\n❌ エラー: 非同期ストリーミング中にエラーが発生しました: {e}")
        print("   (ネットワーク接続、APIキー、利用制限などを確認してください)")

# スクリプトとして直接実行された場合に非同期関数を呼び出す
if __name__ == "__main__":
    print("\n--- 非同期処理実行 ---")
    try:
        # asyncio.run() を使って非同期関数を実行
        asyncio.run(run_async_streaming())
    except RuntimeError as e:
        # Jupyter Notebook など、既にイベントループが実行中の環境への対応
        try:
            loop = asyncio.get_running_loop()
            if loop.is_running():
                 print("\n注意: 既存のイベントループが検出されました。")
                 print("      Jupyter Notebookなどの環境では、セルで `await run_async_streaming()` を直接実行してください。")
            else:
                 print(f"イベントループは存在しますが、実行中ではありません。: {e}")
        except RuntimeError:
             print(f"実行中のイベントループが見つかりませんでした。: {e}")
    except Exception as e:
        print(f"非同期処理の実行中に予期せぬエラーが発生しました: {e}")


print("\n--- 全ての処理が終了しました ---")
```

#### 実行結果のイメージ

コマンドラインで `python step7_streaming_practical.py` を実行すると、"AI 応答:" の後に、指定したトピックに関する解説が**少しずつ表示されていく**様子が確認できるはずです。`temperature=0.7` に設定したため、実行ごとに応答内容が多少変化します。

---

### 4. 深掘り解説：ストリーミングの特性と注意点

#### 🎯 目標

- 同期・非同期の使い分け、チャンクのデータ形式、パフォーマンスへの影響、エラーハンドリングなど、ストリーミングに関する理解を深める。

#### 同期 `.stream()` vs 非同期 `.astream()`：ユースケースに応じた選択

- **同期 `.stream()`:** 実装がシンプル。単一の処理を順番に行う CLI ツールや簡単なスクリプトに適しています。ただし、ストリーミング中に他の処理（特に I/O 待ちが発生するもの）を行うと、全体の実行がブロックされる可能性があります。
- **非同期 `.astream()`:** Web フレームワーク (FastAPI, Django Async, Streamlit など) や GUI アプリケーション、複数のネットワークリクエストを並行して処理するような場面で推奨されます。`asyncio` により、I/O 待ち時間を有効活用し、アプリケーション全体の応答性を高めることができます。

#### チャンクの形式と処理

- **出力パーサーの影響:** チェーンの最後に出力パーサーがあるか、どのパーサーを使うかによって、受け取るチャンクの形式が変わります。`StrOutputParser` ならば文字列 (`str`) の一部が返されることが多いですが、パーサーがない場合や他のパーサー（例: `JsonOutputParser`）の場合、チャンクは特定のオブジェクト (`AIMessageChunk`, `JsonDelta`) になり、そのオブジェクトの属性 (`.content` や差分情報など) にアクセスする必要があります。
- **モデルや API の影響:** 使用する LLM モデルや API によっても、チャンクの分割単位や送信頻度は異なります。予期した形式でない場合は、モデルのドキュメントも参照してください。
- **チャンクの結合:** ストリーミングで受け取ったチャンクは、それ自体が完全な意味を持つとは限りません（単語の途中で分割されることもあります）。完全な応答を得るには、受け取ったチャンクを順番に結合する必要があります。

#### パフォーマンスに関する考慮事項

ストリーミングは、ユーザーが最初の応答を目にするまでの時間 (Time To First Byte) を短縮し、**体感的な応答性を劇的に改善**します。しかし、応答全体の生成が完了するまでの**総時間 (Total Latency) は、`.invoke()` を使った場合と比べて短縮される保証はなく**、むしろ通信オーバーヘッドなどによりわずかに増加する可能性もあります。ストリーミングの主な利点は、応答性の向上にあると理解しましょう。

#### エラーハンドリング

ストリーミング処理はネットワーク通信に依存するため、接続の瞬断、API サーバー側の問題、レート制限超過など、様々な要因で中断される可能性があります。`try...except` ブロックで適切にエラーを捕捉し、必要に応じて以下のような対応を検討することが重要です。

- エラー情報のログ記録
- ユーザーへのフィードバック（例: 「応答の生成中にエラーが発生しました」）
- 指数バックオフを用いた自動リトライ処理の実装

---

### 5. 最終チェック：動作確認とトラブルシューティング

#### 🎯 目標

- 実装したストリーミング処理が正しく機能しているかを確認し、問題が発生した場合の基本的な対処法を理解する。

### 確認してみよう！

1.  **コードの実行:** エラーなくスクリプトが最後まで実行されましたか？
2.  **LLM 設定:** `ChatOpenAI` の初期化時に `streaming=True` が指定されていることを再確認してください。
3.  **逐次表示:** `.stream()` と `.astream()` の両方で、"AI 応答:" の後にテキストが一気に表示されるのではなく、**文字が少しずつ表示される**ことを確認しましたか？
4.  **非同期実行環境:** もし Jupyter Notebook などで `asyncio.run()` がエラーになった場合、コメントの指示に従って `await run_async_streaming()` をセルで直接実行し、動作を確認できましたか？
5.  **エラー発生時:** （意図的に API キーを間違えるなどして）エラーが発生した場合、`except` ブロックで捕捉され、エラーメッセージが表示されることを確認してみましょう。

---

### 6. まとめ：学びの整理と次へのステップ

#### ✅ 達成したこと！

- LangChain で LLM 応答をストリーミングする基本的な方法を理解し、実装できた。
- 同期 (`.stream()`) と非同期 (`.astream()`) のストリーミングメソッドの使い分けを学んだ。
- LLM 初期化時の `streaming=True` パラメータの重要性を理解した。
- 応答チャンクの基本的な扱い方と、パーサーやモデルによる影響について学んだ。
- ストリーミングのパフォーマンス特性（体感応答性の向上）とエラーハンドリングの必要性を認識した。

#### 🔑 学んだキーワード

- ストリーミング (Streaming), チャンク (Chunk)
- `.stream()`, `.astream()`
- `streaming=True`
- LCEL (LangChain Expression Language)
- 同期 / 非同期 (`asyncio`, `async/await`)
- イテレータ / 非同期ジェネレータ
- `langchain_core` (標準モジュール)
- `StrOutputParser`
- エラーハンドリング (`try...except`)

#### 🚀 次のステップへ！

応答の逐次表示によって、AI アプリケーションがよりインタラクティブになりましたね！

しかし、これまでの対話では、AI は過去の発言を覚えていませんでした。実際の会話では、前の文脈を踏まえた応答が自然です。

次の **ステップ 8「ボットに記憶力を！会話を覚えてもらおう」** では、LangChain の**メモリ (Memory)** 機能を探求します。これを使うことで、AI に会話の履歴を記憶させ、より人間らしく、文脈に沿った対話が可能なチャットボットの構築へと進んでいきます。お楽しみに！
