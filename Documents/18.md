前のステップで、基本的な **RAG チェーン** を組み立てることに成功しましたね！ Retriever が見つけてきた Context を使って、LLM が質問に答える流れを作れました。素晴らしい第一歩です！

でも、「もっと検索の精度を上げたいな…」「ユーザーの質問がちょっとフワッとしてても、ちゃんと関連情報を見つけられるようにしたい！」と思いませんか？

今回のステップでは、そんな願いを叶えるための一つの方法、**MultiQueryRetriever** を紹介します！ これは、あなたの質問を AI が「こんな聞き方もあるかも？」「この点の質問もあるよね？」と、いくつかの異なる質問に言い換えてくれて、それら全部で情報を探し、結果を賢くまとめてくれる、検索能力をグンと引き上げるための Retriever なんです。より良い Context を見つけるための新しいテクニックを学びましょう！

## 1. はじめに：このステップで目指すこと

### 🎯 今回のゴール

このステップを最後まで体験すると、あなたはこんなことができるようになります！

- なぜ単一の質問での検索だけでは不十分な場合があるのか、その理由を理解します。
- LLM を活用して検索クエリを複数生成し、検索の網羅性を高める **MultiQueryRetriever** の考え方と動作原理を学びます。
- LangChain を使って **MultiQueryRetriever** をセットアップし、既存の RAG チェーンに組み込む方法を習得します。
- **具体的な成果物:** ステップ 17 の RAG チェーンで使っていた Retriever を **MultiQueryRetriever** に置き換え、同じ質問を投げかけた時に、より網羅的な情報に基づいた回答が得られる可能性があることを確認する Python コードを作成します。

### 🔑 このステップのポイント

今回の検索パワーアップ作戦で重要なキーワードはこちら！

- **MultiQueryRetriever:** 1 つの質問から複数の質問を生成して検索する、賢い Retriever。
- **検索改善:** RAG システムの検索精度や「見逃しにくさ」（網羅性）を高める工夫。
- **クエリ生成 (Query Generation):** LLM が元の質問から新しい質問バリエーションを作り出すこと。
- **結果の統合:** 複数の検索結果を賢くマージして重複をなくすこと。
- **トレードオフ:** 検索品質の向上と、処理時間・API コスト増のバランス感覚。
- **RAG チェーンへの統合:** 作った Retriever を実際の処理フローに組み込むこと。

### ✅ 前提知識

この新しい検索術を身につける前に、以下の準備が整っているか確認しましょう！

- **ステップ 17「RAG 構築(2): Context 注入と LLM 生成」の内容:** Retriever、プロンプト、LLM、パーサーを繋いだ基本的な RAG チェーンを LCEL で構築できること。
- **FAISS Vector Store と Retriever:** ステップ 15, 16 で準備した FAISS ベースの Vector Store と、そこから作成した基本的な Retriever があること。
- **LLM:** 質問生成と最終的な回答生成に使う LLM (`ChatOpenAI` など) が準備できていること。
- **Python の基本:** クラスのインスタンス化、メソッド呼び出しなど。
- **必要なライブラリ:** `langchain`, `langchain-openai`, `langchain-community`, `faiss-cpu` (または `gpu`) などがインストール済みで、API キーも設定済みであること。

準備が OK なら、MultiQueryRetriever の力を体験しに行きましょう！

---

## 2. 準備運動：なぜ複数の質問で検索するの？

### 🎯 目標

単純な検索の限界を理解し、**MultiQueryRetriever** がどのようにして、より多くの関連情報を見つけ出す可能性を高めるのか、そのアイデアと仕組みを掴みましょう。

### 一つの聞き方だけだと、限界がある？

ユーザーが知りたいことは一つでも、それを表現する方法はたくさんありますよね。また、一つの質問の中に、実は複数の要素が含まれていることもよくあります。

例えば、「LangChain のエージェント機能の概要と、メモリの役割について教えて」という質問。これは「エージェントとは？」と「メモリの役割は？」という二つのポイントを含んでいます。もし、Vector Store 内の文書が「エージェントについて」「メモリについて」と別々に書かれていたら、元の質問そのままの類似検索では、どちらか片方の情報しかうまく引っかからないかもしれません。

### MultiQueryRetriever の戦略：質問を増やして、広く深く探す！

**MultiQueryRetriever** は、この問題を解決するために、次のようなステップで動きます。まるで、優秀なアシスタントが質問を練り直してくれるようです！

1.  **質問を分析 & 分解/言い換え:** ユーザーの質問を受け取ると、まず内部の **LLM** に相談します。「この質問、もっと違う聞き方はできる？」「この質問って、具体的にはどんな点について聞いてるんだろう？」という感じです。
2.  **複数の「検索用」質問を生成:** LLM は、元の質問から、視点を変えた質問、より具体的な質問、あるいは質問を分解したサブ質問などをいくつか作り出します。
3.  **それぞれの質問で検索実行！:** 生成された複数の質問を使って、元々用意されていた **ベースの Retriever** (FAISS などを使っているもの) に、一つ一つ検索を依頼します。「この質問でも探して！」「こっちの質問でも！」という具合です。
4.  **結果を集めて、整理整頓:** すべての検索で見つかった文書 (**Document** リスト) をいったん全部集めます。そして、もし同じ文書が複数回見つかっていたら、重複をきちんと取り除いて（通常は文書の内容で判断します）、ユニークな文書のリストにして返してくれます。

この方法によって、元の質問だけでは届かなかったかもしれない情報にもアクセスできる可能性が高まり、結果として、より**網羅的**で質の高い Context を集めることが期待できるわけです！

### 必要なもの

MultiQueryRetriever を使うために用意するものは、主に以下の 2 つです。

- **ベースとなる Retriever:** 実際に Vector Store に検索をかける担当 (ステップ 16 で作ったもの)。
- **質問生成用の LLM:** 質問を言い換える役割を担当 (ステップ 17 で使ったものと同じで大丈夫です)。

---

## 3. 実践タイム：MultiQueryRetriever を RAG チェーンに組み込もう！

### 🎯 目標

さあ、いよいよ実践です！ステップ 17 で作った RAG チェーンの Retriever 部分を、今学んだ **MultiQueryRetriever** に置き換えて、実際に動かしてみましょう。生成される質問もログで確認してみます。

### ステップ・バイ・ステップ実装

#### 1. 準備 (インポート、既存セットアップの流用、ロギング設定):

まず、`MultiQueryRetriever` をインポートします。そして、ステップ 17 で使った LLM、Embedding モデル、FAISS Vector Store、ベースの Retriever を準備（またはコードを流用）します。今回は、LLM がどんな質問を生成したかを見るために、ロギング設定も追加しましょう。

```python
# step18_rag_multi_query_integrated.py
import os
import sys
import logging # ★ 生成されたクエリを見るために追加
from dotenv import load_dotenv

# --- 必要な LangChain モジュール ---
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document
from langchain_core.runnables import RunnablePassthrough, RunnableLambda

# MultiQueryRetriever をインポート (パスは環境により変わる可能性あり)
# langchain 本体からのインポートが一般的
try:
    from langchain.retrievers.multi_query import MultiQueryRetriever
    print("MultiQueryRetriever をインポートしました (from langchain.retrievers.multi_query)。")
except ImportError:
    # もし上記でエラーが出る場合は community を試す
    try:
        from langchain_community.retrievers.multi_query import MultiQueryRetriever
        print("MultiQueryRetriever をインポートしました (from langchain_community.retrievers.multi_query)。")
        print("   'pip install langchain-community' が必要かもしれません。")
    except ImportError:
        print("エラー: MultiQueryRetriever が見つかりません。")
        print("   'pip install langchain' を確認してください。")
        sys.exit(1)


# Vector Store (FAISS)
try:
    from langchain_community.vectorstores import FAISS
    import faiss
    print("FAISS をインポートしました。")
except ImportError:
    print("エラー: FAISS または langchain-community が見つかりません。")
    sys.exit(1)

print("--- 必要なモジュールのインポート完了 ---")

# --- ★ ロギング設定 (INFO レベルで生成クエリを表示) ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logging.getLogger("langchain.retrievers.multi_query").setLevel(logging.INFO)
print("--- ロギング設定完了 ---")

# --- ステップ 17 と同様の準備 ---
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    print("エラー: OpenAI API キーが設定されていません。")
    sys.exit(1)
else:
    print("OpenAI API キーを読み込みました。")

try:
    embeddings_model = OpenAIEmbeddings(model="text-embedding-3-small", api_key=openai_api_key)
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0, api_key=openai_api_key) # クエリ生成と最終回答生成の両方に使う
    print(f"--- Embedding モデルと LLM 準備完了 ---")
except Exception as e:
    print(f"エラー: モデルの初期化に失敗しました: {e}")
    sys.exit(1)

# Vector Store とベース Retriever の準備
sample_documents = [
    Document(page_content="LangChain は LLM アプリケーション開発を支援するフレームワークです。主要な機能に LCEL があります。", metadata={"source": "doc-b", "page": 1, "category": "core"}),
    Document(page_content="エージェントはツールを使って外部システムと対話し、タスクを実行します。", metadata={"source": "doc-c", "page": 1, "category": "agent"}),
    Document(page_content="LangGraph は状態を持つ循環可能なグラフを構築し、複雑なエージェントランタイムを作成できます。", metadata={"source": "doc-a", "page": 1, "category": "graph"}),
    Document(page_content="LangChain のメモリ機能を使うと、チャットボットが会話履歴を記憶できます。", metadata={"source": "doc-g", "page": 1, "category": "memory"}),
    Document(page_content="プロンプトテンプレートは、LLM への入力を動的に生成するための仕組みです。", metadata={"source": "doc-f", "page": 1, "category": "core"}),
    Document(page_content="ベクトルストア (FAISS など) は、テキスト埋め込みの高速な類似検索を可能にします。", metadata={"source": "doc-d", "page": 1, "category": "vectorstore"}),
]
try:
    vector_store = FAISS.from_documents(sample_documents, embeddings_model)
    base_retriever = vector_store.as_retriever(search_kwargs={'k': 2}) # ベースは上位2件
    print("--- Vector Store (FAISS) とベース Retriever 準備完了 (k=2) ---")
except Exception as e:
    print(f"エラー: Vector Store またはベース Retriever の準備中にエラーが発生しました: {e}")
    sys.exit(1)

# Context フォーマット関数 (ステップ 17 と同じ)
def format_docs(docs: list[Document]) -> str:
    return "\n\n".join(doc.page_content for doc in docs)
format_docs_runnable = RunnableLambda(format_docs)
print("--- Context フォーマット関数準備完了 ---")

```

- `MultiQueryRetriever` のインポートパスを確認し、追加しました (ここでは `langchain.retrievers.multi_query` を試しています)。
- ロギング設定を追加して、INFO レベルのログ（生成されたクエリが含まれる）が表示されるようにしました。
- ベースとなる Retriever (`base_retriever`) も準備します。

#### 2. MultiQueryRetriever の作成:

準備したベース Retriever と LLM を使って、`MultiQueryRetriever` を作成します。

```python
# step18_rag_multi_query_integrated.py (続き)

print("\n--- MultiQueryRetriever の作成 ---")
try:
    # MultiQueryRetriever を作成
    multi_query_retriever = MultiQueryRetriever.from_llm(
        retriever=base_retriever,
        llm=llm
    )
    print("MultiQueryRetriever 作成完了！")

except Exception as e:
    print(f"エラー: MultiQueryRetriever の作成中にエラーが発生しました: {e}")
    sys.exit(1)
```

#### 3. RAG チェーンの構築 (Retriever を置き換え):

ステップ 17 で作成した RAG チェーンの構造をベースに、Retriever の部分を今回作成した `multi_query_retriever` に置き換えます。

```python
# step18_rag_multi_query_integrated.py (続き)

print("\n--- MultiQueryRetriever を使った RAG チェーンの組み立て ---")

# RAG プロンプト (ステップ 17 と同じ)
template = """
以下の Context のみを基にして、最後の Question に答えてください。

Context:
{context}

Question: {question}
"""
rag_prompt = ChatPromptTemplate.from_template(template)

# チェーンを組み立てる (Retriever 部分が変わる！)
rag_chain_multi_query = (
    # 入力は {"question": "..."}
    RunnablePassthrough.assign(
        # context の取得に multi_query_retriever を使う！
        context=lambda x: format_docs_runnable.invoke(multi_query_retriever.invoke(x["question"]))
    )
    | rag_prompt
    | llm
    | StrOutputParser()
)

print("--- MultiQueryRetriever を使った RAG チェーン組み立て完了 ---")

```

- ステップ 17 のチェーン定義 `rag_chain = ...` の部分で、`retriever.invoke` を `multi_query_retriever.invoke` に変更しただけです！ LCEL のおかげで、部品の交換がとても簡単ですね。

#### 4. チェーンの実行と比較:

元の質問を使って、新しい RAG チェーン (`rag_chain_multi_query`) を実行し、その結果を見てみましょう。ベースの Retriever だけを使った場合の結果と比較すると面白いかもしれません。

```python
# step18_rag_multi_query_integrated.py (続き)

# --- チェーンの実行 ---
# 質問 (複数の要素を含む可能性のある質問)
question = "LangChain のエージェントとメモリについて教えて"
print(f"\n--- MultiQuery RAG チェーン実行 (質問: '{question}') ---")

try:
    # チェーンを実行！ 内部でクエリ生成のログが出るはず
    final_answer = rag_chain_multi_query.invoke({"question": question})

    print("\n--- 最終的な回答 (MultiQuery) ---")
    print(final_answer)

    # (比較用：もしベースRetrieverのチェーンも実行したい場合)
    # print("\n--- (比較) ベース RAG チェーン実行 ---")
    # rag_chain_base = (
    #     RunnablePassthrough.assign(
    #         context=lambda x: format_docs_runnable.invoke(base_retriever.invoke(x["question"]))
    #     ) | rag_prompt | llm | StrOutputParser()
    # )
    # base_answer = rag_chain_base.invoke({"question": question})
    # print("\n--- 最終的な回答 (ベース) ---")
    # print(base_answer)

except Exception as e:
    print(f"エラー: RAG チェーンの実行中にエラーが発生しました: {e}")

print("\n--- 処理終了 ---")
```

- `.invoke()` を実行すると、まず内部で LLM が複数のクエリを生成し（ログに出力されます）、それらを使って `base_retriever` が検索を実行し、結果が統合されて Context となり、最終的な回答が生成されます。

### 完成コード (`step18_rag_multi_query_integrated.py`)

上記の実装 1〜4 を結合し、必要な準備コードを追加したものが、このステップの完成コードです。

### 実行結果の例

```text
# ...(初期化ログ)...
--- Vector Store (FAISS) とベース Retriever 準備完了 (k=2) ---
--- Context フォーマット関数準備完了 ---

--- MultiQueryRetriever の作成 ---
MultiQueryRetriever 作成完了！

--- MultiQueryRetriever を使った RAG チェーンの組み立て ---
--- MultiQueryRetriever を使った RAG チェーン組み立て完了 ---

--- MultiQuery RAG チェーン実行 (質問: 'LangChain のエージェントとメモリについて教えて') ---
# --- ここに INFO レベルのログが表示されるはず ---
# 例: INFO:langchain.retrievers.multi_query:Generated queries: ["LangChainのエージェントとは何ですか？", "LangChainのメモリ機能はどのように機能しますか？", "LangChainのエージェントとメモリの関係は何ですか？"]

--- 最終的な回答 (MultiQuery) ---
LangChainのエージェントは、ツールを使って外部システムと対話し、タスクを実行するコンポーネントです。一方、メモリ機能は、チャットボットが会話履歴を記憶できるようにするためのものです。これらを組み合わせることで、過去の対話を踏まえた上でツールを利用する、より高度なエージェントを構築できます。

--- 処理終了 ---
```

- 実行ログで、LLM が「エージェントとは？」「メモリの機能は？」といった複数の質問を生成しているのが分かります。
- 最終的な回答を見ると、エージェントに関する文書とメモリに関する文書の両方の Context が使われていることが推測できますね！

---

## 4. 深掘り解説：MultiQueryRetriever をもっと知る！

### 🎯 目標

MultiQueryRetriever がどのように機能するのか、その内部的な動きと、利用する上でのメリット・デメリット、そしてカスタマイズの可能性について理解を深めます。

### 内部の仕組み：LLM による質問生成から統合まで

MultiQueryRetriever の内部では、以下のような処理が行われています。

1.  **クエリ生成 LLM コール:** ユーザーの質問を受け取ると、設定された LLM に対して「この質問を別の視点から言い換えてみて」といった内部的なプロンプトを使って、複数のクエリ案を生成させます。（**追加の LLM API コールが発生します！**）
2.  **ベース Retriever による検索:** 生成された各クエリ（＋元のクエリ）を使って、指定されたベース Retriever (`base_retriever`) に検索を依頼します。（**複数回の検索が発生します！**）
3.  **結果の収集と重複排除:** すべての検索結果を集め、内容が同じ **Document** があれば重複を取り除きます。これにより、ユニークな関連文書のリストが得られます。

### メリットとデメリット（トレードオフ）

- **メリット:**
  - **網羅性の向上:** 元の質問だけでは見つけられなかった関連文書を発見できる可能性が高まります。曖昧な質問や多角的な質問に強いです。
  - **検索品質の向上:** より多様な情報（Context）を LLM に提供できるため、最終的な回答の質が向上することが期待できます。
- **デメリット:**
  - **応答時間の増加（レイテンシ）:** クエリ生成のための LLM 呼び出しと、複数回の検索実行のため、応答までにかかる時間が長くなります。
  - **コストの増加:** LLM API (クエリ生成) と Embedding API (検索時のクエリベクトル化) の呼び出し回数が増えるため、コストが増加します。例えば、3 つのクエリが生成されれば、単純計算でコストは約 3 倍以上になる可能性があります。
  - **ノイズ混入の可能性:** 生成されたクエリによっては、意図しない関連性の低い文書が検索結果に含まれる可能性もあります。

### いつ使うべき？

MultiQueryRetriever は強力ですが、常に最良とは限りません。

- **効果的な場面:** ユーザーの質問が多様、曖昧、または複数のトピックにまたがる場合に特に有効です。検索結果の見逃しを減らしたい場合にも役立ちます。
- **注意が必要な場面:** 応答速度が非常に重要視されるアプリケーションや、API コストを厳しく抑えたい場合には、レイテンシとコストの増加が問題になる可能性があります。単純で明確な質問が多い場合は、オーバーヘッドの方が大きくなるかもしれません。

### カスタマイズについて

- **生成されるクエリ数:** デフォルトでは通常 3 つ程度のクエリが生成されますが、この数を調整する方法は（現時点での LangChain の実装では）直接的なパラメータとしては提供されていないようです。しかし、内部で使われる **クエリ生成用プロンプト** をカスタマイズすることで、生成されるクエリの質や数を間接的にコントロールすることが可能です。`MultiQueryRetriever.from_llm()` の `prompt` 引数に自作のプロンプトを渡すことで実現できます（これは応用的な使い方です）。
- **ベース Retriever の設定:** `base_retriever` の `k` の値を調整することも、最終的な結果に影響します。

### メモリ使用量

複数の検索結果を一時的に保持するため、ベースの Retriever のみを使う場合と比較して、わずかにメモリ使用量が増加する可能性がありますが、通常はレイテンシや API コストほどの大きな影響はありません。

---

## 5. 最終チェック：検索は改善された？

### 🎯 目標

作成したコードが正しく動作し、MultiQueryRetriever が期待通りに機能しているかを確認します。

### 確認してみましょう！

- **実行！:** `step18_rag_multi_query_integrated.py` を実行してください。（必要なライブラリと API キーの設定を確認）
- **エラーなし？:** エラーメッセージが表示されずに最後まで実行できましたか？
- **クエリ生成ログ:** コンソールに INFO レベルで `Generated queries: [...]` のようなログが表示され、LLM によって複数の質問が生成されていることを確認できましたか？
- **最終回答の比較:**
  - もし比較用にベース RAG チェーンも動かした場合、MultiQueryRetriever を使ったチェーンの回答の方が、より多くの情報（例えばエージェントとメモリの両方）に触れている、あるいは質問の意図をより良く捉えているように感じられますか？（質問や Context によって効果は変わります）
  - `question` 変数の内容を、他の少し複雑そうな質問に変えて実行してみると、MultiQueryRetriever の効果がより分かりやすいかもしれません。

これらの点が確認できれば、MultiQueryRetriever を使った検索改善の基本はマスターです！

---

## 6. まとめ：学びの整理と次へのステップ

### ✅ 達成したこと！

これで、RAG システムの検索能力を向上させるための強力なテクニックの一つ、MultiQueryRetriever を使えるようになりました！

- 単一クエリ検索の限界と、複数クエリで検索する **MultiQueryRetriever** の有効性を理解しました。
- LLM を使って質問を自動生成し、複数の検索結果を統合する仕組みを学びました。
- LangChain で **MultiQueryRetriever** を設定し、ベースとなる Retriever と組み合わせて RAG チェーンに統合する方法を実装しました。
- 検索の網羅性向上というメリットと、レイテンシ・コスト増加というトレードオフがあることを認識しました。
- クエリ生成のカスタマイズの可能性や、適切なユースケースについても触れました。

### 🔑 学んだキーワード

- **MultiQueryRetriever** (`langchain.retrievers.multi_query` より)
- **検索改善 (Search Improvement)**
- **クエリ生成 (Query Generation)**
- **LLM (クエリ生成用)**
- **結果の統合・重複排除 (Result Aggregation & Deduplication)**
- **網羅性 (Recall)**
- **レイテンシ・コスト (Latency / Cost Trade-off)**
- **RAG チェーンへの統合**
- **ロギング (Logging)**

### 🚀 次のステップへ！

MultiQueryRetriever で、より多くの関連情報を見つけ出せるようになりました。しかし、時には「たくさん情報が見つかったけど、本当に大事なのはこの一部分だけだな…」「検索結果が長すぎて、LLM に全部渡すのは効率が悪いかも？」と感じることもあるでしょう。

次の **ステップ 19「検索改善(2): ContextualCompressionRetriever」** では、Retriever が取得してきた文書を、さらに賢く**圧縮**して、質問に本当に**関連性の高い部分だけを抽出**する **`ContextualCompressionRetriever`** というテクニックを学びます。取得する情報の「量」だけでなく、「質」と「効率」を高めるための次のステップに進みましょう！
