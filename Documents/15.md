前のステップでは、テキスト（言葉）をコンピュータが扱える数値のリスト、つまり「ベクトル」に変換する Embedding 技術を学びましたね。これで、テキストの意味を数値で表現できるようになりました。これは大きな進歩です！

でも、たくさんの文書から作った大量のベクトルの中から、「この質問に一番関係ありそうな文書はどれ？」とコンピュータに素早く見つけてもらうにはどうすればいいでしょうか？ 普通のファイル検索やデータベース検索では、言葉の「意味」で探すのは難しいのです。

このステップでは、そのための特別なデータベース、**Vector Store (ベクトルストア)** の基本を学びます。これは、言葉の意味を表すベクトルを効率よく整理・保管し、似た意味を持つベクトルを高速に見つけ出すための「情報の検索基盤」です。AI が外部の知識を賢く使うための重要な土台となります。

## 1. はじめに：このステップで目指すこと

### 🎯 今回のゴール

このステップを終えると、あなたはこんなことができるようになります！

- 大量のベクトルデータから関連情報を高速に検索するための **Vector Store** の役割とその必要性を理解します。
- LangChain を使って、ベクトル化されたテキストチャンク（`Document` オブジェクト）を **Chroma** という Vector Store に**保存**する方法を学びます。
- 保存した Vector Store に対して、質問文と**意味的に類似する**テキストチャンクを効率的に**検索**し、その**類似度スコア**も確認する方法を習得します。
- **メタデータ**を使って検索結果を絞り込む基本的な方法を学びます。
- **具体的な成果物:** テキストチャンクのリストを Embedding しながら Chroma Vector Store に格納し、その Vector Store に対して複数の質問を行い、関連性の高いチャンクを類似度スコア付きで検索したり、メタデータでフィルタリングしたりする Python コードを作成します。

### 🔑 このステップのポイント

今回は「ベクトルを検索可能にする」がテーマなので、以下の言葉や考え方が特に重要になります。

- **Vector Store**: ベクトルデータを効率的に格納・検索するための専用データベース。
- **ベクトル類似度検索 (Vector Similarity Search)**: あるベクトルと意味的に「近い」ベクトルを探し出す処理。
- **Retriever (リトリーバー)**: Vector Store などから関連情報を「取ってくる」ための LangChain インターフェース。
- **Chroma**: オープンソースの Vector Store。ローカル環境で手軽に試せます。
- **メタデータフィルタリング**: 文書チャンクに付随する情報 (出所など) を使って検索対象を絞り込む機能。
- **類似度スコア**: 検索結果がクエリとどれくらい類似しているかを示す数値。

### ✅ 前提知識

このステップに進む前に、以下の準備と知識があるとスムーズです。

- **ステップ 14「テキストをベクトルに！Embedding 入門」の内容**: `OpenAIEmbeddings` などを使ってテキストをベクトル化する方法を知っていること。
- **ステップ 13「長文を分割！テキスト分割の技法」の内容**: テキストを `Document` オブジェクトのチャンクに分割する考え方を理解していること。
- Python のリストや辞書の基本的な操作。
- OpenAI API キーが環境変数 `OPENAI_API_KEY` に設定されていること。
- 必要なライブラリのインストール:
  ```bash
  pip install -U chromadb langchain langchain-community langchain-openai python-dotenv tiktoken
  ```
  (`chromadb`: Chroma 本体, `langchain`, `langchain-community`: LangChain 関連, `langchain-openai`: OpenAI 連携, `python-dotenv`: 環境変数読込, `tiktoken`: トークン計算用)

---

## 2. 準備運動：ハンズオンのための基礎知識

### 🎯 目標

Vector Store とは何か、なぜ普通のデータベースでは不十分なのか、そして類似度検索がどのように機能するのか、基本的な概念を理解しましょう。

### Vector Store って何？ - ベクトル専用の高速本棚

Vector Store は、テキストの意味を表す「ベクトル」を大量に保管し、似た意味を持つベクトルを**素早く**探し出すことに特化したデータベースです。普通のデータベースが特定のキーワードで本を探すのに対し、Vector Store は「この本と似た内容の本はどれ？」という探し方が得意です。

### なぜ普通のデータベースじゃダメなの？

ベクトルは数百から数千個の数字の並びであり、その「意味的な近さ」を計算するのは特殊な処理が必要です。普通のデータベースはこの計算が苦手なため、大量のベクトルの中から似たものを探すのに非常に時間がかかってしまいます。Vector Store は、この高次元ベクトル検索を高速に行うための特別な仕組み（インデックスやアルゴリズム）を持っています。

### 類似度検索の仕組み - 「似ている」をどう見つける？

1.  **質問もベクトルに**: あなたの質問文も、文書チャンクと同じ Embedding モデルでベクトルに変換します。
2.  **「近さ」を計算**: Vector Store は、質問ベクトルと保存されている全ベクトルとの「近さ」を計算します (例: コサイン類似度)。
3.  **近い順に並べる**: 「近さ」のスコアが高い順（＝意味が近い順）に、関連する文書チャンク（に対応するベクトル）をいくつか選び出します。
4.  **結果を返す**: 選ばれた文書チャンク (`Document` オブジェクト) が、類似度スコアと共に検索結果として返されます。

### LangChain と Vector Store - Chroma を使ってみよう

LangChain は様々な Vector Store (Chroma, FAISS, Pinecone, Qdrant など) を共通の方法で扱えるようにしています。今回は、ローカル環境で手軽に始められる **`Chroma`** を使います。

### Retriever - 情報を取ってくる係

Vector Store から関連情報を効率よく取ってくるための LangChain の部品が **Retriever** です。Vector Store オブジェクトから `.as_retriever()` メソッドで簡単に作成でき、後の RAG (検索拡張生成) システム構築で中心的な役割を果たします。

---

## 3. 実践タイム：Vector Store を作って検索しよう！

### 🎯 目標

実際に Python コードを書き、`Document` チャンクのリストを `Chroma` Vector Store に格納し、類似度検索（スコア付き、フィルタリング含む）を実行して関連チャンクを取得します。

### ステップ・バイ・ステップ実装

#### 1. 準備 (インポート、Embedding、データ準備):

必要なライブラリをインポートし、Embedding モデルを用意し、サンプル `Document` リストを準備します。

```python
# step15_vectorstore_basic_revised.py
import os
import sys
from dotenv import load_dotenv
from typing import List

# --- 必要な LangChain モジュール ---
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
# Chroma Vector Store (langchain-community または langchain 本体か確認)
# 最新の推奨を確認し、どちらか一方または両方試す
try:
    # langchain 本体からのインポートを試す (最近のバージョンで推奨される場合がある)
    from langchain.vectorstores import Chroma
    print("Chroma をインポートしました (from langchain.vectorstores)。")
except ImportError:
    try:
        # langchain-community からのインポートを試す
        from langchain_community.vectorstores import Chroma
        print("Chroma をインポートしました (from langchain_community.vectorstores)。")
        print("   'pip install langchain-community' が必要かもしれません。")
    except ImportError:
        print("エラー: Chroma Vector Store が見つかりません。")
        print("   'pip install langchain chromadb' または 'pip install langchain-community chromadb' を確認してください。")
        sys.exit(1)

# Chroma DB 本体も必要
try:
    import chromadb
    print(f"chromadb バージョン: {chromadb.__version__}")
except ImportError:
    print("エラー: chromadb がインストールされていません。")
    print("   'pip install chromadb' を実行してください。")
    sys.exit(1)


print("--- 必要なモジュールのインポート完了 ---")

load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    print("エラー: OpenAI API キーが設定されていません。")
    sys.exit(1)
else:
    print("OpenAI API キーを読み込みました。")

# --- Embedding モデルの準備 ---
try:
    # モデル名を明示的に指定し、APIキーを渡す
    embeddings_model = OpenAIEmbeddings(model="text-embedding-3-small", api_key=openai_api_key)
    # text-embedding-3-large など他のモデルも利用可能
    print(f"--- Embedding モデル準備完了 ({embeddings_model.model}) ---")
except Exception as e:
    print(f"エラー: Embedding モデルの初期化に失敗: {e}")
    sys.exit(1)

# --- Vector Store に格納するデータの準備 ---
sample_documents = [
    Document(page_content="LangGraph は状態を持つ複雑なグラフを構築できます。", metadata={"source": "doc-a", "page": 1, "category": "graph"}),
    Document(page_content="LCEL は LangChain Expression Language の略です。", metadata={"source": "doc-b", "page": 1, "category": "core"}),
    Document(page_content="エージェントはツールを使ってタスクを実行します。", metadata={"source": "doc-c", "page": 1, "category": "agent"}),
    Document(page_content="ベクトルストアは埋め込みベクトルを高速に検索します。", metadata={"source": "doc-d", "page": 1, "category": "vectorstore"}),
    Document(page_content="LangGraph ではノード間で状態が共有されます。", metadata={"source": "doc-a", "page": 2, "category": "graph"}),
    Document(page_content="RAG は検索拡張生成の略で、外部知識を利用します。", metadata={"source": "doc-e", "page": 1, "category": "rag"}),
    Document(page_content="プロンプトテンプレートを使うと入力を動的にできます。", metadata={"source": "doc-f", "page": 1, "category": "core"}),
]
print(f"--- サンプル Document (チャンク) {len(sample_documents)} 件準備完了 ---")
```

- **インポートパス**: `Chroma` のインポート元は LangChain のバージョンにより `langchain.vectorstores` か `langchain_community.vectorstores` か変わる可能性があります。両方試すように `try...except` を入れています。ご自身の環境でどちらが動作するか確認してください。
- `chromadb` ライブラリも必要です (`pip install chromadb`)。
- サンプル Document の `metadata` に `category` キーを追加しました。後でフィルタリングに使います。

#### 2. Vector Store の作成とデータ追加 (`.from_documents()`):

`Chroma.from_documents()` を使って、インメモリの Vector Store を作成し、データを格納します。

```python
# step15_vectorstore_basic_revised.py (続き)

print("\n--- Chroma Vector Store の作成 (インメモリ) ---")
vector_store = None # エラー時に備えて初期化
try:
    # .from_documents() で Document リストからインメモリ Vector Store を作成
    # 内部で embedding モデルを使ってベクトル化と格納が行われる
    vector_store = Chroma.from_documents(
        documents=sample_documents,
        embedding=embeddings_model
        # persist_directory を指定しなければメモリ上
    )
    print("Chroma Vector Store (インメモリ) が作成されました。")
    # 件数確認 (Chroma の公式な方法がなければ、これは目安)
    # print(f"格納されたアイテム数 (目安): {vector_store._collection.count()}") # _collection は内部APIの可能性

    # --- (参考) ファイルに永続化する場合 ---
    # db_directory = "./chroma_db_persist"
    # print(f"\n--- Chroma Vector Store の作成 (永続化: {db_directory}) ---")
    # vector_store_persist = Chroma.from_documents(
    #     documents=sample_documents,
    #     embedding=embeddings_model,
    #     persist_directory=db_directory # このフォルダに保存される
    # )
    # print("永続化 Vector Store が作成されました。")
    # # プログラム終了後もデータは残る
    # # 次回読み込む場合:
    # # vector_store_loaded = Chroma(persist_directory=db_directory, embedding_function=embeddings_model)
    # # print("永続化 Vector Store を読み込みました。")

except Exception as e:
    print(f"Vector Store の作成中にエラーが発生しました: {e}")
    sys.exit(1)

if vector_store is None:
     print("エラー: Vector Store の作成に失敗しました。")
     sys.exit(1)

```

- `.from_documents()` は非常に便利で、`Document` リストを渡すだけで、Embedding と Vector Store への格納を一括で行ってくれます。
- **永続化**: コメントアウト部分のように `persist_directory` を指定するとファイルに保存されます。保存したデータは、次回 `Chroma(persist_directory=..., embedding_function=...)` として読み込めます。

#### 3. 類似度検索の実行 (スコア付き、フィルタリング):

`.similarity_search_with_score()` や `filter` を使って、より詳細な検索を試します。

```python
# step15_vectorstore_basic_revised.py (続き)

# --- 検索クエリ ---
query1 = "グラフの状態管理について教えて"
query2 = "LangChain のコア機能は？"

print(f"\n--- 検索クエリ1: '{query1}' ---")
try:
    # スコア付きで類似度検索 (k=上位3件)
    search_results_score = vector_store.similarity_search_with_score(query1, k=3)
    print(f"\nスコア付き類似度検索の結果 ({len(search_results_score)} 件):")
    for doc, score in search_results_score:
        # スコアの解釈: Chroma のデフォルト (L2距離) は小さいほど類似度が高い
        # コサイン類似度を使う設定なら大きいほど高い。指標によります。
        print(f"  Score: {score:.4f} (値が小さいほど類似度が高い可能性)")
        print(f"  Content: {doc.page_content}")
        print(f"  Metadata: {doc.metadata}")

except Exception as e:
    print(f"類似度検索中にエラーが発生しました: {e}")

print(f"\n--- 検索クエリ2: '{query2}' (メタデータでフィルタリング) ---")
try:
    # メタデータの 'category' が 'core' のものだけを対象に検索
    search_results_filtered = vector_store.similarity_search(
        query2,
        k=2,
        filter={"category": "core"} # 'category' が 'core' の Document のみ検索
    )
    print(f"\nメタデータフィルタリング検索の結果 ({len(search_results_filtered)} 件):")
    for i, doc in enumerate(search_results_filtered):
        print(f"--- 結果 {i+1} ---")
        print(f"  Content: {doc.page_content}")
        print(f"  Metadata: {doc.metadata}") # category が 'core' になっているはず

except Exception as e:
    print(f"フィルタリング検索中にエラーが発生しました: {e}")

```

- **`.similarity_search_with_score()`**: `(Document, score)` のタプルのリストを返します。スコアの意味（大きい方が良いか、小さい方が良いか）は、Vector Store や設定（距離指標）によって異なるので注意が必要です。Chroma のデフォルトは L2 距離 (小さいほど近い) です。
- **`filter={"category": "core"}`**: `.similarity_search()` (や `_with_score`, `as_retriever`) の `filter` 引数に辞書を渡すことで、メタデータに基づいた絞り込み検索ができます。ここでは `category` が `core` の Document だけを対象にしています。

#### 4. Retriever としての利用 (`.as_retriever()`):

Retriever を作成し、フィルタリング検索も試してみます。

```python
# step15_vectorstore_basic_revised.py (続き)

print("\n--- Retriever として利用 (.as_retriever) ---")

try:
    # k=2, category='graph' でフィルタリングする Retriever を作成
    retriever_filtered = vector_store.as_retriever(
        search_type="similarity", # 他に "mmr" などもある
        search_kwargs={'k': 2, 'filter': {'category': 'graph'}}
    )
    print("フィルタリング付き Retriever を作成しました (k=2, category='graph')。")

    # Retriever を使って関連文書を取得
    query_graph = "LangGraph の状態について" # query1 と似たクエリ
    retrieved_docs_filtered = retriever_filtered.invoke(query_graph)
    print(f"\nRetriever (フィルタリング付き) による検索結果 ({len(retrieved_docs_filtered)} 件) for query: '{query_graph}'")

    for i, doc in enumerate(retrieved_docs_filtered):
        print(f"--- 結果 {i+1} ---")
        print(f"  Content: {doc.page_content}")
        print(f"  Metadata: {doc.metadata}") # category が 'graph' になっているはず

except Exception as e:
    print(f"Retriever の作成または使用中にエラーが発生しました: {e}")

print("\n--- 処理終了 ---")
```

- `.as_retriever()` の `search_kwargs` に `k` や `filter` を指定することで、Retriever のデフォルト検索挙動を設定できます。
- 作成した `retriever_filtered` は、「`category` が `graph` の文書の中から、質問に最も類似するものを 2 件取ってくる」という動作をします。

### 完成コード (`step15_vectorstore_basic_revised.py`)

上記の実装 1〜4 を結合したものが完成コードとなります。

---

## 4. 深掘り解説：Vector Store の裏側と選択肢

### 🎯 目標

Vector Store がどのように高速な類似度検索を実現しているのか、その仕組みの概要と、様々な Vector Store の種類や選択のポイントについて理解を深めます。

### 類似度検索の裏側：高速検索の秘密

Vector Store は、大量のベクトルの中から似ているものを高速に見つけるために、内部で**インデックス (Index)** という特殊なデータ構造を使っています。これは、ベクトルデータを効率的に検索するための「索引」です。

多くの場合、**近似最近傍探索 (Approximate Nearest Neighbor, ANN)** アルゴリズムが使われます。これは「完全に一番近いもの」ではなく「ほぼ一番近いであろうもの」を非常に高速に見つける技術で、速度と精度のバランスを取っています。

ベクトル間の「近さ」は、**コサイン類似度** (向きの近さ) や **ユークリッド距離** (直線距離) などで測られます。

### Vector Store の種類と選び方

- **ローカル / インメモリ**:
  - **Chroma**: 今回使用。手軽で開発向き。ファイル永続化も可能。
  - **FAISS**: 高速検索ライブラリ。ローカル利用に適している。
  - **メリット**: 簡単、無料、外部依存なし。
  - **デメリット**: 大規模データには不向きな場合がある、自己管理が必要。
- **クラウド / マネージドサービス**:
  - **Pinecone, Qdrant, Weaviate** など多数。
  - **メリット**: 大規模対応、高可用性、運用が楽、高機能。
  - **デメリット**: 有料、外部サービス依存。

まずは `Chroma` などで試し、要件に応じて他のものを検討しましょう。

### チャンクサイズと検索品質の関係

ステップ 13 で学んだテキスト分割の**チャンクサイズ**は、Vector Store の検索品質に影響します。

- **小さすぎるチャンク**: 文脈が不足し、質問との関連性を正しく捉えられない可能性があります。
- **大きすぎるチャンク**: 質問とは直接関係ない情報が多く含まれてしまい、検索ノイズが増えたり、LLM が回答を生成する際の邪魔になったりする可能性があります。

適切なチャンクサイズ（数百〜1000 トークン程度が目安だが、ケースバイケース）を見つけることが重要です。

### ベクトル次元数の制約

通常、一つの Vector Store には**同じ次元数のベクトルしか格納できません**。例えば、1536 次元の Embedding モデルで作ったベクトルと、512 次元のモデルで作ったベクトルを同じ Vector Store に混在させることはできません。使用する Embedding モデルを決めたら、そのモデルで一貫してベクトルを作成・格納する必要があります。

### Retriever の検索タイプ

`.as_retriever()` の `search_type` には、`"similarity"` (類似度順) 以外にも、`"mmr"` (Maximal Marginal Relevance) などがあります。MMR は、類似度が高いだけでなく、結果の**多様性**も考慮して文書を選ぶ方法です（似たような文書ばかりが選ばれるのを防ぐ）。

---

## 5. 最終チェック：検索基盤は機能した？

### 🎯 目標

実装したコードが正しく動作し、Vector Store へのデータ格納と、類似度検索（スコア付き、フィルタリング含む）が期待通りに行われているかを確認します。

### 確認してみよう！

- **実行**: `step15_vectorstore_basic_revised.py` を実行してください。（必要なライブラリと API キーを確認）
- **エラー**: エラーメッセージが表示されずに最後まで実行できましたか？
- **Vector Store 作成**: 「Chroma Vector Store (インメモリ) が作成されました。」と表示されましたか？
- **スコア付き検索**:
  - クエリ 1 の結果で、`Score` 付きで 3 件の Document が表示されましたか？
  - 表示された Document の内容はクエリ 1 に関連していますか？
- **フィルタリング検索**:
  - クエリ 2 の結果で、`metadata` の `category` が `'core'` である Document だけが 2 件表示されていますか？
  - `category` が `'graph'` や `'agent'` の Document は表示されていないことを確認してください。
- **Retriever (フィルタリング付き)**:
  - 最後の検索結果で、`metadata` の `category` が `'graph'` である Document だけが 2 件表示されていますか？

これらの点が確認できれば、Vector Store の基本的な作成と検索、フィルタリングは成功です！

---

## 6. まとめ：学びの整理と次へのステップ

### ✅ 達成したこと！

これで、AI が外部知識を効率的に検索するための重要な基盤、Vector Store の基本を理解し、使えるようになりました！

- 大量のベクトルデータから高速に類似ベクトルを検索する **Vector Store** の役割と必要性を理解しました。
- **Chroma** を使って、`Document` チャンク（とベクトル）を**格納**する方法 (`.from_documents()`) を学び、インメモリと永続化の違いを知りました。
- Vector Store に対して、**類似度検索**を行う方法 (`.similarity_search()`, `.similarity_search_with_score()`) を実装しました。
- **メタデータ**を使って検索結果を**フィルタリング**する方法を学びました。
- Vector Store から検索用インターフェースである **Retriever** を作成 (`.as_retriever()`) し、利用する方法を学びました。

### 🔑 学んだキーワード

- **Vector Store (ベクトルストア)**
- **Chroma** (`langchain.vectorstores` または `langchain_community.vectorstores` より)
- **Embedding (埋め込み / ベクトル)**
- **類似度検索 (Similarity Search)**
- **類似度スコア (Similarity Score)**
- **.from_documents()**
- **.similarity_search()**, **.similarity_search_with_score()**
- **メタデータフィルタリング (Metadata Filtering)** (`filter` 引数)
- **Retriever (リトリーバー)**
- **.as_retriever()**
- **インメモリ (In-memory) / 永続化 (Persistence)**
- **チャンクサイズ (Chunk Size)** と検索品質
- **ベクトル次元数 (Vector Dimension)** の制約

### 🚀 次のステップへ！

Vector Store を使って、質問に関連する情報をテキストチャンクとして取り出すことができるようになりました。これは RAG (Retrieval-Augmented Generation) システムの **Retrieval (検索)** の部分にあたります。

次はいよいよ、この検索してきた情報 (Context) と元の質問を組み合わせて、AI (LLM) に最終的な回答を生成させるステップです。

次の **ステップ 16「RAG 構築(1): Retriever と Context 取得」** と **ステップ 17「RAG 構築(2): Context 注入と LLM 生成」** で、これまで学んだ要素（プロンプト、LLM、そして今回の Retriever）を LCEL で繋ぎ合わせ、**検索拡張生成 (RAG)** の基本的なチェーンを完成させます！ AI が外部知識を参照しながら回答する、強力なアプリケーション構築に挑戦しましょう！
