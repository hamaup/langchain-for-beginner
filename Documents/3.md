AI とのコミュニケーション、もっとスムーズにできたら嬉しいですよね。AI への指示である「プロンプト」は、そのための大切な「指示書」。このステップでは、その指示書作りをレベルアップさせる方法、LangChain の「プロンプトテンプレート」を学びます。これを使えば、もっと賢く、柔軟に AI と対話できるようになりますよ！

## 1. はじめに：このステップで目指すこと

### 🎯 今回のゴール

- プロンプトテンプレートを活用して、AI への指示を効率よく、そして思い通りに作成・管理できるようになる。
- **具体的な成果物:** LangChain が提供する `PromptTemplate` と `ChatPromptTemplate` という２つの道具をマスターし、状況に応じて変化する情報（変数）を含んだ指示や、AI に特定の役割や応答形式を教え込む指示（Few-shot プロンプティング）を作れるようになる。

### 🔑 このステップのポイント

- **テンプレートって便利！:** なぜ指示の「ひな形」を使うと良いのか、そのメリットを実感する。
- **２つのテンプレート:** `PromptTemplate` と `ChatPromptTemplate`、それぞれの得意なことと使い分け方を理解する。
- **カスタマイズ自在:** プロンプトに変数を入れたり、AI に「〇〇専門家」として振る舞ってもらったり、Few-shot でお手本を示して特定のタスク（例：感情分析）を実行させたりする方法を身につける。

### 前提知識

- ステップ 2「AI と初対話！LLM に話しかけてみよう」を完了していること（`ChatOpenAI` と `.invoke()` の基本操作を知っている）。
- Python の基礎知識（変数、辞書、文字列の扱いなど）があること。
- ステップ 1 でセットアップした開発環境（作業フォルダ、仮想環境、API キーなど）が手元にあること。

---

## 2. 準備運動：プロンプトテンプレートってなんだろう？

### 🎯 目標

- プロンプトテンプレートの基本的な考え方と、LangChain で中心となるクラス（道具）の役割を掴む。

### なぜ「テンプレート」が必要なの？

ステップ 2 では、AI に直接質問文を投げかけました。シンプルで良い方法ですが、もう少し込み入ったことを AI にお願いしようとすると、少し物足りなくなってきます。

- **定型文を何度も…:** 「この[文書の種類]を要約してください：[本文]」のように、骨組みは同じで一部分だけ変えたい指示。
- **その時々で情報を変えたい:** 「[さん]、[場所]の明日の天気は？」のように、実行するたびに違う情報を入れたい指示。
- **AI のキャラやタスク精度を高めたい:** 「あなたは翻訳家です。」「この文章がポジティブかネガティブか分類してください。」のように、AI の役割を決めたり、特定のタスクにおける応答の一貫性や精度を高めたい。

こんなとき、毎回全文を手入力するのは面倒だし、ミスも増えますよね。そこで輝くのが**プロンプトテンプレート**です！これは指示文の「ひな形」で、後から情報を埋め込むための「穴（変数）」を用意したり、AI との会話における役割分担（システムからの指示、ユーザーからの質問など）を決めたり、さらには AI に応答のお手本（Few-shot）を見せて特定のタスクの実行方法を教えたりできます。

### LangChain の主な道具たち

LangChain には、主に 2 種類のプロンプトテンプレートが用意されています。

1.  **`PromptTemplate`**:

    - 最もシンプルで基本的なテンプレート。
    - 指示文の中に `{変数名}` という形で「穴」を作っておき、後から好きな文字列を埋め込めます。
    - このテンプレートから作られる最終的な指示は、**1 つの長い文字列**になります。
    - 用途：単純な指示生成、役割分担が不要な場合など。
    - `langchain.prompts` モジュールからインポートします。

2.  **`ChatPromptTemplate`**:

    - `ChatOpenAI` のような、**人間と自然な会話（チャット）をするのが得意な AI モデル**（チャットモデル）にピッタリなテンプレート。**チャットモデルを使う場合は、基本的にこちらを使うのが推奨**されます。
    - 指示を**複数の「メッセージ」**（発言者とその内容のセット）の組み合わせで組み立てます。これにより、「これはシステムからの指示」「これはユーザーからの質問」「これは AI の模範応答」といった役割分担が明確になります。
    - 最終的に LLM には**メッセージオブジェクトのリスト**が渡されます。これにより、モデルは会話の文脈や各発言の意図をより正確に理解できます。
    - `langchain.prompts` モジュールからインポートします。
    - メッセージの種類には、主に以下のようなものがあります。（これらは `langchain.schema` モジュールからインポートします）

      - **`SystemMessage`**: システム（開発者）から AI への全体的な指示や設定。AI の役割、性格、応答スタイル、実行すべきタスクなどを伝えます。（例：「あなたは与えられた文章をポジティブかネガティブに分類する AI です。」）
      - **`HumanMessage`**: 人間（ユーザー）の発言や質問、またはタスクの入力データ。（例：「この映画は最高でした！」）
      - **`AIMessage`**: AI 自身の発言。主に応答のお手本（Few-shot プロンプティング）を示す際に使います。（例：「ポジティブ」）

    - **【重要ポイント！】** `ChatPromptTemplate` を使う際、メッセージの中に `{変数名}` を含めたい場合は、少し注意が必要です。単に `SystemMessage("あなたは{language}の専門家です")` と書くのではなく、`langchain.prompts` 内にある **`SystemMessagePromptTemplate.from_template("あなたは{language}の専門家です")`** のように、**「PromptTemplate」という名前がついたクラス**を使う必要があります。これは、「このメッセージは単なる固定文ではなく、後で変数が埋まるテンプレートですよ」と LangChain に教えるためのルールです。（詳しくは「4. 深掘り解説」で！）

---

## 3. 実践タイム：テンプレートを組み立ててみよう！

### 🎯 目標

- Python コードを書いて、`PromptTemplate` と `ChatPromptTemplate` を実際に作成し、AI への指示を生成して実行結果まで確認する。特に `ChatPromptTemplate` では、Few-shot による感情分析タスクの実行を試す。

### ファイルの準備

- ステップ 1 で作った作業フォルダ内に、`step3_prompt_template.py` という名前で新しい Python ファイルを作成しましょう。

### ステップ・バイ・ステップ実装

1.  **土台の準備（インポートと LLM 初期化）:**
    まずは必要な道具（クラス）を Python コードに読み込み、LLM を使えるように準備します。`langchain.prompts` からテンプレート関連のクラスを、`langchain.schema` からメッセージ関連のクラスをインポートします。

    ```python
    # step3_prompt_template.py
    import os
    from dotenv import load_dotenv
    from langchain_openai import ChatOpenAI

    # --- プロンプト関連の道具をインポート (langchain.prompts) ---
    from langchain.prompts import (
        PromptTemplate,
        ChatPromptTemplate,
        SystemMessagePromptTemplate, # システムメッセージのテンプレート用
        HumanMessagePromptTemplate,  # 人間のメッセージのテンプレート用
    )
    # --- メッセージクラスをインポート (langchain.schema) ---
    # (AIMessage, HumanMessage, SystemMessage)
    from langchain.schema import AIMessage, HumanMessage, SystemMessage

    # 環境変数の読み込み
    load_dotenv()
    print("--- 環境変数読み込み完了 ---")

    # LLMの準備 (temperature=0 で応答を安定させる)
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
    print(f"--- LLM準備完了: {llm.model_name} (temperature={llm.temperature}) ---")
    ```

2.  **`PromptTemplate` でシンプルに指示:**
    テキストとその要約形式を後から指定できる、シンプルな指示テンプレートを作ってみましょう。

    ```python
    # step3_prompt_template.py (続き)

    print("\n--- PromptTemplate のテスト ---")
    template_string = """
    以下のテキストを指定された形式で要約してください。

    テキスト:
    {input_text}

    要約形式:
    {output_format}
    """
    # PromptTemplate を作成
    prompt_template = PromptTemplate.from_template(template_string)

    # 埋め込むデータを辞書で準備
    input_data = {
        "input_text": "LangChainは、大規模言語モデル（LLM）を活用したアプリケーション開発を容易にするためのフレームワークです。多様なコンポーネントを組み合わせて、複雑なワークフローを構築できます。",
        "output_format": "箇条書きで3点"
    }
    # テンプレートにデータを埋め込んで最終的なプロンプト文字列を生成
    final_prompt_string = prompt_template.format(**input_data)
    print("【生成されたプロンプト文字列】:")
    print(final_prompt_string)

    # 生成した文字列をLLMに渡して実行
    response = llm.invoke(final_prompt_string)
    print("\n【LLMからの応答 (PromptTemplate)】:")
    print(response.content)
    ```

3.  **`ChatPromptTemplate` で役割分担 (基本例):**
    チャットモデルが得意な、役割分担のある指示を作ってみましょう。AI に翻訳家の役割をお願いし、翻訳対象の言語とテキストを変数で指定します。

    ```python
    # step3_prompt_template.py (続き)

    print("\n--- ChatPromptTemplate のテスト (基本) ---")
    # システムメッセージのテンプレート (変数 language を含む)
    system_template = SystemMessagePromptTemplate.from_template(
        "あなたは{language}の翻訳家です。丁寧な言葉遣いで回答してください。"
    )
    # 人間のメッセージのテンプレート (変数 text_to_translate を含む)
    human_template = HumanMessagePromptTemplate.from_template(
        "{text_to_translate} を翻訳してください。"
    )
    # 2つのテンプレートを組み合わせて ChatPromptTemplate を作成
    chat_template = ChatPromptTemplate.from_messages([system_template, human_template])

    # 埋め込むデータを辞書で準備
    chat_input_data = {
        "language": "フランス語",
        "text_to_translate": "Hello, how are you?"
    }
    # テンプレートにデータを埋め込んで最終的なメッセージリストを生成
    final_prompt_messages = chat_template.format_messages(**chat_input_data)
    print("【生成されたプロンプトメッセージ (リスト)】:")
    print(final_prompt_messages)

    # 生成したメッセージリストをLLMに渡して実行
    response_chat = llm.invoke(final_prompt_messages)
    print("\n【LLMからの応答 (ChatPromptTemplate 基本)】:")
    print(response_chat.content)
    ```

4.  **`ChatPromptTemplate` でお手本を示す (Few-shot 例：感情分析):**
    AI にいくつかの感情分析の例（Few-shot）を見せて、「ポジティブ」か「ネガティブ」に分類するタスクを実行させましょう。固定メッセージは `langchain.schema` から、変数を含むメッセージは `langchain.prompts` のテンプレートクラスを使います。

    ```python
    # step3_prompt_template.py (続き)

    print("\n--- ChatPromptTemplate のテスト (Few-shot: 感情分析) ---")
    # 1. メッセージテンプレートと固定メッセージのリストを作成
    messages_for_sentiment_analysis = [
        # AIの役割設定 (固定メッセージ: SystemMessage from langchain.schema)
        SystemMessage(content="あなたは与えられた文章の感情を分析し、「ポジティブ」または「ネガティブ」のどちらか一言で分類するAIです。"),
        # --- ここから Few-shot のお手本 ---
        # お手本1 (固定メッセージ: HumanMessage & AIMessage from langchain.schema)
        HumanMessage(content="文章: 「この映画は最高でした！」\n感情:"),
        AIMessage(content="ポジティブ"),
        # お手本2 (固定メッセージ)
        HumanMessage(content="文章: 「サービスがとても遅く、食事も冷めていました。」\n感情:"),
        AIMessage(content="ネガティブ"),
        # お手本3 (固定メッセージ)
        HumanMessage(content="文章: 「新しい職場は雰囲気が良く、同僚も親切です。」\n感情:"),
        AIMessage(content="ポジティブ"),
        # --- ここまで Few-shot のお手本 ---
        # --- 実際にユーザーが分類を依頼する部分（変数 input_sentence が必要） ---
        # HumanMessage のテンプレートを使用 (HumanMessagePromptTemplate from langchain.prompts)
        HumanMessagePromptTemplate.from_template("文章: 「{input_sentence}」\n感情:")
    ]
    # 2. ChatPromptTemplate を組み立てる
    sentiment_analysis_template = ChatPromptTemplate.from_messages(messages_for_sentiment_analysis)

    # 3. 穴埋めする内容を辞書で準備 (分類対象の文章)
    sentiment_input_data = {
        "input_sentence": "昨日の会議は長すぎて退屈でした。"
    }

    # 4. テンプレートにデータを埋め込み、最終的なメッセージリストを生成
    final_sentiment_messages = sentiment_analysis_template.format_messages(**sentiment_input_data)
    print("【生成されたプロンプトメッセージ (Few-shot)】: (Systemと最後のHumanのみ表示)")
    print(final_sentiment_messages[0]) # System Message
    print("...") # お手本部分は省略
    print(final_sentiment_messages[-1]) # 最後の Human Message (テンプレート適用後)

    # 5. 生成したメッセージリストをLLMに渡して実行
    print("\n【LLMへの入力 (Few-shot メッセージリスト)】:")
    # print(final_sentiment_messages) # デバッグ用
    response_sentiment = llm.invoke(final_sentiment_messages)
    print("\n【LLMからの応答 (ChatPromptTemplate Few-shot)】:")
    # お手本に倣って「ネガティブ」という応答が期待される
    print(response_sentiment.content)

    print("\n--- 処理終了 ---")
    ```

    - AI の役割設定(`SystemMessage`)とお手本(`HumanMessage`, `AIMessage`)は内容が固定なので `langchain.schema` のクラスを直接使います。
    - ユーザーが分類したい文章を受け取る部分は変数 `{input_sentence}` が必要なので、`langchain.prompts` の `HumanMessagePromptTemplate` を使います。

#### 完成コード (`step3_prompt_template.py`)

```python
# step3_prompt_template.py
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

# --- プロンプト関連の道具をインポート (langchain.prompts) ---
from langchain.prompts import (
    PromptTemplate,
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
# --- メッセージクラスをインポート (langchain.schema) ---
from langchain.schema import AIMessage, HumanMessage, SystemMessage

# 環境変数の読み込み
load_dotenv()
print("--- 環境変数読み込み完了 ---")

# LLMの準備 (temperature=0 で応答を安定させる)
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
print(f"--- LLM準備完了: {llm.model_name} (temperature={llm.temperature}) ---")

# --- PromptTemplate のテスト ---
print("\n--- PromptTemplate のテスト ---")
template_string = """
以下のテキストを指定された形式で要約してください。

テキスト:
{input_text}

要約形式:
{output_format}
"""
prompt_template = PromptTemplate.from_template(template_string)
input_data = {
    "input_text": "LangChainは、大規模言語モデル（LLM）を活用したアプリケーション開発を容易にするためのフレームワークです。多様なコンポーネントを組み合わせて、複雑なワークフローを構築できます。",
    "output_format": "箇条書きで3点"
}
final_prompt_string = prompt_template.format(**input_data)
print("【生成されたプロンプト文字列】:")
print(final_prompt_string)
response = llm.invoke(final_prompt_string)
print("\n【LLMからの応答 (PromptTemplate)】:")
print(response.content)

# --- ChatPromptTemplate のテスト (基本) ---
print("\n--- ChatPromptTemplate のテスト (基本) ---")
system_template = SystemMessagePromptTemplate.from_template(
    "あなたは{language}の翻訳家です。丁寧な言葉遣いで回答してください。"
)
human_template = HumanMessagePromptTemplate.from_template(
    "{text_to_translate} を翻訳してください。"
)
chat_template = ChatPromptTemplate.from_messages([system_template, human_template])
chat_input_data = {
    "language": "フランス語",
    "text_to_translate": "Hello, how are you?"
}
final_prompt_messages = chat_template.format_messages(**chat_input_data)
print("【生成されたプロンプトメッセージ (リスト)】:")
print(final_prompt_messages)
response_chat = llm.invoke(final_prompt_messages)
print("\n【LLMからの応答 (ChatPromptTemplate 基本)】:")
print(response_chat.content)

# --- ChatPromptTemplate のテスト (Few-shot: 感情分析) ---
print("\n--- ChatPromptTemplate のテスト (Few-shot: 感情分析) ---")
messages_for_sentiment_analysis = [
    # AIの役割設定
    SystemMessage(content="あなたは与えられた文章の感情を分析し、「ポジティブ」または「ネガティブ」のどちらか一言で分類するAIです。"),
    # お手本1
    HumanMessage(content="文章: 「この映画は最高でした！」\n感情:"),
    AIMessage(content="ポジティブ"),
    # お手本2
    HumanMessage(content="文章: 「サービスがとても遅く、食事も冷めていました。」\n感情:"),
    AIMessage(content="ネガティブ"),
    # お手本3
    HumanMessage(content="文章: 「新しい職場は雰囲気が良く、同僚も親切です。」\n感情:"),
    AIMessage(content="ポジティブ"),
    # 実際のユーザーリクエスト (テンプレート)
    HumanMessagePromptTemplate.from_template("文章: 「{input_sentence}」\n感情:")
]
sentiment_analysis_template = ChatPromptTemplate.from_messages(messages_for_sentiment_analysis)
# 分類対象の文章
sentiment_input_data = {
    "input_sentence": "昨日の会議は長すぎて退屈でした。"
}
final_sentiment_messages = sentiment_analysis_template.format_messages(**sentiment_input_data)
print("【生成されたプロンプトメッセージ (Few-shot)】: (Systemと最後のHumanのみ表示)")
print(final_sentiment_messages[0]) # System Message
print("...") # お手本部分は省略
print(final_sentiment_messages[-1]) # 最後の Human Message (テンプレート適用後)

response_sentiment = llm.invoke(final_sentiment_messages)
print("\n【LLMからの応答 (ChatPromptTemplate Few-shot)】:")
# お手本に倣って「ネガティブ」という応答が期待される
print(response_sentiment.content)

print("\n--- 処理終了 ---")

```

### 実行結果の例

- **実行コマンド:** （仮想環境を有効にして） `python step3_prompt_template.py`
- **期待される出力例:**

  ```
  --- 環境変数読み込み完了 ---
  --- LLM準備完了: gpt-3.5-turbo (temperature=0.0) ---

  --- PromptTemplate のテスト ---
  【生成されたプロンプト文字列】:
  (省略...)
  【LLMからの応答 (PromptTemplate)】:
  - LangChainはLLMを活用したアプリ開発を容易にするフレームワーク
  - 多様なコンポーネントで複雑なワークフローを構築可能
  - アプリケーション開発を支援

  --- ChatPromptTemplate のテスト (基本) ---
  【生成されたプロンプトメッセージ (リスト)】:
  [SystemMessage(content='あなたはフランス語の翻訳家です。丁寧な言葉遣いで回答してください。'), HumanMessage(content='Hello, how are you? を翻訳してください。')]
  【LLMからの応答 (ChatPromptTemplate 基本)】:
  Bonjour, comment allez-vous ?

  --- ChatPromptTemplate のテスト (Few-shot: 感情分析) ---
  【生成されたプロンプトメッセージ (Few-shot)】: (Systemと最後のHumanのみ表示)
  SystemMessage(content='あなたは与えられた文章の感情を分析し、「ポジティブ」または「ネガティブ」のどちらか一言で分類するAIです。')
  ...
  HumanMessage(content='文章: 「昨日の会議は長すぎて退屈でした。」\n感情:')

  【LLMからの応答 (ChatPromptTemplate Few-shot)】:
  ネガティブ

  --- 処理終了 ---
  ```

  - **期待される効果:** Few-shot のお手本がない場合、LLM は「この文章はネガティブな感情を表していると考えられます。」のように説明的に答えたり、分類結果が一貫しない可能性があります。しかし、Few-shot で具体的な分類例を示すことで、LLM はタスクを正確に理解し、「ポジティブ」または「ネガティブ」という期待通りの形式で一貫した応答を返すように誘導されます。

---

## 4. 深掘り解説：テンプレートをもっとよく知る

### 🎯 目標

- `PromptTemplate` と `ChatPromptTemplate` の適切な使い分け、メッセージクラスの役割分担、そしてテンプレート化のメリットについて理解を深める。

### `PromptTemplate` vs `ChatPromptTemplate`：どっちを使う？

- **`PromptTemplate`（出力: 文字列）**:
  - シンプルな指示文が必要なときに。
  - 最終的に AI に渡したいのが**一つのまとまったテキスト**の場合。
  - チャットモデル（`ChatOpenAI`など）を使う場合でも利用できますが、役割分担などを明確にしたい場合は `ChatPromptTemplate` の方が適しています。
  - **注意点:** `PromptTemplate` オブジェクト自体は、LLM の `.invoke()` メソッドに直接渡すことはできません。`.format()` などで**生成された文字列**を `.invoke()` に渡す必要があります。
- **`ChatPromptTemplate`（出力: メッセージリスト）**:
  - `ChatOpenAI` などの**チャットモデルを使う場合の標準的な選択肢**。こちらを使うのがおすすめです。
  - AI に役割を与えたり（System）、ユーザー（Human）と AI（AI）の対話を明確に区別したいときに。
  - Few-shot プロンプティングでお手本を示したいときに。
  - モデルが文脈や役割を理解しやすくなり、より自然で適切な応答を引き出しやすくなります。
  - 生成される**メッセージオブジェクトのリスト** (`langchain.schema` のクラスのインスタンス）をそのまま `.invoke()` に渡せます。

### メッセージクラスの使い分け【重要！】

`ChatPromptTemplate` を使いこなす上で、この区別はとても大切です！

- **`SystemMessage`, `HumanMessage`, `AIMessage` (from `langchain.schema`)**:
  - これらは**内容が完全に決まっている、固定的なメッセージ**を表します。
  - 中に `{変数名}` を書いても、それはただの文字列として扱われ、後から `format_messages` で埋め込まれません。
  - Few-shot のお手本のように、内容が完全に決まっているメッセージや、変数を含まないシステム指示に使います。（今回の Few-shot 例の `SystemMessage`, お手本の `HumanMessage`, `AIMessage` はこれにあたります）
- **`SystemMessagePromptTemplate`, `HumanMessagePromptTemplate` (from `langchain.prompts`)**:
  - これらは**変数 `{}` を含むことができるメッセージの「テンプレート（ひな形）」** を表します。
  - `.from_template()` メソッドで `{変数名}` を含む文字列を渡して作ります。
  - `ChatPromptTemplate` の `.format_messages()` を使って、後から辞書で指定した値を変数に埋め込むことができます。
  - **メッセージ内に後から値を埋め込みたい `{変数名}` を使いたい場合は、必ずこちらの「PromptTemplate」が付くクラスを選んでください。** （今回の基本例の `SystemMessagePromptTemplate`, `HumanMessagePromptTemplate` や、Few-shot 例の最後の `HumanMessagePromptTemplate` はこれにあたります）

### Few-shot プロンプティングって？

誰かに何かを教えるとき、口頭説明だけでなく「例えばこんな感じ」と具体例を見せると分かりやすいですよね？ Few-shot プロンプティングはその考え方に似ています。AI に対して、「こんな入力（質問やテキスト）には、こんな風に答えてほしい」という具体的なやりとりの例（ユーザー入力と模範的な AI 応答のペア）をプロンプトの中にいくつか含めておく手法です。

これによって、AI は応答の口調、形式、内容、さらには思考プロセスなどを例から学び取り、よりこちらの期待に近い応答をしてくれるようになります。今回の「感情分析」の例では、`HumanMessage` と `AIMessage` のペアをお手本として使い、「この映画は最高でした！」→「ポジティブ」、「サービスが遅い…」→「ネガティブ」といった分類の具体例を AI に示しました。これにより、AI は「文章の感情をポジティブかネガティブに分類する」というタスクをより正確に理解し、新しい文章に対しても一貫した基準で、かつ期待する形式（「ポジティブ」または「ネガティブ」の一言）で応答できるようになります。これは、特定のタスクにおける AI の**精度と応答の一貫性を向上させる**のに役立ちます。

### なぜテンプレート化が重要？ ココが便利！

プロンプトテンプレートを使うと、いいことがたくさんあります。

- **楽々作成＆ミス防止:** 同じパターンの指示を何度も書く必要がなくなり、タイプミスなども減らせます。
- **修正も簡単:** 指示の仕方や応答フォーマットを変えたくなったら、テンプレートの定義箇所だけ直せば OK。コードの保守性が向上します。
- **動的な情報に対応:** ユーザーからの入力や外部 API から取得したデータなどを、プロンプトの中にスムーズに組み込めます。（例：`input_sentence` に様々な文章を入れる）
- **AI との意思疎通 UP:** 特に `ChatPromptTemplate` で役割やお手本を明確にすると、AI が指示の意図や文脈、期待される出力形式をより正確に理解しやすくなります。
- **高度な機能へのステップ:** LangChain のより高度な機能（チェーンなど）を利用する際、プロンプトテンプレートは基本的な構成要素として機能します。テンプレートをしっかり作れるようになっておくことが、LangChain を使いこなすための重要な基礎になります！

---

## 5. 最終チェック：うまく動いたかな？

### 🎯 目標

- 作成したコードが正しく動作し、プロンプトテンプレートが意図した通り機能しているかを確認する。

### 確認してみよう！

コードを実行した後、以下の点を確認してみましょう。

1.  エラーメッセージが出ずに、最後まで実行できましたか？ (もし `ImportError` などが出た場合、`langchain` のバージョンやインストール状況を確認してください)
2.  `PromptTemplate のテスト` で、「LLM からの応答」が指示通り（箇条書き 3 点）の要約になっていますか？
3.  `ChatPromptTemplate のテスト (基本)` で、「LLM からの応答」が指定した言語（フランス語）での翻訳になっていますか？
4.  `ChatPromptTemplate のテスト (Few-shot: 感情分析)` で、「LLM からの応答」がお手本で示したタスクを実行し、期待通り（「ネガティブ」）に分類されていますか？応答形式も一言になっていますか？
5.  コード内の `sentiment_input_data` の `"input_sentence"` を別の文章（例：「今日は天気が良くて気持ちがいい。」）に変更して実行すると、LLM の応答もちゃんと変わりますか？（この場合は「ポジティブ」になるはずです）

---

## 6. まとめ：今回の学びと成果

### 🎯 目標

- このステップで身につけた重要な知識とスキルを整理し、自分のものにする。

### ✅ できるようになったこと！

- プロンプトテンプレートを使うメリットを理解し、実際にコードで使えるようになった。
- `PromptTemplate` を使って、変数を含む指示文字列を作成できるようになった (`langchain.prompts` を使用)。
- `ChatPromptTemplate` とメッセージテンプレート/固定メッセージを組み合わせて、役割、変数、そして Few-shot のお手本を含む指示メッセージリストを作成できるようになった (`langchain.prompts` と `langchain.schema` を使用)。
- メッセージクラス (`*Message` from `langchain.schema`) とメッセージテンプレートクラス (`*MessagePromptTemplate` from `langchain.prompts`) の重要な使い分けを理解した。
- Few-shot プロンプティングを使って、AI に特定のタスク（例：感情分析）を実行させ、応答の精度や一貫性を向上させることができるようになった。

### 🔑 学んだキーワード

- プロンプトテンプレート (`PromptTemplate`, `ChatPromptTemplate` in `langchain.prompts`)
- メッセージクラス (`SystemMessage`, `HumanMessage`, `AIMessage` in `langchain.schema`)
- メッセージテンプレートクラス (`SystemMessagePromptTemplate`, `HumanMessagePromptTemplate` in `langchain.prompts`)
- 変数埋め込み (`.format()`, `.format_messages()`)
- Few-shot プロンプティング (応答例によるタスク実行の誘導、精度向上)

---

## 🚀 次のステップへ！応答を使いこなす準備をしよう

プロンプトテンプレートを使って AI への指示を自在に組み立てるスキル、しっかり身につきましたね！これで、AI とのコミュニケーションの「入口」はバッチリです。

さて、次のステップ **ステップ 4「AI の応答を整形！Output Parser 入門」** では、AI とのコミュニケーションの「出口」を整えます。AI からの応答は、そのままではただの文字列。これをプログラムで扱いやすい「構造化されたデータ」（例えば Python の辞書や特定のオブジェクト）に変換するテクニック、**Output Parser** を学びます。

まずは AI の応答を確実に受け取るための Output Parser の世界へ進みましょう！
