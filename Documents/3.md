AI とのコミュニケーション、もっとスムーズにできたら嬉しいですよね。AI への指示である「プロンプト」は、そのための大切な「指示書」。このステップでは、その指示書作りをレベルアップさせる方法、LangChain の「プロンプトテンプレート」を学びます。これを使えば、もっと賢く、柔軟に AI と対話できるようになりますよ！

## 1. はじめに：このステップで目指すこと

### 🎯 今回のゴール

- プロンプトテンプレートを活用して、AI への指示を効率よく、そして思い通りに作成・管理できるようになる。
- **具体的な成果物:** LangChain が提供する `PromptTemplate` と `ChatPromptTemplate` という２つの道具をマスターし、状況に応じて変化する情報（変数）を含んだ指示や、AI に特定の役割を演じさせる指示を作れるようになる。

### 🔑 このステップのポイント

- **テンプレートって便利！:** なぜ指示の「ひな形」を使うと良いのか、そのメリットを実感する。
- **２つのテンプレート:** `PromptTemplate` と `ChatPromptTemplate`、それぞれの得意なことと使い分け方を理解する。
- **カスタマイズ自在:** プロンプトにユーザーの名前を入れたり、AI に「〇〇専門家」として振る舞ってもらったりする方法を身につける。

### 前提知識

- ステップ 2「AI と初対話！LLM に話しかけてみよう」を完了していること（`ChatOpenAI` と `.invoke()` の基本操作を知っている）。
- Python の基礎知識（変数、辞書、文字列の扱いなど）があること。
- ステップ 1 でセットアップした開発環境（作業フォルダ、仮想環境、API キーなど）が手元にあること。

---

## 2. 準備運動：プロンプトテンプレートってなんだろう？

### 🎯 目標

- プロンプトテンプレートの基本的な考え方と、LangChain で中心となるクラス（道具）の役割を掴む。

### なぜ「テンプレート」が必要なの？

ステップ 2 では、AI に直接質問文を投げかけました。シンプルで良い方法ですが、もう少し込み入ったことを AI にお願いしようとすると、少し物足りなくなってきます。

- **定型文を何度も…:** 「この[文書の種類]を要約してください：[本文]」のように、骨組みは同じで一部分だけ変えたい指示。
- **その時々で情報を変えたい:** 「[さん]、[場所]の明日の天気は？」のように、実行するたびに違う情報を入れたい指示。
- **AI のキャラを作りたい:** 「あなたは江戸っ子口調の AI です。」のように、AI の話し方や立場を決めたい。

こんなとき、毎回全文を手入力するのは面倒だし、ミスも増えますよね。そこで輝くのが**プロンプトテンプレート**です！これは指示文の「ひな形」で、後から情報を埋め込むための「穴（変数）」を用意したり、AI との会話における役割分担（システムからの指示、ユーザーからの質問など）を決めたりできます。

### LangChain の主な道具たち

LangChain には、主に 2 種類のプロンプトテンプレートが用意されています。

1.  **`PromptTemplate`**:

    - 最もシンプルで基本的なテンプレート。
    - 指示文の中に `{変数名}` という形で「穴」を作っておき、後から好きな文字列を埋め込めます。
    - このテンプレートから作られる最終的な指示は、**1 つの長い文字列**になります。
    - 用途：単純な指示生成、役割分担が不要な場合など。

2.  **`ChatPromptTemplate`**:

    - `ChatOpenAI` のような、**人間と自然な会話（チャット）をするのが得意な AI モデル**（チャットモデル）にピッタリなテンプレート。
    - 指示を**複数の「メッセージ」**（発言者とその内容のセット）の組み合わせで組み立てます。これにより、「これはシステムからの指示」「これはユーザーからの質問」といった役割分担が明確になります。
    - 最終的に LLM には**メッセージのリスト**が渡されます。
    - メッセージの種類には、主に以下のようなものがあります。

      - **`SystemMessage`**: システム（開発者）から AI への全体的な指示や設定。AI の役割、性格、応答スタイルなどを伝えます。（例：「あなたはユーモアあふれるアシスタントです」）
      - **`HumanMessage`**: 人間（ユーザー）の発言や質問。（例：「LangChain について教えて」）
      - **`AIMessage`**: AI 自身の発言。主に応答のお手本（Few-shot プロンプティング）を示す際に使います。（例：「LangChain ですね！それはとても便利なツールですよ。」）

    - **【重要ポイント！】** `ChatPromptTemplate` を使う際、メッセージの中に `{変数名}` を含めたい場合は、少し注意が必要です。単に `SystemMessage("あなたは{language}の専門家です")` と書くのではなく、**`SystemMessagePromptTemplate.from_template("あなたは{language}の専門家です")`** のように、**「PromptTemplate」という名前がついたクラス**を使う必要があります。これは、「このメッセージは単なる固定文ではなく、後で変数が埋まるテンプレートですよ」と LangChain に教えるためのルールです。（詳しくは「4. 深掘り解説」で！）

---

## 3. 実践タイム：テンプレートを組み立ててみよう！

### 🎯 目標

- Python コードを書いて、`PromptTemplate` と `ChatPromptTemplate` を実際に作成し、AI への指示を生成して実行結果まで確認する。

### ファイルの準備

- ステップ 1 で作った作業フォルダ内に、`step3_prompt_template.py` という名前で新しい Python ファイルを作成しましょう。

### ステップ・バイ・ステップ実装

1.  **土台の準備（インポートと LLM 初期化）:**
    まずは必要な道具（クラス）を Python コードに読み込み、LLM を使えるように準備します。

    ```python
    # step3_prompt_template.py
    import os
    from dotenv import load_dotenv
    from langchain_openai import ChatOpenAI

    # --- プロンプト関連の道具をインポート ---
    # 文字列テンプレート用
    from langchain_core.prompts import PromptTemplate
    # チャットテンプレート用 (テンプレートクラスとメッセージクラス)
    from langchain_core.prompts import (
        ChatPromptTemplate,
        SystemMessagePromptTemplate,  # システムメッセージのテンプレート用
        HumanMessagePromptTemplate,  # 人間のメッセージのテンプレート用
        # AIMessagePromptTemplate  # AIメッセージのテンプレート用 (今回は使いません)
    )
    # 変数を含まない固定メッセージ用 (Few-shotで使う)
    from langchain_core.messages import AIMessage, HumanMessage

    # 環境変数の読み込み
    load_dotenv()
    print("--- 環境変数読み込み完了 ---")

    # LLMの準備 (temperature=0 で応答を安定させる)
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
    print(f"--- LLM準備完了: {llm.model_name} (temperature={llm.temperature}) ---")
    ```

    - チャットテンプレート関連では、変数を含む可能性のある `*MessagePromptTemplate` と、変数を含まない固定メッセージ用の `*Message` をインポートしています。（`AIMessagePromptTemplate` は今回は使いませんが、存在は覚えておきましょう）

2.  **`PromptTemplate` でシンプルに指示:**
    テキストとその要約形式を後から指定できる、シンプルな指示テンプレートを作ってみましょう。

    ```python
    # step3_prompt_template.py (続き)

    print("\n--- PromptTemplate のテスト ---")
    # 1. 指示の「ひな形」を文字列で定義 ({変数名} で穴埋め箇所を指定)
    template_string = """
    以下のテキストを指定された形式で要約してください。

    テキスト:
    {input_text}

    要約形式:
    {output_format}
    """
    # 2. 文字列から PromptTemplate オブジェクトを作成
    prompt_template = PromptTemplate.from_template(template_string)

    # 3. 穴埋めする内容を辞書で準備
    input_data = {
        "input_text": "LangChainは、大規模言語モデル（LLM）を活用したアプリケーション開発を容易にするためのフレームワークです。多様なコンポーネントを組み合わせて、複雑なワークフローを構築できます。",
        "output_format": "箇条書きで3点"
    }

    # 4. テンプレートにデータを埋め込み、最終的な指示文字列を生成
    #    .format() メソッドを使うのが簡単です。
    final_prompt_string = prompt_template.format(**input_data)
    print("【生成されたプロンプト文字列】:")
    print(final_prompt_string)

    # 5. 生成した指示文字列をLLMに渡して実行
    print("\n【LLMへの入力 (文字列)】:")
    print(final_prompt_string)
    response = llm.invoke(final_prompt_string)
    print("\n【LLMからの応答 (PromptTemplate)】:")
    print(response.content)
    ```

    - `.from_template()` でテンプレートを作り、`.format()` でデータを埋め込んで最終的な指示**文字列**を得ています。
    - この**文字列**を `llm.invoke()` に渡すのがポイントです。

3.  **`ChatPromptTemplate` で役割分担 (基本例):**
    チャットモデルが得意な、役割分担のある指示を作ってみましょう。AI に翻訳家の役割をお願いし、翻訳対象の言語とテキストを変数で指定します。

    ```python
    # step3_prompt_template.py (続き)

    print("\n--- ChatPromptTemplate のテスト (基本) ---")
    # 1. 各役割のメッセージテンプレートを作成
    #    【重要】変数を含む場合は *MessagePromptTemplate.from_template() を使う！
    system_template = SystemMessagePromptTemplate.from_template(
        "あなたは{language}の翻訳家です。丁寧な言葉遣いで回答してください。"
    )
    human_template = HumanMessagePromptTemplate.from_template(
        "{text_to_translate} を翻訳してください。"
    )

    # 2. メッセージテンプレートをリストにまとめて ChatPromptTemplate を作成
    chat_template = ChatPromptTemplate.from_messages([system_template, human_template])

    # 3. 穴埋めする内容を辞書で準備
    chat_input_data = {
        "language": "フランス語",
        "text_to_translate": "Hello, how are you?"
    }

    # 4. テンプレートにデータを埋め込み、最終的なメッセージリストを生成
    final_prompt_messages = chat_template.format_messages(**chat_input_data)
    print("【生成されたプロンプトメッセージ (リスト)】:")
    # [SystemMessage(...), HumanMessage(...)] という形式のリストが出力されます
    print(final_prompt_messages)

    # 5. 生成したメッセージリストをLLMに渡して実行
    print("\n【LLMへの入力 (メッセージリスト)】:")
    print(final_prompt_messages)
    response_chat = llm.invoke(final_prompt_messages)
    print("\n【LLMからの応答 (ChatPromptTemplate 基本)】:")
    print(response_chat.content)
    ```

    - 変数を含むメッセージには `SystemMessagePromptTemplate` や `HumanMessagePromptTemplate` を使いました。これが正しい方法です。
    - `.format_messages()` で生成されるのは、メッセージオブジェクトの**リスト**です。

4.  **`ChatPromptTemplate` でお手本を示す (Few-shot 例):**
    AI に応答のお手本（Few-shot）を見せて、特定のスタイル（ここでは「簡潔な一言」）で答えてもらうように誘導しましょう。

    ```python
    # step3_prompt_template.py (続き)

    print("\n--- ChatPromptTemplate のテスト (Few-shot) ---")
    # 1. メッセージテンプレートのリストを作成
    message_templates = [
        # AIの役割設定
        SystemMessagePromptTemplate.from_template("あなたはユーザーの質問に簡潔に一言で答えるアシスタントです。"),
        # --- ここから Few-shot のお手本 ---
        # お手本の会話 (内容は固定なので、直接 HumanMessage/AIMessage を使う)
        HumanMessage(content="今日の天気は？"),
        AIMessage(content="晴れです。"),
        HumanMessage(content="明日の天気は？"),
        AIMessage(content="曇り時々雨です。"),
        # --- ここまで Few-shot のお手本 ---
        # --- 実際にユーザーが質問する部分（ここには変数が必要） ---
        HumanMessagePromptTemplate.from_template("{user_question}")
    ]
    # 2. ChatPromptTemplate を組み立てる
    few_shot_template = ChatPromptTemplate.from_messages(message_templates)

    # 3. 穴埋めする内容を辞書で準備（質問内容をより自然なものに変更）
    few_shot_input_data = {
        "user_question": "おすすめのプログラミング言語は？"
    }

    # 4. テンプレートにデータを埋め込み、最終的なメッセージリストを生成
    final_few_shot_messages = few_shot_template.format_messages(**few_shot_input_data)
    print("【生成されたプロンプトメッセージ (Few-shot)】:")
    print(final_few_shot_messages)

    # 5. 生成したメッセージリストをLLMに渡して実行
    print("\n【LLMへの入力 (Few-shot メッセージリスト)】:")
    print(final_few_shot_messages)
    response_few_shot = llm.invoke(final_few_shot_messages)
    print("\n【LLMからの応答 (ChatPromptTemplate Few-shot)】:")
    # お手本に倣って簡潔な応答が期待される
    print(response_few_shot.content)

    print("\n--- 処理終了 ---")
    ```

    - お手本部分は固定的なので `HumanMessage` と `AIMessage` を直接使用。
    - ユーザーの実際の質問部分には変数 `{user_question}` が必要なので `HumanMessagePromptTemplate` を使用。
    - 質問例を「おすすめのプログラミング言語は？」に変更しました。

#### 完成コード (`step3_prompt_template.py`)

```python
# step3_prompt_template.py
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

# --- プロンプト関連の道具をインポート ---
from langchain_core.prompts import PromptTemplate
from langchain_core.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate
)
from langchain_core.messages import AIMessage, HumanMessage

# 環境変数の読み込み
load_dotenv()
print("--- 環境変数読み込み完了 ---")

# LLMの準備 (temperature=0 で応答を安定させる)
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
print(f"--- LLM準備完了: {llm.model_name} (temperature={llm.temperature}) ---")

# --- PromptTemplate のテスト ---
print("\n--- PromptTemplate のテスト ---")
template_string = """
以下のテキストを指定された形式で要約してください。

テキスト:
{input_text}

要約形式:
{output_format}
"""
prompt_template = PromptTemplate.from_template(template_string)
input_data = {
    "input_text": "LangChainは、大規模言語モデル（LLM）を活用したアプリケーション開発を容易にするためのフレームワークです。多様なコンポーネントを組み合わせて、複雑なワークフローを構築できます。",
    "output_format": "箇条書きで3点"
}
final_prompt_string = prompt_template.format(**input_data)
print("【生成されたプロンプト文字列】:")
print(final_prompt_string)
print("\n【LLMへの入力 (文字列)】:")
print(final_prompt_string)
response = llm.invoke(final_prompt_string)
print("\n【LLMからの応答 (PromptTemplate)】:")
print(response.content)

# --- ChatPromptTemplate のテスト (基本) ---
print("\n--- ChatPromptTemplate のテスト (基本) ---")
system_template = SystemMessagePromptTemplate.from_template(
    "あなたは{language}の翻訳家です。丁寧な言葉遣いで回答してください。"
)
human_template = HumanMessagePromptTemplate.from_template(
    "{text_to_translate} を翻訳してください。"
)
chat_template = ChatPromptTemplate.from_messages([system_template, human_template])
chat_input_data = {
    "language": "フランス語",
    "text_to_translate": "Hello, how are you?"
}
final_prompt_messages = chat_template.format_messages(**chat_input_data)
print("【生成されたプロンプトメッセージ (リスト)】:")
print(final_prompt_messages)
print("\n【LLMへの入力 (メッセージリスト)】:")
print(final_prompt_messages)
response_chat = llm.invoke(final_prompt_messages)
print("\n【LLMからの応答 (ChatPromptTemplate 基本)】:")
print(response_chat.content)

# --- ChatPromptTemplate のテスト (Few-shot) ---
print("\n--- ChatPromptTemplate のテスト (Few-shot) ---")
message_templates = [
    SystemMessagePromptTemplate.from_template("あなたはユーザーの質問に簡潔に一言で答えるアシスタントです。"),
    # --- ここから Few-shot のお手本 ---
    HumanMessage(content="今日の天気は？"),
    AIMessage(content="晴れです。"),
    HumanMessage(content="明日の天気は？"),
    AIMessage(content="曇り時々雨です。"),
    # --- ここまで Few-shot のお手本 ---
    # --- 実際にユーザーが質問する部分（ここには変数が必要） ---
    HumanMessagePromptTemplate.from_template("{user_question}")
]
few_shot_template = ChatPromptTemplate.from_messages(message_templates)
# 質問例を変更
few_shot_input_data = {
    "user_question": "おすすめのプログラミング言語は？"
}
final_few_shot_messages = few_shot_template.format_messages(**few_shot_input_data)
print("【生成されたプロンプトメッセージ (Few-shot)】:")
print(final_few_shot_messages)
print("\n【LLMへの入力 (Few-shot メッセージリスト)】:")
print(final_few_shot_messages)
response_few_shot = llm.invoke(final_few_shot_messages)
print("\n【LLMからの応答 (ChatPromptTemplate Few-shot)】:")
print(response_few_shot.content)

print("\n--- 処理終了 ---")
```

### 実行結果の例

- **実行コマンド:** （仮想環境を有効にして） `python step3_prompt_template.py`
- **期待される出力例:**

  ```
  --- 環境変数読み込み完了 ---
  --- LLM準備完了: gpt-3.5-turbo (temperature=0.0) ---

  --- PromptTemplate のテスト ---
  【生成されたプロンプト文字列】:

  以下のテキストを指定された形式で要約してください。

  テキスト:
  LangChainは、大規模言語モデル（LLM）を活用したアプリケーション開発を容易にするためのフレームワークです。多様なコンポーネントを組み合わせて、複雑なワークフローを構築できます。

  要約形式:
  箇条書きで3点

  【LLMへの入力 (文字列)】:

  以下のテキストを指定された形式で要約してください。

  テキスト:
  LangChainは、大規模言語モデル（LLM）を活用したアプリケーション開発を容易にするためのフレームワークです。多様なコンポーネントを組み合わせて、複雑なワークフローを構築できます。

  要約形式:
  箇条書きで3点

  【LLMからの応答 (PromptTemplate)】:
  - LangChainは、大規模言語モデル（LLM）を利用したアプリケーション開発を支援するフレームワーク。
  - 多様なコンポーネントを組み合わせて、複雑なワークフローを構築可能。
  - 開発を容易にするための機能を提供。

  --- ChatPromptTemplate のテスト (基本) ---
  【生成されたプロンプトメッセージ (リスト)】:
  [SystemMessage(content='あなたはフランス語の翻訳家です。丁寧な言葉遣いで回答してください。'), HumanMessage(content='Hello, how are you? を翻訳してください。')]
  【LLMへの入力 (メッセージリスト)】:
  [SystemMessage(content='あなたはフランス語の翻訳家です。丁寧な言葉遣いで回答してください。'), HumanMessage(content='Hello, how are you? を翻訳してください。')]
  【LLMからの応答 (ChatPromptTemplate 基本)】:
  Bonjour, comment allez-vous ?

  --- ChatPromptTemplate のテスト (Few-shot) ---
  【生成されたプロンプトメッセージ (Few-shot)】:
  [SystemMessage(content='あなたはユーザーの質問に簡潔に一言で答えるアシスタントです。'), HumanMessage(content='今日の天気は？'), AIMessage(content='晴れです。'), HumanMessage(content='明日の天気は？'), AIMessage(content='曇り時々雨です。'), HumanMessage(content='おすすめのプログラミング言語は？')]
  【LLMへの入力 (Few-shot メッセージリスト)】:
  [SystemMessage(content='あなたはユーザーの質問に簡潔に一言で答えるアシスタントです。'), HumanMessage(content='今日の天気は？'), AIMessage(content='晴れです。'), HumanMessage(content='明日の天気は？'), AIMessage(content='曇り時々雨です。'), HumanMessage(content='おすすめのプログラミング言語は？')]
  【LLMからの応答 (ChatPromptTemplate Few-shot)】:
  Pythonです。

  --- 処理終了 ---
  ```

  - **注意:** LLM の応答は微妙に変わることがあります。Few-shot の応答も「Python です。」になるとは限りませんが、簡潔な一言になるはずです。

---

## 4. 深掘り解説：テンプレートをもっとよく知る

### 🎯 目標

- `PromptTemplate` と `ChatPromptTemplate` の適切な使い分け、メッセージクラスの役割分担、そしてテンプレート化のメリットについて理解を深める。

### `PromptTemplate` vs `ChatPromptTemplate`：どっちを使う？

- **`PromptTemplate`（出力: 文字列）**:
  - シンプルな指示文が必要なときに。
  - 最終的に AI に渡したいのが**一つのまとまったテキスト**の場合。
  - **注意点:** `PromptTemplate` オブジェクト自体は、LLM の `.invoke()` メソッドに直接渡すことはできません。`.format()` などで**生成された文字列**を `.invoke()` に渡す必要があります。
- **`ChatPromptTemplate`（出力: メッセージリスト）**:
  - `ChatOpenAI` などの**チャットモデルを使う場合の標準的な選択肢**。こちらを使うのがおすすめです。
  - AI に役割を与えたり（System）、ユーザー（Human）と AI（AI）の対話を明確に区別したいときに。
  - Few-shot プロンプティングでお手本を示したいときに。
  - モデルが文脈や役割を理解しやすくなり、より自然で適切な応答を引き出しやすくなります。
  - 生成される**メッセージリスト**をそのまま `.invoke()` に渡せます。

### メッセージクラスの使い分け【重要！】

`ChatPromptTemplate` を使いこなす上で、この区別はとても大切です！

- **`SystemMessage`, `HumanMessage`, `AIMessage`**:
  - これらは**内容が決まっている、固定的なメッセージ**を表します。
  - 中に `{変数名}` を書いても、それはただの文字列として扱われ、後から埋め込まれません。
  - Few-shot のお手本のように、内容が完全に決まっているメッセージに使います。
- **`SystemMessagePromptTemplate`, `HumanMessagePromptTemplate`, `AIMessagePromptTemplate`**:
  - これらは**変数 `{}` を含むことができるメッセージの「テンプレート（ひな形）」** を表します。
  - `.from_template()` メソッドで `{変数名}` を含む文字列を渡して作ります。
  - `.format_messages()` を使って、後から辞書で指定した値を変数に埋め込むことができます。
  - **メッセージ内に `{変数名}` を使いたい場合は、必ずこちらの「PromptTemplate」が付くクラスを選んでください。**

### Few-shot プロンプティングって？

誰かに何かを教えるとき、口頭説明だけでなく「例えばこんな感じ」と具体例を見せると分かりやすいですよね？ Few-shot プロンプティングはその考え方に似ています。AI に対して、「こんな質問には、こんな風に答えてほしい」という具体的な会話の例（ユーザー質問と模範的な AI 応答のペア）をプロンプトの中にいくつか含めておく手法です。これによって、AI は応答の口調、形式、内容などを例から学び取り、よりこちらの期待に近い応答をしてくれるようになります。今回の例では、`HumanMessage` と `AIMessage` のペアをお手本として使いました。

### なぜテンプレート化が重要？ ココが便利！

プロンプトテンプレートを使うと、いいことがたくさんあります。

- **楽々作成＆ミス防止:** 同じパターンの指示を何度も書く必要がなくなり、タイプミスなども減らせます。
- **修正も簡単:** 指示の仕方を変えたくなったら、テンプレートの定義箇所だけ直せば OK。
- **動的な情報に対応:** ユーザーからの入力や外部で取得したデータなどを、プロンプトの中にスムーズに組み込めます。
- **AI との意思疎通 UP:** 特に `ChatPromptTemplate` で役割を明確にすると、AI が指示の意図や文脈をより正確に理解しやすくなります。
- **LCEL へのパスポート:** 次のステップで学ぶ **LCEL (LangChain Expression Language)** では、これらのプロンプトテンプレートが基本的な「部品（Runnable）」として大活躍します。テンプレートをしっかり作れるようになっておくことが、LCEL を使いこなすための重要な基礎になります！

---

## 5. 最終チェック：うまく動いたかな？

### 🎯 目標

- 作成したコードが正しく動作し、プロンプトテンプレートが意図した通り機能しているかを確認する。

### 確認してみよう！

コードを実行した後、以下の点を確認してみましょう。

1.  エラーメッセージが出ずに、最後まで実行できましたか？
2.  `PromptTemplate のテスト` で、「生成されたプロンプト文字列」に変数が埋め込まれ、「LLM からの応答」が指示通りの要約になっていますか？
3.  `ChatPromptTemplate のテスト (基本)` で、「生成されたプロンプトメッセージ (リスト)」に変数が埋め込まれ、「LLM からの応答」が指定した言語（フランス語）になっていますか？
4.  `ChatPromptTemplate のテスト (Few-shot)` で、「生成されたプロンプトメッセージ (Few-shot)」に変数が埋め込まれ、「LLM からの応答」がお手本（簡潔な一言）に近いスタイル（例: "Python です。"）になっていますか？
5.  **(おまけ)** コード内の `input_data` や `chat_input_data` の値（例えば `"language"` を `"スペイン語"` に変えるなど）を変更して実行すると、LLM の応答もちゃんと変わりますか？

すべて自信を持って「はい！」と答えられたら、このステップはクリアです！

---

## 6. まとめ：今回の学びと成果

### 🎯 目標

- このステップで身につけた重要な知識とスキルを整理し、自分のものにする。

### ✅ できるようになったこと！

- プロンプトテンプレートを使うメリットを理解し、実際にコードで使えるようになった。
- `PromptTemplate` を使って、変数を含む指示文字列を作成できるようになった。
- `ChatPromptTemplate` とメッセージテンプレート（`System/HumanMessagePromptTemplate`など）を組み合わせて、役割や変数を持つ指示メッセージリストを作成できるようになった。
- メッセージクラス (`*Message`) とメッセージテンプレートクラス (`*MessagePromptTemplate`) の重要な使い分けを理解した。

### 🔑 学んだキーワード

- プロンプトテンプレート (`PromptTemplate`, `ChatPromptTemplate`)
- メッセージ (`SystemMessage`, `HumanMessage`, `AIMessage`)
- メッセージテンプレート (`SystemMessagePromptTemplate`, `HumanMessagePromptTemplate`)
- 変数埋め込み (`.format()`, `.format_messages()`)
- Few-shot プロンプティング

## 🚀 次のステップへ！

AI への指示書＝プロンプトを自在に操るための基礎が固まりましたね！いよいよ次は、LangChain の強力な機能、**LCEL (LangChain Expression Language)** の登場です。

LCEL は、今回作ったようなプロンプトテンプレート、LLM、そして様々なデータ処理ツールなどを、まるでレゴブロックのように**パイプ `|` で繋ぎ合わせて**、複雑な AI アプリケーションの流れ（チェーン）を驚くほどシンプルに記述できる仕組みです。

今回マスターしたテンプレート作成スキルは、LCEL でパワフルなチェーンを組み立てるための必須要素。準備は OK ですか？さらに高度な AI アプリケーション開発の世界へ、一緒に進んでいきましょう！
