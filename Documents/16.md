前のステップでは、テキストデータを意味で検索するための基盤となる **Vector Store** を構築し、基本的な検索方法を学びました。これで、大量の文書の中から質問に関連しそうな部分を見つけ出す準備が整いました。

ここからは、**RAG (Retrieval-Augmented Generation)** という、AI をより効果的に活用する技術の構築に進みます。RAG は、「AI が応答を生成する前に、外部から関連情報を検索し、その情報に基づいて応答を形成する」というアプローチです。

このステップでは、RAG プロセスの最初の重要な要素である **「検索 (Retrieve)」** 、すなわち Vector Store から関連情報を効率的に取得する **Retriever (レトリバー)** の使い方を学びます。Retriever は、質問応答に必要な **Context (文脈情報)** を見つけ出す役割を担います。それでは、Retriever の実装方法を解説します。

## 1. はじめに：このステップで目指すこと

### 🎯 今回のゴール

このステップを完了すると、以下のことができるようになります。

- RAG における **Retriever** の役割と重要性を理解します。
- ステップ 15 で作成した Vector Store から、**`.as_retriever()`** メソッドを用いて Retriever オブジェクトを作成できるようになります。
- 作成した Retriever を使用し、特定の質問に関連する文書チャンク (Context) のリストを実際に取得し、その内容を確認できるようになります。
- **具体的な成果物:** FAISS Vector Store から Retriever を作成し、質問を投げて関連性の高い **Document** オブジェクトのリストを取得する Python コードを作成します。

### 🔑 このステップのポイント

このステップで特に重要な考え方や技術です。

- **RAG (Retrieval-Augmented Generation):** 検索・拡張・生成のプロセス。
- **Retriever:** Vector Store などから関連情報を取得するための LangChain のコンポーネント。
- **Context:** Retriever によって取得された、質問応答の根拠となる関連情報（通常は **Document** オブジェクトのリスト）。
- **`.as_retriever()`:** Vector Store オブジェクトから Retriever を作成するためのメソッド。
- **`.invoke()`:** Retriever を使用して関連文書を取得するためのメソッド。

### ✅ 前提知識

このステップに進む前に、以下の準備と知識があるとスムーズです。

- **ステップ 15「情報の検索基盤！Vector Store 基礎」の内容:** **FAISS** Vector Store にデータを格納して、`.similarity_search()` などで検索した経験。**Document** オブジェクトの構造 ( `page_content`, `metadata` ) の理解。
- **ステップ 14「テキストをベクトルに！Embedding 入門」の内容:** テキストをベクトル化する `Embedding` の基本の理解。
- **Python の基本:** 関数の呼び出しやリストの操作など、基本的な文法。
- **必要なライブラリ:** `faiss-cpu` (または `faiss-gpu`), `langchain`, `langchain-community`, `langchain-openai` などがインストールされていること。
- **OpenAI API キー:** 環境変数 `.env` などに設定されていること。

準備が整っていれば、Retriever の具体的な使い方に進みます。

---

## 2. 準備運動：Retriever の基本を理解する

### 🎯 目標

RAG システムにおける Retriever の役割と、Vector Store (FAISS) から Retriever を作成して文書を取得する基本的な流れを理解します。

### Retriever の役割：関連情報の取得

AI が質問に対して正確な応答を生成するためには、質問に関連する適切な情報 (Context) を参照することが重要です。**Retriever** は、この Context を情報源 (Vector Store など) から効率的に探し出す役割を担います。

前のステップで試した `.similarity_search()` と似ていますが、Retriever はより汎用的なインターフェースであり、様々な検索戦略を適用するための基盤となります。

Retriever の基本的な機能は、
**「入力 (通常は質問文字列) を受け取り、それに関連する **Document** オブジェクトのリスト (Context) を返す」**
ことです。

### Vector Store から Retriever を作成する方法：`.as_retriever()`

LangChain の Vector Store オブジェクト (今回使用する FAISS オブジェクトを含む) には、**`.as_retriever()`** というメソッドが用意されています。これを呼び出すことで、その Vector Store を利用する Retriever を簡単に作成できます。

```python
# vector_store はステップ 15 で作成した FAISS オブジェクトとします
retriever = vector_store.as_retriever()
# これで、FAISS Vector Store 用の Retriever が作成されます
```

### Retriever を使用して Context を取得する方法：`.invoke()`

作成した Retriever オブジェクトは、**`.invoke()`** メソッドを持っています。（古いバージョンの LangChain では `.get_relevant_documents()` というメソッドもありましたが、現在は `.invoke()` の使用が推奨されます。）このメソッドに質問文字列を渡すことで、関連する **Document** のリストを取得できます。

```python
query = "LangChain について教えてください。"
# invoke メソッドで質問します
relevant_docs = retriever.invoke(query)

# relevant_docs に関連性の高い Document のリストが格納されます
print(relevant_docs)
```

この `relevant_docs` が RAG の最初のステップの成果物、Context です。

### API コストに関する考慮事項

もし OpenAI Embedding のような API ベースの Embedding モデルを使用している場合、Retriever が内部で質問文をベクトル化する際にも **API コールが発生し、コストがかかる** 点に留意してください。Vector Store 作成時だけでなく、検索時にもコストが発生する可能性があります。

---

## 3. 実践タイム：Retriever で Context を取得する

### 🎯 目標

Python コードを記述し、準備した FAISS Vector Store から Retriever を作成し、質問に関連する Context を取得するプロセスを実行します。

### ステップ・バイ・ステップ実装

#### 1. 準備 (インポート、Embedding、FAISS Vector Store の準備):

まず、必要なライブラリをインポートし、Embedding モデル、そして FAISS Vector Store を用意します。（ステップ 15 の `vector_store` がなければ、以下のコードで作成できます。）

```python
# step16_rag_retriever_context_revised.py
import os
import sys
from dotenv import load_dotenv

# --- 必要な LangChain モジュール ---
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
# FAISS Vector Store をインポート
try:
    from langchain_community.vectorstores import FAISS
    print("FAISS をインポートしました (from langchain_community.vectorstores)。")
except ImportError:
    print("エラー: langchain-community が見つかりません。")
    print("   'pip install langchain-community' を実行してください。")
    sys.exit(1)

# FAISS 本体も確認
try:
    import faiss
    print(f"faiss バージョン: {faiss.__version__}")
except ImportError:
    print("エラー: faiss がインストールされていません。")
    print("   'pip install faiss-cpu' または 'pip install faiss-gpu' を実行してください。")
    sys.exit(1)

print("--- 必要なモジュールのインポート完了 ---")

load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    print("エラー: OpenAI API キーが設定されていません。")
    sys.exit(1)
else:
    print("OpenAI API キーを読み込みました。")

# --- Embedding モデルの準備 ---
try:
    # 注意: モデル名は最新のものを確認してください
    embeddings_model = OpenAIEmbeddings(model="text-embedding-3-small", api_key=openai_api_key)
    print(f"--- Embedding モデル準備完了 ({embeddings_model.model}) ---")
except Exception as e:
    print(f"エラー: Embedding モデルの初期化に失敗しました: {e}")
    sys.exit(1)

# --- FAISS Vector Store の準備 (インメモリで作成) ---
sample_documents = [
    Document(page_content="LangGraph は状態を持つ複雑なグラフを構築できます。", metadata={"source": "doc-a", "page": 1, "category": "graph"}),
    Document(page_content="LCEL は LangChain Expression Language の略です。", metadata={"source": "doc-b", "page": 1, "category": "core"}),
    Document(page_content="エージェントはツールを使ってタスクを実行します。", metadata={"source": "doc-c", "page": 1, "category": "agent"}),
    Document(page_content="ベクトルストアは埋め込みベクトルを高速に検索します。", metadata={"source": "doc-d", "page": 1, "category": "vectorstore"}),
    Document(page_content="LangGraph ではノード間で状態が共有されます。", metadata={"source": "doc-a", "page": 2, "category": "graph"}),
    Document(page_content="RAG は検索拡張生成の略で、外部知識を利用します。", metadata={"source": "doc-e", "page": 1, "category": "rag"}),
    Document(page_content="プロンプトテンプレートを使うと入力を動的にできます。", metadata={"source": "doc-f", "page": 1, "category": "core"}),
]
try:
    vector_store = FAISS.from_documents(
        documents=sample_documents,
        embedding=embeddings_model
    )
    print("--- Vector Store (FAISS インメモリ) 準備完了 ---")
except Exception as e:
    print(f"エラー: Vector Store (FAISS) の作成中にエラーが発生しました: {e}")
    sys.exit(1)

```

#### 2. Retriever の作成 (`.as_retriever()`):

準備した `vector_store` (FAISS オブジェクト) から Retriever を作成します。検索で取得する件数 (k) も指定します。

```python
# step16_rag_retriever_context_revised.py (続き)

print("\n--- Retriever の作成 ---")
try:
    # FAISS Vector Store から .as_retriever() で Retriever を作成
    # search_kwargs で検索の挙動を指定
    retriever = vector_store.as_retriever(
        search_type="similarity", # 類似度で検索 (これが基本)
        search_kwargs={'k': 2}    # 上位2件を取得する設定
    )
    print("Retriever 作成完了。")
    print(f"  検索タイプ: {retriever.search_type}")
    print(f"  取得件数 (k): {retriever.search_kwargs.get('k')}")

except Exception as e:
    print(f"エラー: Retriever の作成中にエラーが発生しました: {e}")
    sys.exit(1)

```

#### 3. Context の取得 (`.invoke()`):

作成した Retriever に質問し、関連する Context を取得します。

```python
# step16_rag_retriever_context_revised.py (続き)

# --- Context を取得するための質問 ---
query = "LangGraph の状態管理について教えて" # 聞きたいことを入力
print(f"\n--- Context の取得 (検索クエリ: '{query}') ---")

try:
    # .invoke() メソッドで Retriever に検索を依頼
    context_documents = retriever.invoke(query)

    print(f"取得した Context (Document リスト): {len(context_documents)} 件")

    # 取得した Context の内容を確認します
    print("\n--- 取得した Context の詳細 ---")
    if context_documents:
        for i, doc in enumerate(context_documents):
            print(f"--- Document {i+1} ---")
            print(f"  Content: {doc.page_content}")
            print(f"  Metadata: {doc.metadata}")
    else:
        print("関連する Context が見つかりませんでした。")

except Exception as e:
    # 必要に応じてエラーハンドリングを実装
    print(f"エラー: Context の取得中にエラーが発生しました: {e}")

print("\n--- 処理終了 ---")
```

### 完成コード (`step16_rag_retriever_context_revised.py`)

上記の実装 1〜3 を結合したものが、このステップの完成コードです。

### 実行結果の例

```text
# ...(初期化ログ)...
--- Vector Store (FAISS インメモリ) 準備完了 ---

--- Retriever の作成 ---
Retriever 作成完了。
  検索タイプ: similarity
  取得件数 (k): 2

--- Context の取得 (検索クエリ: 'LangGraph の状態管理について教えて') ---
取得した Context (Document リスト): 2 件

--- 取得した Context の詳細 ---
--- Document 1 ---
  Content: LangGraph ではノード間で状態が共有されます。
  Metadata: {'source': 'doc-a', 'page': 2, 'category': 'graph'}
--- Document 2 ---
  Content: LangGraph は状態を持つ複雑なグラフを構築できます。
  Metadata: {'source': 'doc-a', 'page': 1, 'category': 'graph'}

--- 処理終了 ---
```

質問に対して、LangGraph の状態に関する 2 つの関連文書が Context として取得できていることが確認できます。

---

## 4. 深掘り解説：Retriever をより深く知る

### 🎯 目標

Retriever の役割、設定可能なパラメータ、他の選択肢、そして Retriever が RAG システム全体でどのように機能するのかについて、理解を深めます。

### Retriever の役割再確認

Retriever は、RAG システムにおいて情報源 (Vector Store など) と LLM の間のインターフェースとして機能します。ユーザーの質問に基づき、最も関連性が高いと考えられる情報を効率的に取得し、**Document** のリストとして後続の処理に渡す、重要なコンポーネントです。Retriever が取得する Context の質は、最終的な LLM の回答品質に直接影響します。

### Retriever の設定 (`.as_retriever()` の引数)

`.as_retriever()` を呼び出す際に、検索の挙動をカスタマイズするための引数を指定できます。

- **`search_type`**: 検索戦略を指定します。
  - `"similarity"` (デフォルト): 質問との意味的類似度が高い順に文書を返します。
  - `"mmr"` (Maximal Marginal Relevance): 類似度だけでなく、結果の **多様性** も考慮します。類似しすぎた文書が選ばれるのを防ぐ場合に有効です。`search_kwargs` で `lambda_mult` (0〜1) パラメータで多様性を調整できます。
  - `"similarity_score_threshold"`: 類似度スコアが指定した閾値以上の文書のみを返します。
- **`search_kwargs`**: `search_type` に応じた詳細パラメータを辞書で指定します。
  - `'k'`: 取得する文書の最大数を指定します（例: `{'k': 3}`）。この **`k`の値選びは結構大事** です。小さすぎると情報不足、大きすぎるとノイズ増加やコスト増に繋がる可能性があります。一般的には **3〜5 個程度** から試行し、結果を見ながら調整します。
  - `'score_threshold'`: 類似度の閾値を指定します (`search_type="similarity_score_threshold"` の場合)。
  - `'filter'`: メタデータに基づいて検索対象を絞り込むためのフィルタ条件を指定します（例: `{'filter': {'category': 'core'}}`）。(注意: FAISS ではこの `filter` が直接機能しない場合があります。)

これらの設定を適切に調整することで、目的に合った情報検索を実現できます。

### RAG における Retriever の位置づけ

RAG (Retrieve-Augment-Generate) プロセスにおける Retriever の役割は以下の通りです。

1.  **Retrieve (検索)**: Retriever がユーザーの質問に基づき、Vector Store などから関連文書 (Context) を取得する。（**← 今回のステップ**）
2.  **Augment (拡張)**: 取得した Context と元の質問を組み合わせて、LLM への最終的なプロンプトを作成する。（次のステップで解説）
3.  **Generate (生成)**: そのプロンプトを LLM に渡し、Context を参照しながら最終的な回答を生成させる。（次のステップで解説）

Retriever はこの最初の検索ステップを担当し、その出力品質が RAG システム全体の性能に大きく寄与します。

### テキストの分割サイズ (チャンク) との関係

ステップ 13 で行ったテキスト分割（チャンキング）のサイズは、Retriever の検索精度に影響を与えます。**チャンクのサイズ** が小さすぎると十分な文脈が得られず、大きすぎると関連性の低い情報が含まれる可能性が高まります。適切なチャンクサイズと Retriever の設定（特に `k` の値）のバランスが重要です。

### LangChain の Retriever エコシステムと高度な選択肢

LangChain は、Vector Store ベースの Retriever 以外にも、様々な高度な Retriever を提供しています。

- **`MultiQueryRetriever` (ステップ 18):** 質問を複数の視点から生成し、検索結果を統合。
- **`ContextualCompressionRetriever` (ステップ 19):** 取得した文書を圧縮し、関連性の高い部分のみを抽出。
- **`ParentDocumentRetriever` (ステップ 20):** 小さなチャンクで検索し、関連する親文書全体を取得。

これらの Retriever については、今後のステップで詳しく解説します。

### Vector Store の選択肢と永続化

今回はインメモリの **FAISS** を使用しましたが、実際のアプリケーションではデータ量や同時アクセス数に応じて他の Vector Store を検討します。

- **ローカルファイルベース/ライブラリ:** `Chroma`, `FAISS` など。
- **データベース/マネージドサービス:** `Pinecone`, `Weaviate`, `Qdrant` など。

また、データを永続的に保持したい場合は **永続化** が必要です。FAISS もインデックスをファイルに保存 (`.save_local()`) できます。アプリケーションの要件に応じて、適切な Vector Store と永続化戦略を選択することが重要です。

### エラー処理について

コード例には基本的な `try...except` を含めましたが、実運用では、接続エラー、API レート制限、検索結果が空の場合など、より詳細なエラーハンドリング戦略を検討する必要があります。

---

## 5. 最終チェック：Context は取得できたか？ (FAISS 版)

### 🎯 目標

作成したコードが正しく動作し、Retriever を使用して意図した Context (**Document** リスト) が取得できているかを確認します。

### 確認してみましょう！

- **実行:** `step16_rag_retriever_context_faiss_formal.py` を実行してください。（必要なライブラリと API キーを確認）
- **エラーは出なかったか？:** エラーメッセージが表示されずに最後まで実行できましたか？
- **Retriever 作成:** 「Retriever 作成完了」と表示され、指定した `search_kwargs` (k=2) が反映されていることがログで確認できましたか？
- **Context 取得:**
  - 質問 (`query`) に対して、「取得した Context (Document リスト): 2 件」のように、`k` で指定した数の **Document** が取得できていますか？
  - 出力された **Document** の `page_content` は、質問内容に意味的に関連していると思われますか？
  - 別の質問 (`query` 変数の内容を変える) で実行してみて、取得される Context が変わることを確認しましょう。
  - `.as_retriever()` の `search_kwargs={'k': ...}` の数を変更し、取得件数が変わることを確認しましょう。

これらの点が期待通りであれば、FAISS Retriever を使った Context 取得の基本は習得できています。

---

## 6. まとめ：学びの整理と次へのステップ

### ✅ 達成したこと！

これで、RAG システムの「検索」部分を担当する Retriever を作成し、実際に FAISS を使って動かす方法を学びました。

- RAG プロセスにおける **Retriever** の役割を理解しました。
- FAISS Vector Store から **`.as_retriever()`** メソッドを使って Retriever を作成する方法を学びました。
- Retriever の **`.invoke()`** メソッドを使って、質問に関連する **Context** (**Document** のリスト) を取得する方法を実装しました。
- Retriever 作成時に **`search_kwargs`** を使用して、取得する文書数 (`k`) などの検索パラメータを指定する方法を知りました。
- API コスト、Vector Store の選択肢 (FAISS 含む)、永続化、エラー処理、チャンクサイズとの関連など、実用上の考慮点についても触れました。

### 🔑 学んだキーワード

- **RAG (Retrieval-Augmented Generation)**
- **Retriever**
- **Context**
- **Vector Store** (FAISS, Chroma, Pinecone, ...)
- **FAISS** (`langchain_community.vectorstores` より)
- **`.as_retriever()`**
- **`.invoke()`** / `.get_relevant_documents()`
- **`search_type`** (`similarity`, `mmr`, ...)
- **`search_kwargs`** (`k`, `filter` ...)
- **Document** オブジェクト
- **永続化 (Persistence)**
- **API コスト (Cost)**

### 🚀 次のステップへ！

質問に関連する情報 (Context) を取得する技術を習得しました。これで RAG の「R (Retrieve)」ステップは完了です。

次はいよいよ、この取得した Context を活用して、LLM により精度の高い回答を生成させる段階に進みます。

次の **ステップ 17「RAG 構築(2): Context 注入と LLM 生成」** では、今回取得した Context と元の質問を組み合わせ、それを LLM が理解しやすいようにプロンプトに **注入 (Augment)** し、最終的な回答を **生成 (Generate)** させる方法を学びます。LCEL を用いて、Retrieval から Generation までのプロセスを一つのチェーンとして構築し、基本的な RAG システムを完成させます。
