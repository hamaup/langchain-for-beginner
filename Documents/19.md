前のステップでは、**MultiQueryRetriever** を使って、一つの質問から複数の視点を引き出し、より広く情報を集める方法を学びましたね！ これで検索の「網羅性」が高まりました。

でも、集めてきた情報 (Context) の中に、「うーん、これは質問とちょっとズレてるかも？」とか「この文書、関連するの一文だけなんだけどな…」といったものが混じっていることもあります。LLM に渡す情報は、できるだけ**質が高く**、**簡潔**である方が、より良い回答に繋がりやすいですよね。

そこで今回のステップでは、Retriever が見つけてきた文書を**さらに**賢く処理する **ContextualCompressionRetriever** を紹介します！ これは、取得した文書をギュッと「圧縮」したり、関係ないものをフィルタリングしたりして、本当に必要な情報だけを LLM に届けるための、いわば「情報の編集者」のような Retriever なんです。情報の質を高めるための、次のステップに進みましょう！

## 1. はじめに：このステップで目指すこと

### 🎯 今回のゴール

このステップを最後まで体験すると、あなたはこんなことができるようになります！

- 最初の検索 (Retrieval) で取得した文書に、不要な情報が含まれる可能性とその課題を理解します。
- 取得した文書を、質問との関連性に基づいて「圧縮」または「フィルタリング」する **ContextualCompressionRetriever** の仕組みと目的を学びます。
- 文書を圧縮・フィルタリングするための部品である **Document Compressor** (例: **`EmbeddingsFilter`**, **`LLMChainExtractor`**) の役割を知り、使い方を習得します。
- **具体的な成果物:** ベースとなる Retriever と Document Compressor を組み合わせて **ContextualCompressionRetriever** を作成し、検索を実行して、ベース Retriever の結果と比較して、より関連性の高い、あるいは簡潔な Context が得られることを確認する Python コードを作成します。

### 🔑 このステップのポイント

今回の情報編集術で特に重要なキーワードはこちら！

- **ContextualCompressionRetriever:** ベース Retriever の結果をさらに圧縮・フィルタリングする高機能 Retriever。
- **Document Compressor:** 実際に文書の圧縮やフィルタリングを行う部品。
  - **`EmbeddingsFilter`:** Embedding の類似度スコアで文書全体をフィルタリングする、高速・低コストな Compressor。
  - **`LLMChainExtractor`:** LLM を使って文書から関連部分だけを抽出する、高品質だが高コスト・低速な Compressor。
- **Post-Retrieval Processing:** 情報を取得した「後」に、その情報を加工・編集する処理のこと。
- **関連性フィルタリング:** 質問との関連度が低い情報を除外すること。
- **情報圧縮:** 文書から不要な部分を取り除き、要点だけを残すこと。

### ✅ 前提知識

この新しい技術に挑戦する前に、以下の準備ができているか確認しましょう！

- **ステップ 18「検索改善(1): MultiQueryRetriever」の内容:** ベースとなる **Retriever** (FAISS など) が準備できていること。LLM を使ったクエリ生成の概念を理解していること。
- **Embedding と類似度:** テキストをベクトル化し、その類似度（または距離）を計算する仕組みを知っていること。
- **Python の基本:** クラスのインスタンス化、メソッド呼び出しなど。
- 必要なライブラリと API キーが設定されていること。

準備ができたら、Context を磨き上げる旅へ出発です！

---

## 2. 準備運動：なぜ検索結果を「編集」するの？

### 🎯 目標

最初の検索結果にどんな課題がありうるのか、そして **ContextualCompressionRetriever** と **Document Compressor** がどのようにしてその課題を解決しようとするのか、基本的な考え方を理解しましょう。

### 検索結果、そのままで大丈夫？

前のステップまでで作った Retriever (ベースの Retriever や MultiQueryRetriever) は、質問に「関連しそうな」文書を見つけてきてくれます。でも、その「関連しそう」な文書の中にも、

- 質問とは関係ない話題がたくさん含まれている。
- 全体としては関連が薄いけど、ほんの一部分だけが重要。
- 実はあまり関連性が高くないものが、検索結果の上位に紛れ込んでいる。

といったことが起こりえます。これらの「ノイズ」が多い Context をそのまま LLM に渡してしまうと、LLM が混乱したり、関係ない情報に基づいて回答してしまったり、あるいは単純に処理する情報量が多すぎてコストや時間が増えたりする可能性があります。

### ContextualCompressionRetriever のアイデア：取得「後」に質を高める！

そこで **ContextualCompressionRetriever** の出番です！ これは、通常の Retriever（これを **ベース Retriever** と呼びます）が文書を見つけてきた「後」に、もうひと手間加えることで、Context の質を高めようという考え方に基づいています。

その「ひと手間」を行うのが **Document Compressor** という部品です。ContextualCompressionRetriever は、この Compressor と連携して、以下のような動きをします。

1.  **まず普通に検索:** まず、設定された **ベース Retriever** が、いつも通りに質問に関連しそうな文書を探してきます。
2.  **編集者 (Compressor) にお任せ:** 次に、見つかった文書一つ一つと、元の質問を **Document Compressor** に渡します。「この文書、質問と本当に関係ある？」「関係あるなら、どの部分が大事？」と尋ねるイメージです。
3.  **圧縮 or フィルタリング:** Compressor は、その文書を分析し、
    - 「うーん、これは関係ないな」と判断したら、その文書を**丸ごと削除**します (フィルタリング)。
    - 「関係あるけど、大事なのはここだけだな」と判断したら、文書の中から**関連性の高い部分だけを抽出**します (圧縮)。
    - 「うん、これは全部大事だ」と判断したら、そのまま通します。
4.  **最終 Context をお届け:** こうして編集された、より質の高い文書（またはその抜粋）のリストが、最終的な Context として返されます。

### Document Compressor の種類：編集方針いろいろ！

どんな風に編集するかは、使う **Document Compressor** によって変わります。代表的なものには、

- **`EmbeddingsFilter`:**
  - 仕組み: 取得した各文書の Embedding と、質問の Embedding の**類似度**を再度計算し、設定した閾値（ボーダーライン）に満たない文書は**丸ごと削除**します。
  - 特徴: 非常に**高速**で **API コストも（通常は）かからない** ので手軽に使えますが、文書の一部分だけを抜き出すことはできません。
- **`LLMChainExtractor`:**
  - 仕組み: 取得した各文書と質問を **LLM** に渡し、「この文書の中から、質問に関連する部分だけ抜き出して」とお願いして、その**抜粋**を返します。関連性が低いと判断されれば、何も返さない（＝削除）こともあります。
  - 特徴: 文脈を理解した上で**重要な部分だけを的確に抽出**できる可能性があり高品質ですが、文書ごとに LLM を呼び出すため、**非常に時間がかかり、API コストも高く**なります。

今回は、手軽に試せる **`EmbeddingsFilter`** を中心に見ていきましょう！

---

## 3. 実践タイム：ContextualCompressionRetriever を使ってみよう！

### 🎯 目標

実際に Python コードを書き、ベース Retriever と **`EmbeddingsFilter`** を使って **ContextualCompressionRetriever** を作成し、検索を実行して、その効果（取得される文書の変化）を確認します。

### ステップ・バイ・ステップ実装

#### 1. 準備 (インポート、既存セットアップの流用):

まず、`ContextualCompressionRetriever` と `EmbeddingsFilter` をインポートします。そして、ステップ 18 までと同様に、LLM, Embedding モデル, FAISS Vector Store, ベースとなる Retriever を準備します。

```python
# step19_rag_compression.py
import os
import sys
from dotenv import load_dotenv

# --- 必要な LangChain モジュール ---
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.documents import Document
# Contextual Compression Retriever と EmbeddingsFilter をインポート
try:
    from langchain.retrievers import ContextualCompressionRetriever
    # Document Compressors は通常 langchain 本体から
    from langchain.retrievers.document_compressors import EmbeddingsFilter
    print("ContextualCompressionRetriever と EmbeddingsFilter をインポートしました。")
    # 必要なら: pip install langchain
except ImportError:
    print("エラー: ContextualCompressionRetriever または EmbeddingsFilter が見つかりません。")
    print("   'pip install langchain' を確認してください。")
    sys.exit(1)

# Vector Store (FAISS)
try:
    from langchain_community.vectorstores import FAISS
    import faiss
    print("FAISS をインポートしました。")
except ImportError:
    print("エラー: FAISS または langchain-community が見つかりません。")
    sys.exit(1)

print("--- 必要なモジュールのインポート完了 ---")

# --- ステップ 17/18 と同様の準備 ---
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
# (APIキーチェック) ...
print("OpenAI API キーを読み込みました。")
# (Embedding モデルと LLM の準備) ...
try:
    embeddings_model = OpenAIEmbeddings(model="text-embedding-3-small", api_key=openai_api_key)
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0, api_key=openai_api_key) # Extractorで使う可能性も考慮
    print(f"--- Embedding モデルと LLM 準備完了 ---")
except Exception as e:
    print(f"エラー: モデルの初期化に失敗しました: {e}")
    sys.exit(1)
# (Vector Store とベース Retriever の準備) ...
sample_documents = [
    Document(page_content="LangChain は LLM アプリケーション開発を支援するフレームワークです。", metadata={"source": "doc-b"}),
    Document(page_content="LCEL は LangChain Expression Language の略で、チェーン構築を容易にします。", metadata={"source": "doc-b"}),
    Document(page_content="エージェントはツールを使って外部システムと対話し、タスクを実行します。", metadata={"source": "doc-c"}),
    Document(page_content="LangGraph は状態を持つ複雑なグラフを構築できます。", metadata={"source": "doc-a"}),
    Document(page_content="ベクトルストア (FAISSなど) は埋め込みの高速検索を可能にします。", metadata={"source": "doc-d"}),
    # わざと関連性の低そうな文書を追加
    Document(page_content="一般的な料理レシピ：カレーライスの作り方。", metadata={"source": "doc-recipe"}),
]
try:
    vector_store = FAISS.from_documents(sample_documents, embeddings_model)
    # ベースの Retriever は少し多めに取得するように k=4 に設定
    base_retriever = vector_store.as_retriever(search_kwargs={'k': 4})
    print("--- Vector Store (FAISS) とベース Retriever 準備完了 (k=4) ---")
except Exception as e:
    print(f"エラー: Vector Store またはベース Retriever の準備中にエラーが発生しました: {e}")
    sys.exit(1)

```

- `ContextualCompressionRetriever` と `EmbeddingsFilter` をインポートしました。
- ベース Retriever (`base_retriever`) は、後でフィルタリングされることを見越して、少し多めに `k=4` で文書を取得するように設定しました。
- わざと関連性の低そうな「カレーライスの作り方」の文書を追加して、フィルタリングの効果を見やすくしました。

#### 2. Document Compressor (EmbeddingsFilter) の作成:

`EmbeddingsFilter` のインスタンスを作成します。どの Embedding モデルを使って類似度を計算するか、そしてどのくらいの類似度（スコア）をボーダーラインにするか（閾値）を指定します。

```python
# step19_rag_compression.py (続き)

print("\n--- Document Compressor (EmbeddingsFilter) の作成 ---")

# EmbeddingsFilter を作成
# embeddings 引数には類似度計算に使う Embedding モデルを渡す
# similarity_threshold 引数で類似度の閾値を指定 (0~1、高いほど厳しい。要調整)
embeddings_filter = EmbeddingsFilter(
    embeddings=embeddings_model,
    similarity_threshold=0.76 # この値は実験して調整する必要がある
)
print("EmbeddingsFilter 作成完了！")
print(f"  使用する Embedding: {embeddings_filter.embeddings.model}")
print(f"  類似度閾値: {embeddings_filter.similarity_threshold}")

```

- `embeddings`: ベースの Vector Store で使ったものと同じ Embedding モデルを指定します。
- `similarity_threshold`: 類似度の閾値です。コサイン類似度を使う場合、通常 0 から 1 の間の値を指定し、1 に近いほど「似ている」と判断されます。どの値が良いかはデータや質問によりますが、まずは 0.7〜0.8 くらいから試して調整することが多いです (ここでは 0.76 を例としました)。

#### 3. ContextualCompressionRetriever の作成:

ベースとなる Retriever (`base_retriever`) と、作成した Compressor (`embeddings_filter`) を組み合わせて、`ContextualCompressionRetriever` を作成します。

```python
# step19_rag_compression.py (続き)

print("\n--- ContextualCompressionRetriever の作成 ---")

# ContextualCompressionRetriever を作成！
# base_compressor に使う Compressor を、base_retriever に元の Retriever を指定
compression_retriever = ContextualCompressionRetriever(
    base_compressor=embeddings_filter,
    base_retriever=base_retriever
)
print("ContextualCompressionRetriever 作成完了！")
print(f"  ベース Retriever: {type(compression_retriever.base_retriever)}")
print(f"  ベース Compressor: {type(compression_retriever.base_compressor)}")

```

- これで、`compression_retriever` が「まず `base_retriever` で探し、次に `embeddings_filter` で絞り込む」という動作をするようになりました。

#### 4. 検索の実行と比較:

作成した `compression_retriever` を使って検索を実行し、ベースの `base_retriever` の結果と比較してみましょう。

```python
# step19_rag_compression.py (続き)

# --- 検索の実行と比較 ---
query = "LangChain のコアコンセプトは？"
print(f"\n--- 検索実行 (質問: '{query}') ---")

print("\n--- 1. ベース Retriever (k=4) での検索結果 ---")
try:
    base_results = base_retriever.invoke(query)
    print(f"取得した文書数: {len(base_results)}")
    for i, doc in enumerate(base_results):
        # 類似度スコアも見てみる (FAISS は距離なので小さい方が類似)
        # score = vector_store.similarity_search_with_score(query, k=4)[i][1] # 正確ではないが目安
        print(f"  {i+1}. {doc.page_content[:50]}... (Source: {doc.metadata.get('source')})")
except Exception as e:
    print(f"エラー: ベース Retriever の検索中にエラーが発生しました: {e}")


print("\n--- 2. ContextualCompressionRetriever (EmbeddingsFilter) での検索結果 ---")
try:
    # Compression Retriever を実行！
    compressed_results = compression_retriever.invoke(query)
    print(f"圧縮/フィルタリング後の文書数: {len(compressed_results)}")
    for i, doc in enumerate(compressed_results):
        print(f"  {i+1}. {doc.page_content[:50]}... (Source: {doc.metadata.get('source')})")
    print(f"\n-> ベース Retriever の結果 ({len(base_results)}件) からフィルタリングされました。")

except Exception as e:
    print(f"エラー: Compression Retriever の検索中にエラーが発生しました: {e}")


print("\n--- 処理終了 ---")

```

- ベース Retriever (k=4) の結果と、Compression Retriever の結果を比較します。
- `EmbeddingsFilter` を使った場合、ベースの結果の中から類似度 (ここでは埋め込みベクトル間の距離) が閾値 (`similarity_threshold`) を満たさなかった文書（例：「カレーライスの作り方」）が除外され、結果の件数が減っていることが期待されます。

### 完成コード (`step19_rag_compression.py`)

上記の実装 1〜4 を結合したものが、このステップの完成コードです。

### 実行結果の例

```text
# ...(初期化ログ)...
--- Vector Store (FAISS) とベース Retriever 準備完了 (k=4) ---

--- Document Compressor (EmbeddingsFilter) の作成 ---
EmbeddingsFilter 作成完了！
  使用する Embedding: text-embedding-3-small
  類似度閾値: 0.76

--- ContextualCompressionRetriever の作成 ---
ContextualCompressionRetriever 作成完了！
  ベース Retriever: <class 'langchain_core.vectorstores.VectorStoreRetriever'>
  ベース Compressor: <class 'langchain.retrievers.document_compressors.embeddings_filter.EmbeddingsFilter'>

--- 検索実行 (質問: 'LangChain のコアコンセプトは？') ---

--- 1. ベース Retriever (k=4) での検索結果 ---
取得した文書数: 4
  1. LangChain は LLM アプリケーション開発を支援するフレームワークです。... (Source: doc-b)
  2. LCEL は LangChain Expression Language の略で、チェーン構築を容易にします。... (Source: doc-b)
  3. プロンプトテンプレートは、LLM への入力を動的に生成するための仕組みです。... (Source: doc-f)
  4. RAG は検索拡張生成の略で、外部知識を利用します。... (Source: doc-e) # 4番目に関連度が少し低いものが入る可能性

--- 2. ContextualCompressionRetriever (EmbeddingsFilter) での検索結果 ---
圧縮/フィルタリング後の文書数: 3 # <- 1件減った！
  1. LangChain は LLM アプリケーション開発を支援するフレームワークです。... (Source: doc-b)
  2. LCEL は LangChain Expression Language の略で、チェーン構築を容易にします。... (Source: doc-b)
  3. プロンプトテンプレートは、LLM への入力を動的に生成するための仕組みです。... (Source: doc-f)

-> ベース Retriever の結果 (4件) からフィルタリングされました。

--- 処理終了 ---
```

- この例では、ベース Retriever が 4 件取得したのに対し、Compression Retriever (EmbeddingsFilter 使用) は類似度が閾値 0.76 に満たなかった可能性のある 1 件 (例: RAG に関する文書) を除外し、3 件の結果を返しています。このように、関連性の低い文書が除去され、Context の質が向上しました！

---

## 4. 深掘り解説：圧縮とフィルタリングの世界

### 🎯 目標

**ContextualCompressionRetriever** と **Document Compressor** の連携、特に **`EmbeddingsFilter`** と **`LLMChainExtractor`** の違い、そしてこれらを使う際の考慮点について理解を深めます。

### Retriever と Compressor の連携プレイ

`ContextualCompressionRetriever` は、まさに司令塔です。

1.  まず、部下である `base_retriever` に「とりあえず関連しそうな情報を k 件持ってきて！」と指示します。
2.  次に、もう一人の部下である `base_compressor` に「この情報リストと元の質問を見て、本当に必要なものだけに編集して！」と指示します。
3.  Compressor が編集した最終結果を、司令塔が受け取って返します。

この「検索」と「編集」の分業体制が、ContextualCompressionRetriever の特徴です。

### Compressor の選択：速度 vs 品質

どの Compressor を使うかは、重要な選択です。

- **`EmbeddingsFilter` (今回使ったもの):**
  - **動作:** 取得した文書と質問の Embedding ベクトルの類似度（距離）を計算し、閾値と比較して**文書全体を**残すか捨てるか決めます。
  - **長所:** 高速。Embedding 計算だけで済むため API コストが（通常）低い。シンプルで導入しやすい。
  - **短所:** 文書の一部分だけを抽出することはできない。閾値の設定が結果を大きく左右し、調整が必要。
  - **使い所:** 速度やコストが重要で、明らかに無関係な文書を除外したい場合。
- **`LLMChainExtractor`:**
  - **動作:** 取得した文書それぞれについて、LLM に「この文書から、質問 X に関連する部分だけ抜き出して」と依頼し、**関連部分のテキストのみを抽出**します。
  - **長所:** 文脈を理解した上で、ピンポイントで関連部分だけを抽出できるため、非常に質の高い、簡潔な Context を得られる可能性がある。
  - **短所:** **取得した文書の数だけ LLM API コールが発生**するため、非常に**低速**で**高コスト**になる。LLM がうまく抽出できない場合もある。
  - **使い所:** コストや速度よりも Context の質と簡潔さを最優先したい場合。文書の中から特定の情報を正確に抜き出したい場合。

```python
# LLMChainExtractor を使う場合のイメージ (参考)
# from langchain.retrievers.document_compressors import LLMChainExtractor
# from langchain_openai import ChatOpenAI # LLM が必要

# llm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo") # 圧縮用LLM
# compressor = LLMChainExtractor.from_llm(llm)
# compression_retriever_llm = ContextualCompressionRetriever(
#     base_compressor=compressor,
#     base_retriever=base_retriever
# )
# # これを使うと、検索結果の各文書に対してLLMが抽出処理を行う
# compressed_results_llm = compression_retriever_llm.invoke(query)
```

### パイプラインで組み合わせる (`DocumentCompressorPipeline`)

さらに高度な使い方として、複数の Compressor を順番に適用するパイプラインも作成できます。例えば、

1.  まずテキストを小さなチャンクに分割 (`TextSplitter`)。
2.  次に `EmbeddingsFilter` で明らかに無関係なチャンクを除外。
3.  最後に残ったチャンクに対して `LLMChainExtractor` で関連部分を抽出。

といった多段構成も可能です。これは `DocumentCompressorPipeline` を使って実装できます（応用）。

### トレードオフを意識する

結局のところ、どの方法を使うかはトレードオフです。

- **Context の質・簡潔さ** を追求すれば **コスト・速度** が犠牲になりやすく (LLMChainExtractor)、
- **コスト・速度** を重視すれば **質・簡潔さ** がある程度犠牲になる可能性があります (EmbeddingsFilter)。

アプリケーションの目的や制約に合わせて、最適な Compressor や設定を見つけることが重要です。

---

## 5. 最終チェック：Context はうまく編集された？

### 🎯 目標

作成した `ContextualCompressionRetriever` が期待通りに動作し、ベース Retriever の結果と比較して Context が変化（通常は件数が減少）していることを確認します。

### 確認してみましょう！

- **実行！:** `step19_rag_compression.py` を実行してください。（必要なライブラリと API キーの設定を確認）
- **エラーなし？:** エラーメッセージが表示されずに最後まで実行できましたか？
- **Compressor 作成:** `EmbeddingsFilter` が指定した閾値で作成されていることをログで確認できましたか？
- **結果の比較:**
  - 「ベース Retriever (k=4) での検索結果」と「ContextualCompressionRetriever (EmbeddingsFilter) での検索結果」の**文書数**を比較してください。Compression Retriever の結果の方が少なくなっていますか？（閾値によっては、同じになることもありえます）
  - Compression Retriever の結果から除外された文書は、質問との関連性が低いと思われるものでしたか？（今回の例では「カレーライスの作り方」が除外されているはずです）
- **(実験) 閾値の変更:** `EmbeddingsFilter` の `similarity_threshold` の値を変えてみてください。値を高く（例: 0.8）すると、より多くの文書が除外される（結果が少なくなる）はずです。逆に低く（例: 0.7）すると、除外される文書が減る（結果が多くなる）はずです。この閾値がフィルタリングの厳しさをコントロールしていることを体感しましょう。

これらの点が確認できれば、Contextual Compression Retriever の基本的な使い方はマスターです！

---

## 6. まとめ：学びの整理と次へのステップ

### ✅ 達成したこと！

これで、RAG システムの検索結果をさらに磨き上げ、Context の質を高めるためのテクニックを習得しました！

- 最初の検索で得られた文書に不要な情報が含まれるという課題と、それを解決する **ContextualCompressionRetriever** の役割を理解しました。
- 取得した文書を編集する **Document Compressor** の概念を学び、代表的な **`EmbeddingsFilter`** と **`LLMChainExtractor`** の仕組みと特徴（速度、コスト、品質のトレードオフ）を知りました。
- 実際に **`EmbeddingsFilter`** を使って `ContextualCompressionRetriever` を作成し、ベース Retriever の結果をフィルタリングする方法を実装しました。

### 🔑 学んだキーワード

- **ContextualCompressionRetriever** (`langchain.retrievers` より)
- **Document Compressor**
- **`EmbeddingsFilter`** (`langchain.retrievers.document_compressors` より)
- **`LLMChainExtractor`** (同上、参考)
- **`DocumentCompressorPipeline`** (同上、応用)
- **ベース Retriever (Base Retriever)**
- **ベース Compressor (Base Compressor)**
- **Post-Retrieval Processing** (検索後処理)
- **関連性フィルタリング (Relevance Filtering)**
- **情報圧縮 (Document Compression)**
- **類似度閾値 (Similarity Threshold)**

### 🚀 次のステップへ！

MultiQueryRetriever で検索の「網羅性」を、ContextualCompressionRetriever で結果の「質・簡潔さ」を高める方法を学びました。RAG システムの検索部分はかなり強力になってきましたね！

しかし、検索の工夫はまだあります。特に、文書を小さなチャンクに分割して検索する場合、「検索ではヒットしたけど、そのチャンクだけだと文脈が足りない…前後の情報も欲しい！」というケースがあります。

次の **ステップ 20「検索改善(3): ParentDocumentRetriever」** では、まさにこの課題に対応する **`ParentDocumentRetriever`** を学びます。これは、小さなチャンクで効率よく検索しつつ、関連が見つかったらそのチャンクが含まれる**元の大きな文書（親文書）全体**、あるいは**より大きなチャンク**を取得してくれる、賢い Retriever です。検索精度と文脈理解の両立を目指す、さらに一歩進んだテクニックを探求しましょう！
