前のステップでは、テキストファイルや PDF ファイルから情報を読み込み、`Document` オブジェクトとして扱う方法を学びました。これで AI に外部知識を与える準備はできましたが、一つ大きな課題があります。多くの AI モデル (LLM) には、一度に処理できるテキストの量、いわゆる **コンテキストウィンドウ** に**トークン数**という単位での上限があるのです。非常に長い文書をそのまま AI に渡そうとすると、エラーになったり、情報が途中で切り捨てられたりしてしまいます。

そこでこのステップでは、長いテキストや `Document` を、AI が処理しやすい**適切なサイズのかたまり (チャンク)** に分割するための技術、**`Text Splitters`** (テキストスプリッター) を学びます。AI の処理単位である **トークン数** を意識し、かつ日本語の文章構造も考慮しながら賢く分割するのがポイントです。

## 1. はじめに：このステップで目指すこと

### 🎯 今回のゴール

このステップを終えると、以下のことができるようになります。

- なぜ長いテキストを分割する必要があるのか、その理由 (コンテキストウィンドウ制限など) を **トークン数** の観点から理解します。
- LangChain の **`Text Splitters`** を使って、長いテキスト文字列や `Document` オブジェクトを、日本語の構造とトークン数を考慮しながら意味的にまとまりのあるチャンクに分割する方法を習得します。
- **具体的な成果物:** **`RecursiveCharacterTextSplitter`** を使い、**トークン数**を基準にしたチャンクサイズとオーバーラップを指定し、日本語テキストを適切に分割して、その結果を確認する Python コードを作成します。

### 🔑 このステップのポイント

このステップで特に重要な考え方や技術です。

- **`Text Splitters`**: 長いテキストを小さなチャンクに分割するための LangChain のツール群。
- **`RecursiveCharacterTextSplitter`**: 一般的によく使われる Text Splitter。日本語にも対応可能。
- **トークン (Token)**: AI がテキストを処理する際の基本単位。**チャンクサイズ**はこのトークン数を基準に設定することが推奨されます。
- **`chunk_size`**: チャンク 1 つあたりの目標とする最大 **トークン数**。
- **`chunk_overlap`**: 隣接するチャンク間で内容を重複させる部分の **トークン数**。
- **`language` パラメータ**: Splitter に言語を指定し、分割方法を最適化するための設定 (例: `language="ja"`)。

### ✅ 前提知識

このステップをスムーズに進めるために、以下の知識があると役立ちます。

- Python の基本的な文法（文字列操作、リスト操作など）。
- ステップ 12「外部ファイル活用！ドキュメント読込」で学んだ `Document` オブジェクトの概念。
- AI モデルのコンテキストウィンドウと **トークン** という概念についての基本的な理解。

---

## 2. 準備運動：ハンズオンのための基礎知識

### 🎯 目標

なぜテキスト分割が必要で、LangChain の **`Text Splitters`** がどのようにその問題を解決するのか、特に **`RecursiveCharacterTextSplitter`** の基本的な仕組みと、**トークン数ベース**での分割の重要性を理解しましょう。

### なぜテキスト分割が必要？ - トークン数の壁

主な理由は AI モデル (LLM) の **コンテキストウィンドウ制限** です。これは、モデルが一度に処理できる情報の量の上限を **トークン数** で表したものです。日本語の場合、1 文字が 1 トークン以上になることが多く、文字数だけでは正確なサイズを把握できません。例えば、上限が 4096 トークンのモデルに 5000 トークン分のテキストを渡すと、処理できないか、一部の情報が欠落してしまいます。

そのため、長いテキストは、モデルの制限を超えない **トークン数** を目安に、小さなチャンクに分割する必要があるのです。また、分割により、後の検索処理で質問に関連する部分だけを効率的に見つけやすくなるという利点もあります。

### `Text Splitters` とは？ - 賢い分割屋さん

単純に N トークンごと、のように区切ると、文や意味の途中で切れてしまう可能性があります。LangChain の **`Text Splitters`** は、テキストの構造（改行、句読点など）を考慮し、できるだけ意味のまとまりを保ちながら分割することを目指します。

今回は、汎用性が高く日本語にも対応しやすい **`RecursiveCharacterTextSplitter`** を使います。

### `RecursiveCharacterTextSplitter` の仕組みと日本語対応

この Splitter は、指定された区切り文字のリストを使って、テキストを再帰的に分割しようと試みます。重要なのは **`language`** パラメータです。

- **`language="ja"` の指定**: このように指定すると、Splitter は**日本語に適した区切り文字** (例: `\n\n`, `\n`, `。`, `、`, ` ` など、内部的に定義されたもの) を優先的に使って分割しようとします。これにより、単なるスペースや改行だけでなく、句読点なども考慮した、より自然な日本語のチャンク分割が期待できます。
- **分割プロセス**: まず優先度の高い区切り文字（例: 空行）で分割し、チャンクサイズ（トークン数）を超えていれば、次に優先度の高い区切り文字（例: 改行や句点）でさらに分割…という処理を繰り返します。

### 主要なパラメータ (トークン数ベース)

`RecursiveCharacterTextSplitter` を使う際、主に以下のパラメータを **トークン数** を意識して設定します。

- **`chunk_size`**:
  - 1 つのチャンクの目標とする最大 **トークン数** を指定します。
  - どのくらいの値にするかは、後続の Embedding モデルや LLM のトークン制限、検索したい情報の粒度などを考慮して決めます。数百〜1000 トークン程度が一般的な出発点です。
- **`chunk_overlap`**:
  - 隣り合うチャンク間で、内容を重複させる部分の **トークン数** を指定します。
  - 文脈の維持に役立ちます。`chunk_size` の 10% 程度 (例: `chunk_size=500` トークンなら `overlap=50` トークン) が目安の一つですが、調整が必要です。
- **`length_function`**:
  - チャンクのサイズを計算するための関数を指定します。**トークン数ベース**で分割するため、**`tiktoken`** ライブラリを使ってトークン数を数える関数をここに指定します。

---

## 3. 実践タイム：テキストをトークン数で分割してみよう！

### 🎯 目標

実際に Python コードを書き、`RecursiveCharacterTextSplitter` を使って、日本語の長文テキストを**トークン数**を基準にチャンク分割し、その結果を確認します。

### ステップ・バイ・ステップ実装

#### 1. 必要なモジュールのインポートと準備:

`RecursiveCharacterTextSplitter` と、トークン数を計算するための `tiktoken` ライブラリをインポートします。分割対象の日本語テキストも用意します。

```python
# step13_text_splitters_revised.py
import os
import sys
from dotenv import load_dotenv
import tiktoken # トークン数を計算するために必要

# RecursiveCharacterTextSplitter をインポート
# 最新の推奨パッケージからインポート
try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    print("RecursiveCharacterTextSplitter をインポートしました (from langchain_text_splitters)。")
    # 必要ならインストール: pip install langchain-text-splitters
except ImportError:
    print("エラー: langchain-text-splitters が見つかりません。")
    print("   'pip install langchain-text-splitters' を実行してください。")
    sys.exit(1)

# Document オブジェクトを扱うためにインポート (後で使用)
from langchain_core.documents import Document

print("--- 必要なモジュールのインポート完了 ---")
load_dotenv() # .envファイルがあれば読み込む

# 分割対象のサンプルテキスト (日本語・長文)
long_text_jp = """LangChain（ランクチェイン）は、大規模言語モデル（LLM）を活用したアプリケーション開発のための強力なフレームワークです。開発者は LangChain を使うことで、複雑な AI アプリケーションの構築プロセスを大幅に簡略化できます。

主な特徴として、様々なコンポーネント（プロンプトテンプレート、LLMラッパー、エージェント、メモリなど）をモジュール式に組み合わせられる点が挙げられます。これにより、特定のユースケースに合わせた柔軟なカスタマイズが可能になります。例えば、社内ドキュメントに基づいた質問応答システムの構築、複数のツールを自律的に使いこなすエージェントの作成、過去の対話履歴を記憶するチャットボットの開発などが実現できます。

LangChain は Python と TypeScript で提供されており、活発なオープンソースコミュニティによって支えられています。ドキュメントも充実しており、多くのサンプルコードが公開されているため、比較的容易に学習を始めることができます。しかし、提供される機能が多岐にわたるため、全体像を把握するにはある程度の学習時間が必要です。

特に重要なコンセプトの一つが LCEL (LangChain Expression Language) です。これは、パイプ演算子を使ってコンポーネントを直感的に連結し、処理フローを宣言的に記述するための言語（または記述方法）です。LCEL をマスターすることで、コードの可読性が向上し、より洗練されたアプリケーション開発が可能になります。

今後の AI アプリケーション開発において、LangChain のようなフレームワークの重要性はますます高まっていくと考えられます。最新の動向を追いかけ、基本的な使い方を習得しておくことは、AI エンジニアにとって有益でしょう。"""

print(f"--- サンプルテキスト準備完了 (文字数: {len(long_text_jp)}) ---")

# --- トークン数計算の準備 ---
# OpenAI モデルでよく使われる 'cl100k_base' エンコーディングを使用
# (注意: 使用する LLM や Embedding モデルに合わせて適切なエンコーダーを選ぶ必要があります)
try:
    tokenizer = tiktoken.get_encoding("cl100k_base")
    print(f"--- tiktoken トークナイザ準備完了 (encoding: cl100k_base) ---")
    # 必要ならインストール: pip install tiktoken
except Exception as e:
    print(f"エラー: tiktoken の準備に失敗しました: {e}")
    print("   'pip install tiktoken' を実行してください。")
    sys.exit(1)

# テキストのトークン数を計算する関数を定義
def tiktoken_len(text):
    tokens = tokenizer.encode(
        text,
        disallowed_special=() # 特殊トークンを許可しない (一般的なテキストカウント用)
    )
    return len(tokens)

# サンプルテキスト全体のトークン数も確認しておく (目安)
total_tokens = tiktoken_len(long_text_jp)
print(f"サンプルテキスト全体のトークン数 (目安): {total_tokens}")
```

- **`tiktoken`**: OpenAI が提供する、同社のモデルで使われるトークン化を高速に行うライブラリです (`pip install tiktoken` が必要)。`cl100k_base` は多くの最新 OpenAI モデルで使われるエンコーディング名です。
- `tiktoken_len` 関数: これを Splitter の `length_function` に渡すことで、文字数ではなくトークン数でチャンクサイズを計算します。

#### 2. `RecursiveCharacterTextSplitter` の初期化 (トークン数基準):

**`language="ja"`**, **`length_function=tiktoken_len`** を指定し、`chunk_size` と `chunk_overlap` を **トークン数** で設定します。

```python
# step13_text_splitters_revised.py (続き)

# Text Splitter を初期化 (トークン数基準、日本語対応)
text_splitter_token = RecursiveCharacterTextSplitter(
    chunk_size=200,         # 目標とする最大トークン数
    chunk_overlap=20,         # チャンク間の重複トークン数
    length_function=tiktoken_len, # トークン数を計算する関数を指定
    is_separator_regex=False,
    separators=["\n\n", "\n", "。", "、", " ", ""], # 日本語用の区切り文字リスト（デフォルトから少し調整も可能）
    # language="ja" # language 引数は langchain_text_splitters 0.2.0 以降で利用可能
                   # 利用可能な場合は、separators を自動設定してくれるため便利
)
print("\n--- RecursiveCharacterTextSplitter 初期化完了 (トークン数基準) ---")
print(f"  chunk_size (トークン数): {text_splitter_token._chunk_size}")
print(f"  chunk_overlap (トークン数): {text_splitter_token._chunk_overlap}")
# print(f"  language: {text_splitter_token._language}") # language 引数がある場合
```

- **注意:** `language` パラメータは比較的新しい機能です。利用している `langchain-text-splitters` のバージョンが古い場合、このパラメータが存在しない可能性があります。その場合は、`separators` 引数で日本語に適した区切り文字 (例: `["\n\n", "\n", "。", "、", " ", ""]`) を明示的に指定する必要があります。

#### 3. テキスト文字列の分割 (`.split_text()` - トークン数):

用意した日本語テキストをトークン数基準で分割します。

```python
# step13_text_splitters_revised.py (続き)
print("\n--- テキスト文字列の分割 (.split_text - トークン数基準) ---")

text_chunks_token = text_splitter_token.split_text(long_text_jp)
print(f"分割後のチャンク数: {len(text_chunks_token)}")

print("\n--- 分割されたチャンク (一部) ---")
for i, chunk in enumerate(text_chunks_token):
    chunk_token_count = tiktoken_len(chunk)
    print(f"--- チャンク {i+1} (トークン数: {chunk_token_count}, 文字数: {len(chunk)}) ---")
    if i < 3:
        print(chunk)
        # 重複部分の確認 (次のチャンクが存在する場合)
        if i + 1 < len(text_chunks_token):
            next_chunk = text_chunks_token[i+1]
            # 簡単な重複確認ロジック (厳密ではない)
            overlap_candidate = chunk[-text_splitter_token._chunk_overlap * 2 :] # 重複候補を少し広めに取る
            if next_chunk.startswith(overlap_candidate[-len(next_chunk):]): # 簡易的な前方一致
                 print("  (次のチャンクとの重複の可能性あり)")
    elif i == 3:
        print("...") # 4番目以降は省略
        break
```

- 各チャンクの **トークン数** が `chunk_size` (200) に近くなっているか確認しましょう。文字数とは異なることに注意してください。
- チャンク間の重複も確認しますが、トークン数での重複は文字数での重複と完全に一致しないため、目視での確認は少し難しくなります。

#### 4. `Document` オブジェクトの分割 (`.split_documents()` - トークン数):

`Document` オブジェクトも同様にトークン数基準で分割します。

```python
# step13_text_splitters_revised.py (続き)
print("\n--- Document オブジェクトの分割 (.split_documents - トークン数基準) ---")

original_document_jp = Document(
    page_content=long_text_jp,
    metadata={"source": "jp_document.txt", "language": "ja"}
)
print(f"元の Document: (トークン数: {tiktoken_len(original_document_jp.page_content)}, metadata: {original_document_jp.metadata})")

# 同じ splitter を使って Document を分割
document_chunks_token = text_splitter_token.split_documents([original_document_jp])
print(f"分割後の Document 数: {len(document_chunks_token)}")

print("\n--- 分割された Document (最初のチャンク) ---")
if document_chunks_token:
    first_doc_chunk_token = document_chunks_token[0]
    chunk_token_count = tiktoken_len(first_doc_chunk_token.page_content)
    print(f"Type: {type(first_doc_chunk_token)}")
    print(f"Page Content (トークン数: {chunk_token_count}):")
    print(first_doc_chunk_token.page_content)
    print(f"Metadata: {first_doc_chunk_token.metadata}") # メタデータが引き継がれる
else:
    print("Document チャンクリストが空です。")

print("\n--- 処理終了 ---")

```

- `.split_documents()` も `length_function` で指定されたトークン数計算関数を使って分割を行います。メタデータが引き継がれる点は文字数ベースの場合と同じです。

### 完成コード (`step13_text_splitters_revised.py`)

上記の実装 1〜4 を結合したものが完成コードとなります。

---

## 4. 深掘り解説：分割をコントロールする

### 🎯 目標

`RecursiveCharacterTextSplitter` の分割の仕組み、`chunk_size` や `chunk_overlap` の設定の考え方、そして他の Text Splitter について理解を深めます。

### `RecursiveCharacterTextSplitter` の分割ロジックと日本語

`language="ja"` を指定した場合 (または日本語に適した `separators` を指定した場合)、この Splitter は日本語の文章構造を考慮した分割を試みます。優先順位は内部的に定義されていますが、一般的には以下のような順序で区切り文字を探します。

1.  空行 (`\n\n`)
2.  改行 (`\n`)
3.  句点 (`。`)
4.  読点 (`、`)
5.  スペース (` `)
6.  文字 (`""`)

これにより、段落、文、句といった単位をできるだけ保ちながら、指定された **トークン数** (`chunk_size`) に収まるように分割が行われます。

### `chunk_size` の設定：トークン数で考える

LLM や Embedding モデルの性能を最大限に引き出すためには、**トークン数** を基準に `chunk_size` を設定することが非常に重要です。

- **基準**: 利用する **Embedding モデル** が扱える最大トークン数、そして **LLM** のコンテキストウィンドウ上限を考慮します。Embedding モデルの上限を超えるチャンクはベクトル化できず、LLM の上限を超えるプロンプト (チャンクを含む) はエラーになります。
- **モデルの制限を確認**: 例えば OpenAI の `text-embedding-3-small` は 8191 トークンまで、GPT-4 Turbo は 128k トークンまでなど、モデルによって上限が異なります。公式ドキュメントで確認しましょう。
- **推奨値**: 一概には言えませんが、Embedding の効率や検索精度を考慮すると、**数百トークン (例: 200〜512 トークン)** 程度が、多くの Embedding モデルにとって扱いやすいサイズとされることが多いです。LLM に渡す際に複数のチャンクを組み合わせることを考えると、個々のチャンクは LLM の上限よりかなり小さく設定するのが一般的です。
- **`tiktoken` の活用**: `tiktoken` ライブラリを使えば、OpenAI モデルが実際にどのようにテキストをトークン化するかをシミュレートし、正確なトークン数を計算できます。これにより、より厳密な `chunk_size` 管理が可能です。

### `chunk_overlap` の効果と調整 (トークン数)

トークン数で `chunk_overlap` を設定する場合も考え方は同じです。文脈の維持に役立ちますが、冗長性が増えコストが増加する可能性があります。

- **目安**: `chunk_size` (トークン数) の 10% 程度、例えば `chunk_size=500` トークンなら `chunk_overlap=50` トークンあたりから試してみるのが良いでしょう。

### 他の Text Splitter

- **`CharacterTextSplitter`**: シンプルに指定した 1 文字で分割します。
- **`TokenTextSplitter`**: `tiktoken` などを使って、厳密にトークン数で分割します。`RecursiveCharacterTextSplitter` よりも厳密なサイズ制御が必要な場合に検討します。
- **構造化文書向け**:
  - **`MarkdownHeaderTextSplitter`**: Markdown のヘッダー (`#`, `##`) で分割します。ドキュメントの章や節ごとに分割したい場合に便利です。
  - HTML 用、Python コード用など、特定の構造に特化した Splitter も存在します。

文書の種類や目的に合わせて適切な Splitter を選ぶことが重要ですが、`RecursiveCharacterTextSplitter` はその汎用性の高さから多くの場面で有効です。

---

## 5. 最終チェック：うまく分割できたかな？

### 🎯 目標

作成したコードが正しく動作し、テキストが指定した**トークン数**パラメータに従ってチャンクに分割されていることを確認します。

### 確認してみよう！

- **実行**: `step13_text_splitters_revised.py` を実行してください。（事前に `pip install langchain-text-splitters tiktoken` が必要です）
- **エラー**: エラーメッセージが表示されずに最後まで実行できましたか？
- **`.split_text()` の結果**:
  - 出力された各チャンクの **トークン数** は、指定した `chunk_size` (200) を大きく超えていませんか？（区切り文字の位置により多少前後します）
  - チャンク間に `chunk_overlap` (20 トークン) 程度の重複が見られますか？（目視での正確な確認は難しいですが、チャンクの境界付近を見てみましょう）
- **`.split_documents()` の結果**:
  - 元の `Document` が、複数の `Document` チャンクに分割されていますか？
  - 分割後の各 `Document` チャンクの `metadata` に、元のメタデータが正しく引き継がれていますか？

これらの点が確認できれば、トークン数を意識したテキスト分割の基本はマスターです！

---

## 6. まとめ：学びの整理と次へのステップ

### ✅ 達成したこと！

これで、AI が扱いやすいように長いテキストを分割する技術を、より実践的な形で身につけました！

- AI のコンテキストウィンドウ制限のために、**トークン数** を基準としたテキスト分割が必要であることを理解しました。
- LangChain の **`Text Splitters`**、特に **`RecursiveCharacterTextSplitter`** を使い、**`tiktoken`** と連携してトークン数ベースでチャンク分割する方法を学びました。
- 日本語テキストを扱う際に **`language="ja"`** (または適切な `separators`) を指定する重要性を理解しました。
- **`chunk_size`** と **`chunk_overlap`** をトークン数で設定する方法を習得しました。
- 分割されたチャンクが元のメタデータを保持することを確認しました (`.split_documents()`)。

### 🔑 学んだキーワード

- **`Text Splitters`**
- **`RecursiveCharacterTextSplitter`** (`langchain_text_splitters` より)
- **チャンク (Chunk)**
- **トークン (Token)**
- **`tiktoken`** (トークン数計算ライブラリ)
- **`chunk_size`** (トークン数)
- **`chunk_overlap`** (トークン数)
- **`length_function`**
- **`language="ja"`** / **`separators`**
- **.split_text()**
- **.split_documents()**
- コンテキストウィンドウ (Context Window)

### 🚀 次のステップへ！

テキストを適切なサイズのチャンクに分割できるようになりました。これで、各チャンクを AI が個別に処理する準備が整いました。

しかし、これらのチャンクはまだ単なるテキストです。コンピュータがチャンク同士の意味の近さを理解し、関連性の高いチャンクを検索できるようにするためには、次のステップが必要です。

次の **ステップ 14「テキストをベクトルに！Embedding 入門」** では、分割された各チャンクを、その意味を捉えた**数値ベクトル**に変換する **`Embedding`** 技術を学びます。これにより、テキストデータが検索や分析に適した形式に変換され、RAG (Retrieval-Augmented Generation) システム構築への道がさらに進みます！ (OpenAI 以外の Embedding モデルを使う場合も、この分割処理は同様に重要です。)
