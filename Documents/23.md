これまでのステップで、私たちは RAG チェーンを組み立て、検索を改善し、プロンプトも最適化してきました。かなり高性能な Q&A システムが作れるようになってきましたね！

でも、ちょっと待ってください。「動いた！」だけで満足していては、本当に良いアプリケーションとは言えません。「この AI の答え、本当に信頼できるのかな？」「思ったのと違う答えが返ってきたけど、どこがおかしいんだろう？」… こういった疑問に答えられるようになることが、プロへの道です！

そこで今回は、「作った RAG システムって、本当にうまく機能してる？」を客観的にチェックする**評価**と、「あれ？ なんだか動きがおかしいぞ？」という時に原因を探る**デバッグ**の、基本的な考え方と最初のステップを学びます。作ったアプリをきちんと評価し、問題を解決する能力は、これからもっと複雑な AI アプリを作る上で絶対に役立ちますよ！

## 1. はじめに：このステップで目指すこと

### 🎯 今回のゴール

このステップを最後まで体験すると、あなたはこんなことができるようになります！

- 作成した RAG アプリケーションの性能を**評価**する必要性と、そのための基本的な観点（考え方）を理解します。
- RAG システム特有の評価指標（例: **Context Relevance**, **Answer Faithfulness**, **Answer Relevance**）の概念を知ります。
- RAG アプリケーションが期待通りに動作しない場合に、問題の原因を突き止めるための基本的な**デバッグ**手法（**トレース**、**個別テスト**など）の考え方を学びます。
- LangChain のデバッグモードや、**LangSmith** という開発支援ツールの存在を知ります。
- **具体的な成果物:** RAG チェーンの実行時にデバッグログを出力させる方法を試し、評価の観点（Context は適切か？回答は Context に忠実か？）を持って結果を確認します。（今回は評価やデバッグの「考え方」と「初歩的なツール」に焦点を当てます。）

### 🔑 このステップのポイント

今回の「アプリの健康診断」で重要なキーワードはこちら！

- **評価 (Evaluation):** アプリケーションの性能や品質を客観的に測定すること。
- **デバッグ (Debugging):** プログラムのエラーや意図しない動作の原因を見つけて修正すること。
- **RAG 評価指標:**
  - **Context Relevance (文脈の関連性):** Retriever が質問に合った Context を取ってきたか？
  - **Answer Faithfulness (回答の忠実度):** 回答がちゃんと Context に基づいているか？ (ハルシネーションしてないか？)
  - **Answer Relevance (回答の関連性):** 回答が元の質問にちゃんと答えているか？
- **トレース (Tracing):** チェーンの処理の「足跡」を追いかけること。
- **コンポーネント分離テスト:** チェーンの部品を個別にテストすること。
- **LangSmith:** LangChain アプリの開発、デバッグ、評価、監視を助けるプラットフォーム（今回は紹介のみ）。

### ✅ 前提知識

この評価とデバッグの世界に足を踏み入れる前に、これらは大丈夫？

- **ステップ 22 までの内容:** 動作する RAG チェーン（回答と出典を返すものが望ましい）が手元にあること。
- **LCEL チェーンの構造理解:** プロンプト、LLM、Retriever、パーサーがどのように連携しているか分かっていること。
- **基本的な Python の実行と確認:** コードを実行し、`print` 文などで出力を確認できること。

準備ができたら、アプリの内部を覗いて、その健康状態をチェックする方法を学びましょう！

---

## 2. 準備運動：なぜ評価とデバッグが必要？

### 🎯 目標

「動いたから OK！」ではなく、なぜ「評価」して「デバッグ」する必要があるのか、その重要性と、特に RAG システムで注目すべきポイントを理解しましょう。

### 作っただけじゃ終われない！ アプリの「健康診断」

自転車を組み立てたら、ちゃんとブレーキが効くか、タイヤに空気が入っているか、試運転して確認しますよね？ AI アプリケーションも同じです。コードがエラーなく動くだけでなく、「期待通りの性能が出ているか？」「おかしな動きをしていないか？」をチェックする必要があります。

- **評価:** アプリがどれくらい「良い仕事」をしているかを測る物差しです。これがないと、改善したつもりが実は改悪だった…なんてことも。客観的な評価は、開発を進める上での道しるべになります。
- **デバッグ:** アプリが思ったように動かない（エラーが出る、変な答えを返すなど）ときに、その原因を探し出して修正する作業です。これができないと、問題を解決できず、信頼性の低いアプリになってしまいます。

### RAG システムの「ここが心配！」ポイント

RAG は強力ですが、いくつかの落とし穴もあります。評価やデバッグでは、特に以下の点に注目する必要があります。

1.  **検索 (Retrieve) はうまくいった？:** Retriever は、本当に質問と関係のある Context を見つけてきてくれたでしょうか？ 関係ない情報ばかりだと、良い答えは期待できません。（**Context Relevance**）
2.  **AI は Context をちゃんと使った？:** LLM は、提供された Context に基づいて答えてくれたでしょうか？ それとも無視して、自分の知識や想像で答えてしまった（ハルシネーション）のでしょうか？（**Answer Faithfulness**）
3.  **そもそも質問に答えてる？:** 生成された回答は、元の質問に対する的確な答えになっていますか？ Context に忠実でも、質問の意図とズレていては意味がありません。（**Answer Relevance**）

これらの観点を持って、アプリの出力結果を見ていくことが、評価とデバッグの第一歩になります。

### デバッグの基本：問題を切り分ける

「なんだか動きがおかしい…」と思ったとき、闇雲にコードを眺めても原因は見つけにくいものです。デバッグの基本は**問題を切り分ける**こと。

- **Retriever はちゃんと動いてる？:** まずは Retriever 単体で、質問に対して適切な Context を返しているか確認する。
- **LLM はちゃんと答えてる？:** 次に、意図的に「正しい Context」と質問をプロンプトにセットして LLM に渡し、期待通りの回答が生成されるか確認する。
- **チェーン全体は？:** 個々の部品が正しくても、繋ぎ合わせたチェーン全体でデータがうまく流れていない可能性もあります。

このように、怪しい箇所を一つずつ潰していくのがデバッグの定石です。

---

## 3. 実践タイム：評価の観点とデバッグの初歩を体験！

### 🎯 目標

このステップでは、新しい複雑なコードを書くというよりは、これまでの RAG チェーンを**評価の観点**で見直し、**基本的なデバッグツール**を使って内部の動きを観察する方法を体験します。

### ステップ・バイ・ステップ実践

#### 1. 準備 (ステップ 21/22 のコード):

ステップ 21 または 22 で作成した、回答と出典 (Context) を返す RAG チェーンのコード (`step21_rag_citation_final.py` や `step22_rag_prompt_optimization_final.py`) を準備します。

#### 2. 評価の観点で結果を見てみる:

準備したコードを実行し、いくつかの質問を試してみましょう。そして、出力された「AI の回答」と「出典情報 (Context)」を見ながら、以下の点を自問自答してみてください。

- **Context Relevance:** 質問に対して、表示されている Context は本当に関連性が高いと言えるか？ もっと適切な情報はなかったか？
- **Answer Faithfulness:** AI の回答は、表示されている Context の内容だけに基づいているか？ Context に書かれていないことを言っていないか？
- **Answer Relevance:** AI の回答は、そもそも元の質問に対する答えとして的確か？ 質問の意図を汲み取れているか？

```python
# 例：step22 のコードを実行し、結果を評価の観点で見る

question_1 = "LCEL は何の略称ですか？"
# ... チェーンを実行して output_1 を取得 ...
print("[AIの回答 1]", output_1.get("answer"))
print("[出典情報 1]")
# print_sources(output_1.get("context")) # <- Context を表示

# 【評価の問い】
# - この Context (doc-b, page: 2) は質問に適切か？ (Relevance: Yes)
# - 回答は Context ("LCEL は LangChain Expression Language の略...") に基づいているか？ (Faithfulness: Yes)
# - 回答は質問「何の略称か？」に答えているか？ (Relevance: Yes)
# → このケースは良さそう！

question_2 = "LangChain の創設者は誰ですか？" # サンプル文書にはない情報
# ... チェーンを実行して output_2 を取得 ...
print("[AIの回答 2]", output_2.get("answer"))
print("[出典情報 2]")
# print_sources(output_2.get("context")) # <- 何か関連性の低いものが表示されるかも

# 【評価の問い】
# - この Context は質問（創設者）に関連しているか？ (Relevance: No)
# - 回答は「提供された情報には...見つかりませんでした。」か？ (Faithfulness: Yes - プロンプトに従っている)
# - 回答は質問に（間接的に）答えているか？ (Relevance: Yes - 答えられないことを伝えている)
# → プロンプトは機能しているが、そもそも Context が不適切。Retriever の課題かも？
```

このように、いくつかの質問で試してみて、「この RAG システム、どんな時にうまく動いて、どんな時にイマイチなんだろう？」と感じることが、評価の第一歩です。

#### 3. デバッグモードで動きを見る！:

LangChain には、チェーンが内部で何をやっているかを詳しく表示してくれる「デバッグモード」があります。これを有効にしてみましょう。

```python
# step22_rag_prompt_optimization_final.py の冒頭あたりに追加

# ... 他
# --- 必要な LangChain モジュール ---
# ... 他のインポート ...
from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableParallel
from langchain.globals import set_debug # ★ デバッグモード切り替え用

# ... Vector Store, Splitter, Tokenizer などの準備 ...

print("--- 必要なモジュールのインポート完了 ---")

# ★★★ デバッグモードを有効にする ★★★
set_debug(True)
print("--- ★ LangChain デバッグモード有効 ★ ---")
# これ以降に実行される LangChain のチェーンは詳細なログを出力します

# ... (LLM, Retriever, チェーンの準備は同じ) ...

# ... (チェーンの実行) ...
# question = "LCEL について教えて"
# output = new_rag_chain_with_source.invoke({"question": question})
# print(output) # 回答と Context

# ★★★ デバッグモードを無効にする (必要なら) ★★★
# set_debug(False)
# print("--- ★ LangChain デバッグモード無効 ★ ---")

# ... (以降の処理) ...

```

- スクリプトの初めの方で `from langchain.globals import set_debug` をインポートし、`set_debug(True)` を呼び出すだけです！
- これでチェーンを実行すると、コンソールに**非常にたくさんのログ**が表示されます。
  - `[chain/start]` や `[chain/end]` で各処理ブロックの開始と終了が分かります。
  - `[llm/start]` や `[llm/end]` で LLM の呼び出しと入力・出力が見えます。
  - `[retriever/start]` や `[retriever/end]` で Retriever が何を入力され、どの文書を返したかが見えます。
- ログは詳細ですが、どの部品 (Retriever, Prompt, LLM) にどんなデータが渡され、何が出力されたかを追跡することで、問題箇所を特定する手がかりになります。

#### 4. LangSmith の紹介 (今回は触れるだけ):

もっと本格的にデバッグや評価を行いたい場合、LangChain は **LangSmith** ([https://smith.langchain.com/](https://smith.langchain.com/)) という専用のプラットフォームを提供しています。

- **何ができる？:**
  - チェーンの実行履歴（トレース）を Web UI で視覚的に確認できる。
  - 各ステップの入出力、実行時間、消費トークン数などを詳細に分析できる。
  - 評価データセットを作成し、RAG システムの性能を定量的に測定・比較できる。
- **使い方:** 環境変数を設定するなど簡単なセットアップで、LangChain の実行ログが自動的に LangSmith に送られるようになります。
- **今回は:** LangSmith は非常に強力ですが、セットアップが必要なのでこのステップでは深入りしません。「こんな便利なツールがあるんだな」と知っておくだけで十分です！

---

## 4. 深掘り解説：評価とデバッグの考え方

### 🎯 目標

RAG システムの評価指標と、基本的なデバッグ戦略、そして LangSmith の役割について、もう少し詳しく理解しましょう。

### RAG 評価の主要な観点

RAG システムを評価する際には、主に以下の点を考慮します。これらは先ほど「評価の観点で見てみる」で試した問いに対応します。

1.  **Context Relevance (文脈の関連性):** Retriever が取得した Context は、ユーザーの質問に対してどれくらい関連性が高いか？
    - 評価方法例: 人間が Context を見て判断する、あるいは LLM に判断させる。
2.  **Answer Faithfulness / Groundedness (回答の忠実度 / 接地性):** 生成された回答は、提供された Context の情報にどれだけ忠実か？ Context にない情報を捏造（ハルシネーション）していないか？
    - 評価方法例: 回答と Context を比較して人間が判断する、あるいは LLM に判断させる。
3.  **Answer Relevance (回答の関連性):** 生成された回答は、元の質問の意図に対してどれだけ的確か？ ちゃんと質問に答えているか？
    - 評価方法例: 質問と回答を見て人間が判断する、あるいは LLM に判断させる。

これらの指標を評価するには、いくつかの質問と、それに対する「理想的な Context」や「理想的な回答」をまとめた**評価データセット**を作成し、それを使ってシステムをテストするのが一般的です。

### LLM-as-Judge (LLM による評価)

人間が一つ一つ評価するのは大変なので、「LLM に評価させる」というアプローチ (LLM-as-Judge) が注目されています。性能の高い LLM (GPT-4 など) に、上記の評価基準（例:「この回答は Context に忠実ですか？ Yes/No」）と、評価対象のデータ（質問、Context、回答）を与えて、評価スコアや理由を出力させる方法です。LangChain にもこれを行うための評価用チェーン (`QAEvalChain` など) が用意されています（応用）。

### デバッグ戦略の基本

RAG チェーンがうまく動かない場合、以下のステップで原因を探るのが基本です。

1.  **トレースで全体像把握:** まず `set_debug(True)` や LangSmith を使って、チェーン全体のデータの流れと各ステップの入出力を確認します。「どこでデータがおかしくなっているか？」の見当をつけます。
2.  **問題箇所の特定（分離）:** 怪しい部分が見つかったら、そのコンポーネント（Retriever、プロンプト＋ LLM など）だけを単体で動かしてみます。
    - Retriever が問題？ → `.invoke()` で色々な質問を試し、返ってくる Context を確認。Retriever の設定 (`k` の値、`search_type`) や Vector Store の中身を見直す。
    - プロンプト＋ LLM が問題？ → Retriever が返してきた Context (または手動で用意した Context) と質問を使って、プロンプトを直接 LLM に渡してみて、どんな回答が返ってくるか確認。プロンプトの指示を見直す。
    - チェーンの繋ぎ方 (`|` や `{}`) が問題？ → 各ステップ間のデータの型や構造が期待通りか、`print` 文などを挟んで確認する。
3.  **修正と再テスト:** 原因と思われる箇所を修正し、再度テストして問題が解消されたか確認します。

### LangSmith の強力さ

LangSmith は、これらの評価とデバッグのプロセスを劇的に効率化してくれます。

- **トレースの可視化:** 複雑なチェーンの実行も、Web UI 上でステップごとに何が起こったか一目瞭然です。
- **デバッグ支援:** 各ステップの入出力を簡単に確認でき、エラーの原因特定が容易になります。
- **評価の自動化:** 評価データセットをアップロードし、様々な指標（忠実度、関連性など）で RAG システムの性能を自動測定・比較できます。

本格的な LangChain アプリ開発には、LangSmith の活用が非常に有効です。

---

## 5. 最終チェック：評価とデバッグの第一歩

### 🎯 目標

このステップで学んだ評価の観点と基本的なデバッグ手法を意識できるようになったかを確認します。

### 確認してみましょう！

- **デバッグモード:** `set_debug(True)` を使って RAG チェーンを実行し、大量のログが出力されるのを確認できましたか？ ログの中から、Retriever が返した Context や LLM への入力プロンプト、LLM の生の応答などを目で追ってみましょう。
- **中間データの確認:** ステップ 21 のコードで `rag_output['context']` の中身を出力しましたね。このように、チェーンの途中や最終出力に含まれるデータ（特に Retriever が返した Context）を確認することが、評価やデバッグの第一歩になることを実感できましたか？
- **評価の観点:** 自分で RAG チェーンにいくつか質問をしてみて、「Context は適切だったか？」「回答は Context に忠実だったか？」「質問にちゃんと答えていたか？」という視点で結果を吟味してみましたか？

これらの基本的な観察ができるようになれば、評価とデバッグの重要な第一歩を踏み出せています！

---

## 6. まとめ：学びの整理と次へのステップ

### ✅ 達成したこと！

これで、作った RAG アプリケーションの良し悪しを判断し、問題点を見つけ出すための基本的な考え方とツールを知ることができました！

- RAG システムの**評価**がなぜ重要で、どのような観点（**Context Relevance**, **Answer Faithfulness**, **Answer Relevance**）で見るべきかの基本を理解しました。
- RAG システムが期待通りに動かない場合の**デバッグ**の基本的なアプローチ（**トレース**、**問題の切り分け**）を学びました。
- LangChain の**デバッグモード** (`set_debug(True)`) を有効にして、チェーン内部の動作を観察する方法を体験しました。
- より高度なデバッグ・評価プラットフォームである **LangSmith** の存在を知りました。

### 🔑 学んだキーワード

- **評価 (Evaluation)**
- **デバッグ (Debugging)**
- **Context Relevance (文脈の関連性)**
- **Answer Faithfulness (回答の忠実度)**
- **Answer Relevance (回答の関連性)**
- **トレース (Tracing)**
- **`set_debug(True)`**
- **LangSmith** (紹介)
- **LLM-as-Judge** (評価手法の紹介)
- **評価データセット**

### 🚀 次のステップへ！

RAG システムの構築から評価・デバッグの入門まで、一通りの流れを体験しました。これで、かなり本格的な Q&A システムを作るための基礎知識が身についたはずです。

さて、これまでのステップでは、主に「情報を検索して、それに基づいて回答する」という比較的シンプルなタスクを扱ってきました。しかし、LangChain の真価は、もっと複雑で自律的なタスクを実行できる**エージェント (Agent)** の構築にあります。

次の **ステップ 24「AI に道具を！カスタムツール作成」** からは、新しい章「**エージェント**」に入ります！ エージェントは、LLM を思考エンジンとして、様々な「道具（ツール）」を自分で判断して使いこなし、与えられた目標を達成しようとします。まずは、エージェントに与えるオリジナルの「道具」を作る方法から学んでいきましょう！ AI がもっと能動的に動き出す、エキサイティングな世界の幕開けです！
