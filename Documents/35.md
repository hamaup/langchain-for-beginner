さあ、いよいよ設計図を形にする時が来ました！ 前のステップでは、「リサーチャー」と「ライター」という二人の専門家 AI エージェントが協力してレポートを作成するワークフローを設計しましたね。

今回は、その設計図に基づいて、実際に Python のコードを書き、LangGraph 上で AI エージェントチームを動かしてみます。LangChain を使ってエージェントを作り、それらを LangGraph のグラフに組み込んで、情報 (状態) をバトンタッチさせながら連携させる具体的な手順を学びます。AI チームが実際に動き出す様子を一緒に見ていきましょう！

## 1. はじめに：このステップで目指すこと

### 🎯 今回のゴール

このステップを最後まで終えると、あなたはこんなことができるようになります！

- ステップ 34 で設計したマルチエージェントワークフロー（リサーチャーとライター）を、LangGraph を使って**コードで実装**します。
- LangChain の現在の推奨方法で、特定の役割とツールを持つ AI エージェントを作成します。
- 作成したエージェントを LangGraph の**ノード**として機能させるための「ラッパー関数」を作成し、グラフに組み込みます。
- **状態 (State)** を介してエージェント間で情報を連携させ、エラーが発生した場合の簡単な処理パスも実装します。
- **具体的な成果物:** ユーザーリクエストに基づき、リサーチャーが Web 検索し、エラーがなければライターがその結果をもとにレポートを作成する、**連携動作し、簡単なエラー処理も備えた LangGraph アプリケーション**の Python コードを作成し、実行結果を確認します。

### 🔑 このステップのポイント

今回は「実装」がテーマなので、以下の技術が特に重要になります。

- **エージェント作成 (`create_openai_tools_agent`, `AgentExecutor`)**: 現在の LangChain (LCEL ベース) で推奨されるエージェント構築方法。
- **ノード関数 (ラッパー関数)**: エージェントの実行と状態の更新を仲介する重要な関数。
- **状態経由の情報連携**: `TypedDict` で定義した状態を使って、ノード間でデータを正確に受け渡す方法。
- **エラー処理分岐**: グラフ内でエラー発生を検知し、処理の流れを変える基本的な実装。

### ✅ 前提知識

このステップに進む前に、以下の準備と知識があるとスムーズです。

- **ステップ 34「LangGraph 応用(1): 複数エージェント連携設計」の内容**: 今回実装するワークフローの設計（役割、状態、グラフ構造）を理解していること。
- **ステップ 33 までの LangGraph の基本**: `StateGraph`, 状態 (`TypedDict`), ノード追加 (`.add_node`), 通常のエッジ (`.add_edge`), 条件分岐エッジ (`.add_conditional_edges`) の使い方。
- **ステップ 24〜27 あたりのエージェントの基本**: LangChain でエージェントを作成し、ツールを使わせる基本的な方法。
- **必要なツールのインストール**:
  ```bash
  pip install -U langgraph langchain langchain-openai duckduckgo-search python-dotenv tiktoken
  ```
  (langgraph, langchain 本体, openai 連携, 検索ツール, .env 読み込み, トークン計算)
- **OpenAI API キー**: 環境変数 `OPENAI_API_KEY` に設定されていること。

---

## 2. 準備運動：ハンズオンのための基礎知識

### 🎯 目標

LangChain で作成したエージェントを、どのように LangGraph のノードとして組み込むのか、その「橋渡し」の方法と、今回の実装の流れを再確認しましょう。

### 実装の流れを確認

設計図をコードにするために、以下のステップで進めます。

1.  **準備**: 必要なライブラリをインポートし、LLM やツールを用意します。
2.  **エージェント作成**: リサーチャー役とライター役のエージェントを作ります。**現在の LangChain では `create_openai_tools_agent` と `AgentExecutor` を使うのが一般的**です。
3.  **状態定義**: 設計した通りの状態を `TypedDict` で定義します。今回はエラー情報を入れる `error` キーも使います。
4.  **ノード関数 (ラッパー関数) 定義**: 各エージェントを呼び出すためのラッパー関数と、エラー処理用のノード関数、そしてエラーチェック用の分岐関数を作ります。
5.  **グラフ構築**: `StateGraph` を使って、ノードとエッジ（通常および条件分岐）を定義します。エラー処理パスも組み込みます。
6.  **グラフコンパイルと実行**: グラフを動かし、連携とエラー処理を確認します。

### ラッパー関数 - エージェントとグラフの通訳

LangChain の `AgentExecutor` で作ったエージェントは、そのままでは LangGraph のノード関数（状態を受け取り、更新辞書を返す形式）としては使えません。そこで、間に「通訳」のような役割を持つ**ラッパー関数**を置きます。

**ラッパー関数の仕事:**

1.  グラフの現在の**状態 (State)** をもらう。
2.  状態から、担当エージェントに必要な**入力**を作る。
3.  エージェントを `.invoke()` で実行する。
4.  エージェントの**出力**を受け取る。
5.  出力を、グラフの**状態を更新するための辞書**に変換して返す。
6.  (今回追加) もしエージェント実行でエラーが起きたら、エラー情報を状態に書き込む辞書を返す。

このラッパー関数をノードとして登録すれば、エージェントがグラフの一部としてスムーズに機能します。

---

## 3. 実践タイム：エージェント連携グラフを実装しよう！

### 🎯 目標

設計図に基づき、リサーチャーとライターのエージェントを作成し、それらを LangGraph で連携させ、簡単なエラー処理を含むレポート作成ワークフローを実装します。

### ステップ・バイ・ステップ実装

#### 1. 準備 (インポート、LLM、ツール):

```python
# step35_multi_agent_implementation_revised.py
import os
import sys
from dotenv import load_dotenv
from typing import TypedDict, Optional, Dict, Any, List, Literal

# LangGraph
try:
    from langgraph.graph import StateGraph, START, END
    print("LangGraph をインポートしました。")
except ImportError:
    print("エラー: langgraph が見つかりません。'pip install langgraph'")
    sys.exit(1)

# LLM, Prompt, Messages
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import SystemMessage, HumanMessage # AIMessage は直接使わないかも

# Agent & Tools
try:
    from langchain.agents import AgentExecutor, create_openai_tools_agent
    from langchain_community.tools.ddg_search import DuckDuckGoSearchRun
    print("Agent と Tool 関連モジュールをインポートしました。")
except ImportError:
    print("エラー: langchain または langchain_community が不足している可能性があります。")
    print("   'pip install langchain langchain-community' を実行してください。")
    sys.exit(1)

print("--- 必要なモジュールのインポート完了 ---")

load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    print("エラー: OpenAI API キーが設定されていません。")
    sys.exit(1)
else:
    print("OpenAI API キーを読み込みました。")

# LLM の準備
try:
    # gpt-4o や gpt-3.5-turbo など、利用可能なモデルを指定
    llm = ChatOpenAI(model="gpt-4o", temperature=0.1, api_key=openai_api_key)
    print(f"--- LLM準備完了 ---") # ひとまず成功したことを出力

    # ★★★ モデル名表示部分を修正 ★★★
    # 属性名 'model' ではなく 'model_name' を試す
    try:
        print(f"  (使用モデル: {llm.model_name})")
    except AttributeError:
        # もし 'model_name' もない場合は、指定したモデル名を直接表示する
        print(f"  (使用モデル: gpt-4o)") # 初期化時に指定したモデル名を書く
        print("  (注意: model_name 属性が見つかりませんでした)")

except Exception as e:
    print(f"エラー: LLMの初期化に失敗しました: {e}")
    sys.exit(1)

# ツールの準備 (リサーチャー用)
try:
    search_tool = DuckDuckGoSearchRun()
    print("--- DuckDuckGo Search ツール準備完了 ---")
except ImportError:
    print("エラー: DuckDuckGo Search を使うには 'duckduckgo-search' が必要です。")
    print("   'pip install -U duckduckgo-search' を実行してください。")
    sys.exit(1)
```

- モデル名を `gpt-4o` に変更しました (利用可能なモデルに変更してください)。

#### 2. エージェントの作成:

`create_openai_tools_agent` と `AgentExecutor` を使ってエージェントを作成します。

```python
# step35_multi_agent_implementation_revised.py (続き)

print("\n--- エージェントの作成 ---")

# --- リサーチャーエージェント ---
researcher_prompt = ChatPromptTemplate.from_messages([
    ("system", "あなたは優秀なリサーチャーです。与えられたトピックについて DuckDuckGo Search ツールを必ず使って Web 検索を行い、最新かつ信頼性の高い情報源から重要なポイントを収集し、結果を箇条書きで要約してください。"),
    MessagesPlaceholder(variable_name="agent_scratchpad"),
    ("human", "調査トピック: {input}")
])
researcher_tools = [search_tool]
researcher_agent_runnable = create_openai_tools_agent(llm, researcher_tools, researcher_prompt)
researcher_executor = AgentExecutor(agent=researcher_agent_runnable, tools=researcher_tools, verbose=True)
print("リサーチャーエージェントを作成しました。")

# --- ライターエージェント ---
writer_prompt = ChatPromptTemplate.from_messages([
    ("system", "あなたはプロのライターです。提供された元のリクエストとリサーチ結果の要約に基づいて、読者にとって分かりやすく、簡潔で、客観的なレポートを作成してください。リサーチ結果を適切に引用・参照し、レポートのタイトルも付けてください。"),
    MessagesPlaceholder(variable_name="agent_scratchpad"),
    ("human", "以下の情報からレポートを作成してください:\n\n# 元のリクエスト:\n{request}\n\n# リサーチ結果要約:\n{research_summary}")
])
writer_tools = [] # ライターはツールを使わない
writer_agent_runnable = create_openai_tools_agent(llm, writer_tools, writer_prompt)
writer_executor = AgentExecutor(agent=writer_agent_runnable, tools=writer_tools, verbose=True)
print("ライターエージェントを作成しました。")
```

#### 3. 状態 (State) の定義:

エラー情報を保持する `error` キーを含む状態を定義します。

```python
# step35_multi_agent_implementation_revised.py (続き)

# グラフの状態を定義
class ResearchWorkflowState(TypedDict):
    request: str
    research_summary: Optional[str]
    report: Optional[str]
    error: Optional[str] # エラーメッセージ用

print("\n--- グラフの状態 (ResearchWorkflowState) 定義完了 ---")
```

#### 4. ノード関数 (ラッパー関数とエラー処理) の定義:

エージェント実行ラッパーに加え、エラーチェック分岐関数とエラーハンドラーノード関数を定義します。

```python
# step35_multi_agent_implementation_revised.py (続き)

# --- ノード関数 の定義 ---

def run_researcher(state: ResearchWorkflowState) -> Dict[str, Optional[str]]:
    """リサーチャーエージェントを実行し、結果またはエラーを返す"""
    print("\n--- ノード: Researcher 実行 ---")
    try:
        response = researcher_executor.invoke({"input": state['request']})
        summary = response.get("output")
        if summary:
             print(f"  -> リサーチ結果取得 (一部): {summary[:100]}...")
             # エラーがなければ error を None にしておく (重要)
             return {"research_summary": summary, "error": None}
        else:
             print("  -> リサーチ結果が空です。")
             return {"error": "Researcher returned empty summary."}
    except Exception as e:
        print(f"  -> リサーチャー実行エラー: {e}")
        return {"error": f"Researcher failed: {e}"}

def run_writer(state: ResearchWorkflowState) -> Dict[str, Optional[str]]:
    """ライターエージェントを実行し、結果またはエラーを返す"""
    print("\n--- ノード: Writer 実行 ---")
    # 前のステップでエラーが発生していないか確認 (エラーがあればスキップも可能だが今回はそのまま進む)
    if state.get("error"):
        print("  -> 前のステップでエラーが発生したため、ライターはスキップします。")
        # エラー状態を維持するか、ここで処理を決める
        return {} # 何も更新しない

    summary = state.get('research_summary')
    if not summary: # None または空文字列の場合
        print("  -> エラー: リサーチ結果がありません。")
        return {"error": "Writer error: Research summary is missing or empty."}

    try:
        writer_input = {
            "request": state['request'],
            "research_summary": summary
        }
        response = writer_executor.invoke(writer_input)
        report_text = response.get("output")
        if report_text:
            print(f"  -> レポート生成完了 (一部): {report_text[:100]}...")
            return {"report": report_text, "error": None} # 成功時は error を None に
        else:
            print("  -> レポートが空です。")
            return {"error": "Writer returned empty report."}
    except Exception as e:
        print(f"  -> ライター実行エラー: {e}")
        return {"error": f"Writer failed: {e}"}

# エラー処理用ノード (シンプルにエラーを出力するだけ)
def handle_error(state: ResearchWorkflowState) -> Dict[str, str]:
    """エラー発生時に呼び出されるノード"""
    print("\n--- ノード: Error Handler 実行 ---")
    error_message = state.get("error", "不明なエラーが発生しました。")
    print(f"  -> エラー発生: {error_message}")
    # 必要に応じて、ここでエラー通知などの処理を追加できる
    # 最終出力としてエラーメッセージを設定する例
    return {"output": f"処理中にエラーが発生しました: {error_message}"}

print("--- ノード関数 (run_researcher, run_writer, handle_error) 定義完了 ---")


# --- エラーチェック用 分岐関数 ---
def check_error(state: ResearchWorkflowState) -> Literal["writer", "error_handler"]:
    """状態にエラーがあるかチェックし、次のノードを決定"""
    print(f"\n--- 分岐関数: エラーチェック実行 (エラー状態: {state.get('error')}) ---")
    if state.get("error"):
        print("  -> エラー検出: handle_error へ")
        return "error_handler"
    else:
        print("  -> エラーなし: writer へ")
        return "writer"

print("--- 分岐関数 (check_error) 定義完了 ---")

```

- 各ラッパー関数 (`run_researcher`, `run_writer`) は、成功時に `error` キーを `None` で更新し、失敗時にエラーメッセージを設定して返すようにしました。
- `handle_error`: エラー発生時に呼ばれるシンプルなノード。
- `check_error`: 状態の `error` キーを見て、`writer` に進むか `error_handler` に進むかを決定します。

#### 5. グラフの構築 (エラー処理パス込み):

`StateGraph` を使い、エラーチェックの分岐を含むグラフを構築します。

```python
# step35_multi_agent_implementation_revised.py (続き)

print("\n--- グラフの構築開始 (エラー処理分岐あり) ---")
workflow = StateGraph(ResearchWorkflowState)

# ノードを追加
workflow.add_node("researcher", run_researcher)
workflow.add_node("writer", run_writer)
workflow.add_node("error_handler", handle_error)
print("ノードを追加しました: researcher, writer, error_handler")

# エッジを設定
workflow.add_edge(START, "researcher") # まずリサーチャーを実行

# researcher の後にエラーチェック分岐を設定
workflow.add_conditional_edges(
    source="researcher",
    path=check_error, # check_error 関数の結果で分岐
    path_map={ # 分岐関数が返す値と実際のノード名を対応付ける
        "writer": "writer",
        "error_handler": "error_handler"
    }
)
print("条件分岐エッジを追加しました: researcher からエラーチェック経由で writer または error_handler へ")

# writer または error_handler が終わったら終了
workflow.add_edge("writer", END)
workflow.add_edge("error_handler", END)
print("終了へのエッジを追加しました。")

# エントリーポイントは START からのエッジで自動設定
print("--- グラフの構築完了 ---")
```

- `researcher` ノードの次に `check_error` 分岐関数が呼ばれ、エラーがなければ `writer` へ、あれば `error_handler` へ進むように `.add_conditional_edges` を設定しました。今回は `path_map` を使ってみました（分岐関数が直接ノード名を返しても同じです）。

#### 6. グラフのコンパイルと実行:

グラフをコンパイルし、実行して動作を確認します。ストリーミング実行の例も示します。

```python
# step35_multi_agent_implementation_revised.py (続き)

print("\n--- グラフのコンパイル開始 ---")
app = workflow.compile()
print("--- グラフのコンパイル完了 ---")

print("\n--- グラフの実行 ---")
initial_state = {"request": "LangGraph の主要な機能とその利点について日本語でレポートしてください"}
print(f"初期状態 (リクエスト): {initial_state}")

# --- ストリーミング実行の例 ---
print("\n--- ストリーミング実行ログ (各ステップの結果) ---")
final_state = {} # 最終状態を保持する変数
try:
    # config で再帰制限を設定
    for event in app.stream(initial_state, config={'recursion_limit': 10}):
        # イベントの内容を表示 (キーはイベントが発生したノード名)
        # print(f"イベント: {list(event.keys())[0]}")
        # print(event[list(event.keys())[0]]) # イベントデータを表示
        print(".", end="", flush=True) # 進行状況をドットで表示

        # 最後のイベントの状態を final_state に保持する
        # stream() の最後は __end__ キーになることが多い
        if END not in event:
            latest_key = list(event.keys())[-1]
            final_state.update(event[latest_key]) # 状態をマージ

    print("\n--- ストリーミング実行完了 ---")

except Exception as e:
    print(f"\n--- グラフ実行中に予期せぬエラー ---")
    print(f"エラー: {e}")
    final_state = {"error": f"Unexpected error during execution: {e}"} # エラー情報を状態に記録


# --- 最終結果の表示 ---
print("\n\n--- 最終状態の確認 ---")
# print(f"最終状態全体: {final_state}") # デバッグ用に全状態を表示

print(f"\nリクエスト:\n{final_state.get('request')}")
print(f"\nリサーチ結果要約:\n{final_state.get('research_summary', '(リサーチ未実行または失敗)')}")
print(f"\n最終レポート:\n{final_state.get('report', '(レポート未作成または失敗)')}")
if final_state.get('error'):
    print(f"\nエラーメッセージ:\n{final_state.get('error')}")


# --- (オプション) グラフ構造を Mermaid で出力 ---
# print("\n--- グラフ構造 (Mermaid) ---")
# try:
#     mermaid_txt = app.get_graph().draw_mermaid()
#     print(mermaid_txt)
# except Exception as e:
#     print(f"Mermaid図の生成中にエラー: {e}")

print("\n--- 全体の処理終了 ---")

```

- `.stream()` を使って実行し、各ステップ（ノード実行）のイベントを受け取ります。ここでは簡単にドット `.` を表示して進行状況を示し、最後の状態を `final_state` 変数に集約するようにしています。
- 最後に、集約した `final_state` から必要な情報を取り出して表示します。エラーが発生した場合も `error` キーの内容が表示されます。
- Mermaid 図の出力コードもオプションとして残しています。

### 完成コード (`step35_multi_agent_implementation_revised.py`)

上記の実装 1〜6 を結合したものが完成コードとなります。

---

## 4. 深掘り解説：実装のポイントと応用

### 🎯 目標

今回実装したコードのポイント、特にラッパー関数の役割やエラーハンドリング、そしてさらなる応用について理解を深めます。

### ラッパー関数の役割再訪 - 状態とエージェントの通訳

`run_researcher` や `run_writer` のようなラッパー関数が、LangGraph の状態管理と LangChain エージェントを繋ぐ「通訳」として機能します。状態からエージェントへの入力を作り、エージェントを実行し、結果を状態更新用の辞書に戻す、このパターンが基本です。エラーが発生した場合に `{"error": ...}` を返すようにすることで、グラフにエラー情報を伝えることができます。

### エラーハンドリング - グラフで流れを制御

今回は、`researcher` ノードの後にエラーチェック用の分岐 (`check_error`) を設け、エラーがあれば `handle_error` ノードに進むというシンプルなエラー処理パスを実装しました。このように LangGraph では、エラー発生時の処理フローをグラフ構造自体に組み込むことができます。より複雑なアプリケーションでは、エラーの種類に応じてリトライさせたり、別のエージェントに処理を引き継いだりする、といった高度なエラーハンドリングも設計・実装可能です。

### エージェントのカスタマイズ

- **プロンプト**: エージェントの指示を調整することで、リサーチの質やレポートのスタイルを細かく制御できます。
- **ツール**: より専門的なツール（データベース検索、計算、ファイル操作など）をエージェントに追加することで、対応できるタスクの幅が広がります。
- **LLM**: より能力の高い LLM を使うと結果の質が向上しますが、コストとのバランスを考える必要があります。

### 状態管理の工夫

今回は状態を介して一方的に情報を渡しましたが、エージェント間で複数回のやり取り（相談やフィードバックなど）が必要な場合は、状態に「現在の担当者」「前の担当者からのコメント」といった情報を追加し、条件分岐やループを使って複雑な対話フローを実装することも可能です。

### ストリーミング実行 (`.stream()`) の利点

`.invoke()` が最終結果だけを返すのに対し、`.stream()` はグラフ内の各ステップが完了するたびに、その時点での状態の変化やノードの出力などをリアルタイムで返します。ユーザーに進捗状況を見せたり、時間がかかる処理でも応答性を高く見せたりしたい場合に非常に有効です。

---

## 5. 最終チェック：AI チームはうまく連携できた？

### 🎯 目標

実装したマルチエージェント連携グラフが、設計通りに動作し、期待される結果を出力するか、エラー処理も含めて確認します。

### 確認してみよう！

- **実行**: `step35_multi_agent_implementation_revised.py` を実行してください。（必要なライブラリと API キーを確認）
- **正常系**:
  - エラーが発生せず、最後まで実行できましたか？
  - コンソールログを見て、「Researcher 実行」→「エラーチェック実行 (エラーなし)」→「Writer 実行」の順で処理が進みましたか？
  - 最終出力の「リサーチ結果要約」と「最終レポート」に、それぞれ内容が表示されていますか？レポートは要約を反映したものになっていますか？
- **(エラー系のテスト - 任意)**:
  - 例えば、`run_researcher` 関数内の `researcher_executor.invoke()` をわざと失敗させる（例: 無効な入力を与える）ように一時的にコードを書き換えて実行してみてください。
  - コンソールログで「エラーチェック実行 (エラー検出)」→「Error Handler 実行」という流れになり、「Writer 実行」はスキップされることを確認してください。
  - 最終出力にエラーメッセージが表示されることを確認してください。（テスト後はコードを元に戻してください）
- **リクエスト変更**: `initial_state` の `request` を別のトピックに変えて実行し、異なるレポートが生成されるか試してみましょう。

これらの動作が確認できれば、エラー処理を含むマルチエージェント連携の実装は成功です！

---

## 6. まとめ：学びの整理と次へのステップ

### ✅ 達成したこと！

設計図を元に、AI エージェントチームを LangGraph 上で動かすことができました！

- ステップ 34 で設計したマルチエージェントワークフロー（リサーチャー → ライター）を LangGraph で**実装**しました。
- LangChain の現在の推奨方法 (`create_openai_tools_agent`, `AgentExecutor`) で **AI エージェント**を作成しました。
- エージェントを LangGraph ノードとして実行するための**ラッパー関数**を作成し、状態との情報連携を行いました。
- **条件分岐**を使って、簡単な**エラー処理パス**をグラフに組み込みました。
- `.stream()` を使った実行方法にも触れました。

### 🔑 学んだキーワード

- **マルチエージェント連携実装 (Multi-Agent Implementation)**
- **エージェント作成 (`create_openai_tools_agent`, `AgentExecutor`)**
- **ツール (`DuckDuckGoSearchRun`)**
- **ラッパー関数 (Node Function / Helper Function)**
- **状態経由の情報連携 (State-based Information Passing)**
- **エラー処理分岐 (Error Handling Branch)**
- **`.invoke()` / `.stream()`** (グラフ実行)

### 🚀 次のステップへ！

基本的なエージェント連携とエラー処理を実装できました。LangGraph の強力な機能を使えば、さらに高度なワークフローが構築可能です。

次の **ステップ 36「LangGraph 応用(3): 人間の確認を挟む」** では、自動化されたプロセスの中に**人間の判断や承認**を組み込む **「Human-in-the-loop」** の仕組みを LangGraph で実装する方法を探求します。AI だけでは判断が難しい場面や、最終的なアウトプットを人間がチェックしたい場合に非常に役立つテクニックです。AI と人間が協力する、より洗練されたワークフローを目指しましょう！
