AI との対話で、「さっき話した内容をもう忘れてしまったのかな？」と感じた経験はありませんか？ 自己紹介をした直後に、まるで初めて会ったかのように応答されると、少し物足りなさを感じることがありますよね。

実は、私たちが対話する AI (LLM) は、基本的に過去のやり取りを**記憶しません**。各リクエストは独立して処理され、前の会話の文脈は引き継がれないのです。しかし、心配はいりません。このステップでは、LangChain の「メモリ」機能を利用して、AI に会話履歴を効果的に「記憶」させる方法を学びます。

これにより、あなたの作る AI は文脈を理解し、より自然で継続性のある対話を実現できるようになります。

## 1\. はじめに：このステップで目指すこと

### 🎯 今回のゴール

- なぜ AI アプリケーションにおいて会話履歴の「記憶」が重要になるのかを理解する。
- LangChain を用いて会話履歴を管理し、AI の応答に反映させる基本的な手法を習得する。
- **具体的な成果物:** ユーザーの名前など、過去の発言内容を記憶し、それに基づいた応答ができるチャットボットの LCEL チェーンを作成します。

### 🔑 このステップのポイント

- **AI のステートレス性:** LLM が本質的に状態を持たず、会話履歴を保持しない特性（ステートレス性）と、その対応策としてのメモリ機能の必要性を理解します。
- **メモリ管理コンポーネント:**
  - **`ChatMessageHistory`:** 会話のメッセージ履歴を管理するためのインターフェースと、その具体的な実装（今回はインメモリ実装を利用）。
  - **`RunnableWithMessageHistory`:** 既存の LCEL チェーンに履歴管理機能を追加するためのラッパー。
- **LCEL との統合:** メモリ機能を LCEL チェーンにスムーズに組み込む方法を学びます。

### ✅ 準備するもの

- これまでのステップの内容（特に LCEL の基本操作）。
- Python 開発環境（ステップ 1 でセットアップ済み）。
- OpenAI API キー。

---

## 2\. なぜ AI は「記憶」しないのか？：ステートレス性とメモリの仕組み

### 🎯 目標

- LLM が会話履歴を保持しない理由（ステートレス性）と、LangChain がプロンプトを通じてどのように文脈を維持するかを理解する。

### AI の性質：ステートレスであるということ

AI (特に LLM) は、驚くほど高度な応答を生成しますが、本質的には**ステートレス (stateless)** です。つまり、過去のインタラクションに関する内部的な「記憶」を持っていません。一つ一つのリクエストは、それまでの文脈から切り離されて処理されるのが基本です。

これが、「私の名前、覚えてますか？」という質問に対して、「以前の情報はありません」といった応答が返ってくる理由です。

### LangChain の解決策：プロンプトへの履歴注入

では、どのようにして AI に文脈を「記憶」させるのでしょうか？ LangChain のアプローチは、**過去の会話履歴を、現在のユーザー入力と共にプロンプト（指示）に含めて LLM に渡す**というものです。

LLM はステートレスですが、プロンプトに含まれる情報は文脈として最大限活用します。したがって、

1.  会話のやり取りを**記録する仕組み (`ChatMessageHistory`)** を用意する。
2.  新しいリクエストを受けたら、その記録を参照し、**過去の履歴を現在のプロンプトに組み込む**。
3.  LLM は、履歴を含むプロンプト全体を解釈し、**文脈に沿った応答を生成**する。

このプロセスを自動化し、開発者が容易に実装できるようにするのが LangChain のメモリ機能であり、以下のコンポーネントが中心的な役割を果たします。

- **`ChatMessageHistory` (会話履歴インターフェース):**
  - 会話メッセージ（HumanMessage, AIMessage など）のリストを時系列で管理するための標準的な方法（インターフェース）を提供します。
  - 実際の保存先（メモリ、ファイル、DB など）に応じて、様々な**実装クラス**が存在します。今回は、`langchain-community` パッケージに含まれるインメモリ実装を使用します。（`pip install langchain-community` が必要になる場合があります）
- **`RunnableWithMessageHistory` (履歴管理ラッパー):**
  - 既存の LCEL チェーン (`prompt | llm` など) をラップし、`ChatMessageHistory` と連携して自動的に履歴の読み込みと保存を行います。
  - チェーン実行前に履歴を読み込んでプロンプトに挿入し、実行後に新しいやり取りを履歴に保存します。

---

## 3\. 実践：記憶機能を持つチャットボットの構築

### 🎯 目標

- 最新の推奨方法に基づき、`ChatMessageHistory` と `RunnableWithMessageHistory` を使用して、会話履歴を保持・利用する LCEL チェーンを実装する。

### ファイルの準備

- 作業フォルダに `step8_memory_bot_refined.py` という名前で新しい Python ファイルを作成してください。

### ステップ・バイ・ステップ実装

1.  **環境設定とインポート:**
    必要なライブラリをインポートし、`ChatOpenAI` を初期化します。`ChatMessageHistory` は `langchain-community` からインポートします。

    ```python
    # step8_memory_bot_refined.py
    import os
    from dotenv import load_dotenv
    from langchain_openai import ChatOpenAI

    # --- LangChain Core Components ---
    from langchain_core.prompts import (
        ChatPromptTemplate,
        MessagesPlaceholder,
        HumanMessagePromptTemplate,
    )
    from langchain_core.messages import SystemMessage
    from langchain_core.chat_history import BaseChatMessageHistory
    from langchain_core.runnables.history import RunnableWithMessageHistory

    # --- Chat History Implementation (from Community) ---
    # Requires: pip install langchain-community
    try:
        from langchain_community.chat_message_histories import ChatMessageHistory
    except ImportError:
        print("エラー: 'langchain-community' パッケージが必要です。")
        print("コマンドラインで 'pip install langchain-community' を実行してください。")
        exit()

    # --- Initial Setup ---
    load_dotenv()
    print("環境変数を読み込みました。")
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
    print(f"LLM ({llm.model_name}) の準備完了。")

    # --- In-memory Chat History Store ---
    # Using a simple dictionary to store history per session_id
    session_memory_store = {}

    def get_chat_history_for_session(session_id: str) -> BaseChatMessageHistory:
        """Retrieves or creates a ChatMessageHistory instance for a given session ID."""
        if session_id not in session_memory_store:
            print(f"システム: 新しいセッション (ID: {session_id}) の履歴を作成します。")
            session_memory_store[session_id] = ChatMessageHistory()
        else:
            print(f"システム: 既存のセッション (ID: {session_id}) の履歴を使用します。")
        return session_memory_store[session_id]

    print("インメモリ会話履歴ストアの準備完了。")
    ```

2.  **プロンプトテンプレートの定義:**
    `SystemMessage`, `MessagesPlaceholder`, `HumanMessagePromptTemplate` を使用して、履歴を組み込めるプロンプトを作成します。

    ```python
    # step8_memory_bot_refined.py (続き)
    # Define the prompt template
    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content="あなたはフレンドリーなアシスタントです。以前の会話を考慮して応答してください。"),
        MessagesPlaceholder(variable_name="chat_history"), # Placeholder for history
        HumanMessagePromptTemplate.from_template("{user_input}") # Placeholder for user input
    ])
    print("プロンプトテンプレートを作成しました。")
    ```

3.  **基本的な LCEL チェーンの構築:**
    プロンプトと LLM を接続する基本チェーンを定義します。

    ```python
    # step8_memory_bot_refined.py (続き)
    # Build the base LCEL chain
    base_chain = prompt | llm
    print("基本LCELチェーンを構築しました。")
    ```

4.  **`RunnableWithMessageHistory` によるラップ:**
    基本チェーンにメモリ機能を追加します。

    ```python
    # step8_memory_bot_refined.py (続き)
    # Wrap the base chain with memory management
    chain_with_memory = RunnableWithMessageHistory(
        base_chain,
        get_chat_history_for_session, # Function to get/create history
        input_messages_key="user_input", # Key for user input in the prompt
        history_messages_key="chat_history", # Key for history placeholder in the prompt
    )
    print("メモリ管理機能付きチェーンを構築しました。")
    ```

5.  **対話の実行:**
    セッション ID を指定し、複数回の対話を実行してメモリ機能を確認します。

    ```python
    # step8_memory_bot_refined.py (続き)
    # --- Start Conversation ---
    session_id = "user123_session_A"
    print(f"\n会話を開始します (セッションID: {session_id})")

    # Configuration object to pass the session_id
    chat_config = {"configurable": {"session_id": session_id}}

    # --- Interaction 1: Introduction ---
    print("\n--- 1回目の対話 ---")
    input1 = {"user_input": "こんにちは。私の名前は佐藤です。"}
    print(f"あなた: {input1['user_input']}")
    try:
        response1 = chain_with_memory.invoke(input1, config=chat_config)
        print(f"AI: {response1.content}")
    except Exception as e:
        print(f"エラーが発生しました: {e}")

    # --- Interaction 2: Casual Talk ---
    print("\n--- 2回目の対話 ---")
    input2 = {"user_input": "おすすめの映画はありますか？"}
    print(f"あなた: {input2['user_input']}")
    try:
        response2 = chain_with_memory.invoke(input2, config=chat_config)
        print(f"AI: {response2.content}")
    except Exception as e:
        print(f"エラーが発生しました: {e}")

    # --- Interaction 3: Memory Check ---
    print("\n--- 3回目の対話 ---")
    input3 = {"user_input": "私の名前を覚えていますか？"}
    print(f"あなた: {input3['user_input']}")
    try:
        response3 = chain_with_memory.invoke(input3, config=chat_config)
        # Expect the AI to respond with "佐藤さん"
        print(f"AI: {response3.content}")
    except Exception as e:
        print(f"エラーが発生しました: {e}")

    # --- Inspect the Chat History (Optional) ---
    print("\n--- 会話履歴の確認 ---")
    try:
        final_history = get_chat_history_for_session(session_id)
        if final_history.messages:
            for message in final_history.messages:
                print(f"[{message.type.upper()}] {message.content}")
        else:
            print("(履歴は空です)")
    except Exception as e:
        print(f"履歴の取得中にエラーが発生しました: {e}")

    print("\n会話を終了します。")
    ```

### 完成コード (`step8_memory_bot_refined.py`)

```python
# step8_memory_bot_refined.py
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

# --- LangChain Core Components ---
from langchain_core.prompts import (
    ChatPromptTemplate,
    MessagesPlaceholder,
    HumanMessagePromptTemplate,
)
from langchain_core.messages import SystemMessage
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

# --- Chat History Implementation (from Community) ---
# Requires: pip install langchain-community
try:
    from langchain_community.chat_message_histories import ChatMessageHistory
except ImportError:
    print("エラー: 'langchain-community' パッケージが必要です。")
    print("コマンドラインで 'pip install langchain-community' を実行してください。")
    exit()

# --- Initial Setup ---
load_dotenv()
print("環境変数を読み込みました。")
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
print(f"LLM ({llm.model_name}) の準備完了。")

# --- In-memory Chat History Store ---
session_memory_store = {}
def get_chat_history_for_session(session_id: str) -> BaseChatMessageHistory:
    if session_id not in session_memory_store:
        print(f"システム: 新しいセッション (ID: {session_id}) の履歴を作成します。")
        session_memory_store[session_id] = ChatMessageHistory()
    else:
        print(f"システム: 既存のセッション (ID: {session_id}) の履歴を使用します。")
    return session_memory_store[session_id]
print("インメモリ会話履歴ストアの準備完了。")

# --- Define the prompt template ---
prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content="あなたはフレンドリーなアシスタントです。以前の会話を考慮して応答してください。"),
    MessagesPlaceholder(variable_name="chat_history"),
    HumanMessagePromptTemplate.from_template("{user_input}")
])
print("プロンプトテンプレートを作成しました。")

# --- Build the base LCEL chain ---
base_chain = prompt | llm
print("基本LCELチェーンを構築しました。")

# --- Wrap the base chain with memory management ---
chain_with_memory = RunnableWithMessageHistory(
    base_chain,
    get_chat_history_for_session,
    input_messages_key="user_input",
    history_messages_key="chat_history",
)
print("メモリ管理機能付きチェーンを構築しました。")

# --- Start Conversation ---
session_id = "user123_session_A"
print(f"\n会話を開始します (セッションID: {session_id})")
chat_config = {"configurable": {"session_id": session_id}}

# --- Interaction 1: Introduction ---
print("\n--- 1回目の対話 ---")
input1 = {"user_input": "こんにちは。私の名前は佐藤です。"}
print(f"あなた: {input1['user_input']}")
try:
    response1 = chain_with_memory.invoke(input1, config=chat_config)
    print(f"AI: {response1.content}")
except Exception as e:
    print(f"エラーが発生しました: {e}")

# --- Interaction 2: Casual Talk ---
print("\n--- 2回目の対話 ---")
input2 = {"user_input": "おすすめの映画はありますか？"}
print(f"あなた: {input2['user_input']}")
try:
    response2 = chain_with_memory.invoke(input2, config=chat_config)
    print(f"AI: {response2.content}")
except Exception as e:
    print(f"エラーが発生しました: {e}")

# --- Interaction 3: Memory Check ---
print("\n--- 3回目の対話 ---")
input3 = {"user_input": "私の名前を覚えていますか？"}
print(f"あなた: {input3['user_input']}")
try:
    response3 = chain_with_memory.invoke(input3, config=chat_config)
    print(f"AI: {response3.content}") # Expect the AI to respond with "佐藤さん"
except Exception as e:
    print(f"エラーが発生しました: {e}")

# --- Inspect the Chat History (Optional) ---
print("\n--- 会話履歴の確認 ---")
try:
    final_history = get_chat_history_for_session(session_id)
    if final_history.messages:
        for message in final_history.messages:
            print(f"[{message.type.upper()}] {message.content}")
    else:
        print("(履歴は空です)")
except Exception as e:
    print(f"履歴の取得中にエラーが発生しました: {e}")

print("\n会話を終了します。")
```

### 実行結果のイメージ

```
環境変数を読み込みました。
LLM (gpt-3.5-turbo) の準備完了。
インメモリ会話履歴ストアの準備完了。
プロンプトテンプレートを作成しました。
基本LCELチェーンを構築しました。
メモリ管理機能付きチェーンを構築しました。

会話を開始します (セッションID: user123_session_A)

--- 1回目の対話 ---
あなた: こんにちは。私の名前は佐藤です。
システム: 新しいセッション (ID: user123_session_A) の履歴を作成します。
AI: こんにちは、佐藤さん。お会いできて嬉しいです。今日はどのようなご用件でしょうか？

--- 2回目の対話 ---
あなた: おすすめの映画はありますか？
システム: 既存のセッション (ID: user123_session_A) の履歴を使用します。
AI: おすすめの映画ですね。[AIが映画のタイトルや簡単な説明を生成]... などはいかがでしょうか。佐藤さんはどのようなジャンルがお好きですか？

--- 3回目の対話 ---
あなた: 私の名前を覚えていますか？
システム: 既存のセッション (ID: user123_session_A) の履歴を使用します。
AI: はい、もちろんです。佐藤さんですよね。何かお手伝いできることがあれば、お気軽にお声がけください。

--- 会話履歴の確認 ---
[HUMAN] こんにちは。私の名前は佐藤です。
[AI] こんにちは、佐藤さん。お会いできて嬉しいです。今日はどのようなご用件でしょうか？
[HUMAN] おすすめの映画はありますか？
[AI] おすすめの映画ですね。...佐藤さんはどのようなジャンルがお好きですか？
[HUMAN] 私の名前を覚えていますか？
[AI] はい、もちろんです。佐藤さんですよね。何かお手伝いできることがあれば、お気軽にお声がけください。

会話を終了します。
```

AI が 3 回目の対話で「佐藤さん」と応答していることから、履歴が正しく参照されていることがわかります。

---

## 4\. 深掘り解説：メモリ機能の仕組みと考慮事項

### 🎯 目標

- メモリ管理の内部動作、コンテキストウィンドウの制約、永続化、非同期処理、実運用上の注意点について理解を深める。

### `RunnableWithMessageHistory` の動作フロー

このラッパーは、内部で以下のステップを実行し、履歴管理を自動化します。

1.  **履歴取得:** `config` の `session_id` を使い、指定された関数 (`get_chat_history_for_session`) 経由で `ChatMessageHistory` から過去のメッセージを取得。
2.  **プロンプト構築:** 取得した履歴と現在のユーザー入力（`input_messages_key`）を、プロンプトテンプレート（`history_messages_key` を持つ `MessagesPlaceholder`）に挿入。
3.  **チェーン実行:** 完成したプロンプトで元のチェーン (`base_chain`) を実行。
4.  **履歴保存:** ユーザー入力と AI 応答を `ChatMessageHistory` に追加。

### コンテキストウィンドウ制限への対応

LLM には処理可能なトークン数（コンテキストウィンドウ）に上限があります。会話履歴が長くなると、この上限を超えてエラーになるか、古い情報が無視されます。

**主な対策:**

- **ウィンドウメモリ (`ConversationBufferWindowMemory`):** 直近 K 回の対話のみを保持。
- **要約メモリ (`ConversationSummaryMemory`, etc.):** 古い履歴を要約して保持。
- **検索ベースメモリ (`VectorStoreRetrieverMemory`):** 関連性の高い過去の対話のみを検索して利用 (RAG 的アプローチ)。
- **エンティティメモリ (`EntityMemory`):** 特定のエンティティ（人名など）に関する情報を構造化して記憶。

### 履歴の永続化

今回のインメモリ実装 (`ChatMessageHistory` from `langchain-community`) はプログラム終了時に履歴が消えます。実運用では**永続化**が必要です。

- **ファイル (`FileChatMessageHistory`)**
- **データベース (`RedisChatMessageHistory`, `SQLChatMessageHistory` など)**

利用するストレージに応じた `ChatMessageHistory` の実装クラスを選択します。

### 非同期処理の利点

多くのリクエストを捌く Web アプリなどでは、LLM の応答待ち時間を有効活用できる**非同期 API** (`.ainvoke()`, `.astream()`) の利用が効果的です。アプリケーション全体の応答性を高められます。

### 実運用上の考慮事項

- **エラーハンドリング:** LLM API エラー、履歴ストアのエラー、予期せぬ応答形式など、様々なエラーへの対応（ログ記録、リトライ、フォールバック）が不可欠です。
- **セキュリティ:** 会話履歴には機密情報が含まれる可能性があります。保存時の暗号化、アクセス制御、不要になったデータの削除ポリシーなどを検討する必要があります。インメモリストアは本番環境には不向きです。
- **セッション管理:** ユーザーごとに履歴を安全に管理するため、推測されにくいセッション ID の生成、有効期限管理、セッションストアの保護など、堅牢なセッション管理が必要です。

---

## 5\. 最終チェック：記憶の定着確認

### 🎯 目標

- 実装したチャットボットが、意図通りに会話履歴を利用しているかを確認する。

### 確認してみよう！

- `step8_memory_bot_refined.py` を実行してください。（必要なら `pip install langchain-community` を先に）
- 3 回目の対話で、AI は正しくあなたの名前（佐藤さん）を呼びましたか？
- 会話履歴は正しく記録・表示されましたか？
- `session_id` を変更して実行し、新しい履歴が作成され、AI が最初は名前を知らないことを確認してください。

---

## 6\. まとめ：AI に記憶力が宿った！

### ✅ 達成したこと！

- LLM のステートレス性と、プロンプトへの履歴注入による「記憶」の実現方法を理解した。
- `ChatMessageHistory` インターフェースと `RunnableWithMessageHistory` ラッパーを用いた基本的なメモリ管理を実装できた。
- 最新の推奨形式でプロンプトテンプレートを記述し、履歴を効果的に組み込めるようになった。
- 会話の文脈を維持するチャットボットを LCEL で構築できた。
- コンテキストウィンドウ、永続化、非同期処理、セキュリティなど、実用化に向けた重要な考慮点を学んだ。

### 🔑 学んだキーワード

- メモリ (Memory), 会話履歴 (Chat History), 文脈 (Context)
- ステートレス (Stateless)
- `ChatMessageHistory` (インターフェースと実装)
- `RunnableWithMessageHistory`
- `MessagesPlaceholder`, `SystemMessage`, `HumanMessagePromptTemplate`
- セッション ID (`session_id`), 設定オブジェクト (`config`)
- コンテキストウィンドウ (Context Window)
- 永続化 (Persistence), 非同期 API (Async API), エラーハンドリング, セキュリティ

### 🚀 次のステップへ！

基本的な記憶機能は実装できました。次は、AI の応答をさらに豊かにしてみましょう。

次のステップでは、AI に特定の**役割や性格**を付与する方法を学びます。`SystemMessage` を調整し、プロンプトエンジニアリングのテクニックを駆使して、あなただけの個性的な AI アシスタントを作り上げていきましょう！ (→ ステップ 11「ボットに性格を！役割と会話を調整」へ)
